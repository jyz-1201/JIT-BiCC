ioexception ioe = new malformedurlexception ( "couldn't convert '" +
url . getport ( ) , url . getpath ( ) , url . getquery ( ) , url . getref ( ) ) ;
if ( "http" . equals ( url . getprotocol ( ) ) | | "https" . equals ( url . getprotocol ( ) ) ) {
dest = normalize ( dest ) ;
url = new url ( escape pattern . matcher ( uristring ) . replaceall ( " % $1" ) ) ;
uri uri = new uri ( url . getprotocol ( ) , url . getuserinfo ( ) , url . gethost ( ) ,
}
import java . util . regex . matcher ;
return url ;
url . tostring ( ) + "' to a valid uri" ) ;
private url normalize ( url url ) throws ioexception {
throw ioe ;
string uristring = uri . tostring ( ) ;
import java . net . malformedurlexception ;
src = normalize ( src ) ;
import java . net . uri ;
import java . net . urisyntaxexception ;
private static final pattern escape pattern = pattern . compile ( " % 25 ( [ 0 - 9a - fa - f ] [ 0 - 9a - fa - f ] ) " ) ;
} catch ( urisyntaxexception e ) {
ioe . initcause ( e ) ;
import java . util . regex . pattern ;
url = normalize ( url ) ;
try {
private static final string license = "license" ;
private static final string license url = "url" ;
string url = getfirstchildtext ( license , license url ) ;
licenses . normalize ( ) ;
}
import org . apache . ivy . core . module . descriptor . license ;
return ( license [ ] ) lics . toarray ( new license [ lics . size ( ) ] ) ;
for ( iterator it = getallchilds ( licenses ) . iterator ( ) ; it . hasnext ( ) ; ) {
if ( name ! = null | | url ! = null ) {
private static final string licenses = "licenses" ;
if ( licenses = = null ) {
if ( license . equals ( license . getnodename ( ) ) ) {
return new license [ 0 ] ;
private static final string license name = "name" ;
element license = ( element ) it . next ( ) ;
lics . add ( new license ( name , url ) ) ;
string name = getfirstchildtext ( license , license name ) ;
import java . util . arraylist ;
mdbuilder . setlicenses ( domreader . getlicenses ( ) ) ;
public license [ ] getlicenses ( ) {
element licenses = getfirstchildelement ( projectelement , licenses ) ;
list / * < license > * / lics = new arraylist ( ) ;
for ( int i = 0 ; i < licenses . length ; i + + ) {
license [ ] licenses = md . getlicenses ( ) ;
out . println ( "" + md . getdescription ( ) ) ;
if ( md . gethomepage ( ) ! = null | | md . getdescription ( ) ! = null ) {
}
import org . apache . ivy . core . module . descriptor . license ;
private static boolean requireinnerinfoelement ( moduledescriptor md ) {
return md . getextrainfo ( ) . size ( ) > 0
if ( license . geturl ( ) ! = null ) {
| | ( md . getdescription ( ) ! = null & & md . getdescription ( ) . trim ( ) . length ( ) > 0 )
if ( requireinnerinfoelement ( md ) ) {
| | md . gethomepage ( ) ! = null
} else {
out . println ( " / > " ) ;
license license = licenses [ i ] ;
out . print ( " < description" ) ;
out . print ( " homepage = \ "" + md . gethomepage ( ) + " \ "" ) ;
out . print ( " < license " ) ;
out . print ( "url = \ "" + license . geturl ( ) + " \ " " ) ;
| | md . getlicenses ( ) . length > 0 ;
out . println ( " < / description > " ) ;
if ( license . getname ( ) ! = null ) {
out . print ( "name = \ "" + license . getname ( ) + " \ " " ) ;
if ( md . gethomepage ( ) ! = null ) {
out . println ( " > " ) ;
if ( md . getdescription ( ) ! = null & & md . getdescription ( ) . trim ( ) . length ( ) > 0 ) {
parentmodrevid ) ;
modulerevisionid parentmodrevid = modulerevisionid . newinstance (
resolvedmodulerevision parentmodule = parseotherpom ( ivysettings ,
pomreader domreader = new pomreader ( descriptorurl , res ) ;
domreader . getparentgroupid ( ) ,
domreader . getparentversion ( ) ) ;
}
domreader . getparentartifactid ( ) ,
domreader . setproperty ( ( string ) prop . getkey ( ) , ( string ) prop . getvalue ( ) ) ;
if ( parentmodule ! = null ) {
moduledescriptor parentdescr = null ;
parentdescr = parentmodule . getdescriptor ( ) ;
} else {
message . warn ( "impossible to load parent for " + descriptorurl + " . "
string pluginextrainfo = ( string ) ivymoduledescriptor . getextrainfo ( ) . get ( "m : maven . plugins" ) ;
if ( domreader . hasparent ( ) ) {
for ( iterator iter = parentpomprops . entryset ( ) . iterator ( ) ; iter . hasnext ( ) ; ) {
+ " parent = " + parentmodrevid ) ;
map parentpomprops = pommoduledescriptorbuilder . extractpomproperties ( parentdescr . getextrainfo ( ) ) ;
map . entry prop = ( map . entry ) iter . next ( ) ;
parser . parsedescriptor (
parentdescr . getextrainfo ( ) ) ;
getsettings ( ) , temp . touri ( ) . tourl ( ) , res , false ) ;
map parentpomprops = pommoduledescriptorbuilder . extractpomproperties (
long getresponsecontentlength ( httpmethodbase method ) ;
getmethod get = doget ( url , 0 ) ;
if ( method ! = null ) {
}
if ( getrequestmethod ( ) = = urlhandler . request method head ) {
private getmethod doget ( url url , int timeout ) throws ioexception {
method = doget ( url , timeout ) ;
getmethod get = doget ( src , 0 ) ;
return new urlinfo ( true , getresponsecontentlength ( method ) , getlastmodified ( method ) ) ;
} else {
header header = method . getresponseheader ( "content - length" ) ;
public long getresponsecontentlength ( httpmethodbase method ) {
method = dohead ( url , timeout ) ;
client . settimeout ( timeout ) ;
method . releaseconnection ( ) ;
httpmethodbase method = null ;
if ( checkstatuscode ( url , method ) ) {
return method . getresponsecontentlength ( ) ;
private long getresponsecontentlength ( httpmethodbase head ) {
if ( "ivy . instance" . equals ( id ) & & getproject ( ) . getreferences ( ) . get ( id ) = = null ) {
import java . io . linenumberreader ;
count + = nbrbytescopied ;
linenumberreader reader = new linenumberreader ( new inputstreamreader ( in , "utf - 8" ) ) ;
nbrbytescopied + = in . read ( b , off + nbrbytescopied , len - nbrbytescopied ) ;
e . printstacktrace ( ) ;
in . mark ( 10000 ) ;
}
return new inputsource ( pomreader . class . getresourceasstream ( "m2 - entities . ent" ) ) ;
parentelement = getfirstchildelement ( projectelement , parent ) ;
} catch ( saxexception e ) {
string firstline = reader . readline ( ) ;
throw new illegalargumentexception ( "the inputstream doesn't support mark" ) ;
import org . xml . sax . inputsource ;
throw new indexoutofboundsexception ( ) ;
documentbuilderfactory factory = documentbuilderfactory . newinstance ( ) ;
document pomdomdoc = xmlhelper . parsetodom ( stream , res , new entityresolver ( ) {
private static string doctype = " < !doctype project system \ "m2 - entities . ent \ " > \ n" ;
inputstream stream = new adddtdfilterinputstream ( descriptorurl . openstream ( ) ) ;
public inputsource resolveentity ( string publicid , string systemid ) throws saxexception ,
if ( stream ! = null ) {
super ( in ) ;
throw e ;
} finally {
private static final class adddtdfilterinputstream extends filterinputstream {
return 0 ;
return nbrbytescopied ;
private adddtdfilterinputstream ( inputstream in ) throws ioexception {
try {
if ( !project . equals ( projectelement . getnodename ( ) ) ) {
string trimmed = firstline . trim ( ) ;
in . read ( ) ;
public int read ( byte [ ] b , int off , int len ) throws ioexception {
if ( !in . marksupported ( ) ) {
res . getname ( ) , 0 , 0 ) ;
} else if ( len = = 0 ) {
public int read ( ) throws ioexception {
if ( nbrbytescopied < len ) {
} ) ;
pomdomdoc = docbuilder . parse ( stream , res . getname ( ) ) ;
throw new saxparseexception ( "project must be the root tag" , res . getname ( ) ,
throw new nullpointerexception ( ) ;
docbuilder . setentityresolver ( entityresolver ) ;
} else if ( ( off < 0 ) | | ( off > b . length ) | | ( len < 0 ) | |
if ( count < prefix . length ) {
if ( entityresolver ! = null ) {
documentbuilder docbuilder = getdocbuilder ( entityresolver ) ;
int nbrbytesfromprefix = math . min ( prefix . length - count , len ) ;
for ( int i = 0 ; i < bytestoskip ; i + + ) {
( ( off + len ) > b . length ) | | ( ( off + len ) < 0 ) ) {
return prefix [ count + + ] ;
string xmldecl = trimmed . substring ( 0 , endindex + 2 ) ;
ioexception {
int nbrbytescopied = 0 ;
public static documentbuilder getdocbuilder ( entityresolver entityresolver ) {
factory . setvalidating ( false ) ;
in . reset ( ) ;
import java . io . inputstreamreader ;
private int count ;
} catch ( ioexception e ) {
import java . io . inputstream ;
int bytestoskip = 0 ;
public static document parsetodom ( inputstream stream , resource res , entityresolver entityresolver ) throws ioexception ,
return null ;
int endindex = trimmed . indexof ( " ? > " ) ;
return result ;
int result = super . read ( ) ;
system . arraycopy ( prefix , count , b , off , nbrbytesfromprefix ) ;
import org . xml . sax . entityresolver ;
prefix = ( xmldecl + " \ n" + doctype ) . getbytes ( ) ;
nbrbytescopied = nbrbytesfromprefix ;
bytestoskip = xmldecl . getbytes ( ) . length ;
if ( ( systemid ! = null ) & & systemid . endswith ( "m2 - entities . ent" ) ) {
if ( firstline ! = null ) {
if ( trimmed . startswith ( " < ? xml " ) ) {
stream . close ( ) ;
projectelement = pomdomdoc . getdocumentelement ( ) ;
private byte [ ] prefix = doctype . getbytes ( ) ;
docbuilder = factory . newdocumentbuilder ( ) ;
if ( b = = null ) {
import java . io . filterinputstream ;
ioexception ioe = new malformedurlexception ( "couldn't convert '"
url . getport ( ) , url . getpath ( ) , url . getquery ( ) , url . getref ( ) ) ;
return new url ( normalizetostring ( url ) ) ;
uri uri = new uri ( url . getprotocol ( ) , url . getuserinfo ( ) , url . gethost ( ) ,
}
return url . toexternalform ( ) ;
return escape pattern . matcher ( uristring ) . replaceall ( " % $1" ) ;
protected string normalizetostring ( url url ) throws ioexception {
if ( !"http" . equals ( url . getprotocol ( ) ) & & !"https" . equals ( url . getprotocol ( ) ) ) {
return url ;
throw ioe ;
string uristring = uri . tostring ( ) ;
import java . net . malformedurlexception ;
uristring = uristring . replaceall ( " \ \ + " , " % 2b" ) ;
import java . net . uri ;
import java . net . urisyntaxexception ;
private static final pattern escape pattern = pattern . compile ( " % 25 ( [ 0 - 9a - fa - f ] [ 0 - 9a - fa - f ] ) " ) ;
} catch ( urisyntaxexception e ) {
ioe . initcause ( e ) ;
+ url . tostring ( ) + "' to a valid uri" ) ;
import java . util . regex . pattern ;
protected url normalizetourl ( url url ) throws ioexception {
try {
in . mark ( mark ) ;
} catch ( saxexception e ) {
public static document parsetodom (
public inputsource resolveentity ( string publicid , string systemid )
pomreader . class . getresourceasstream ( "m2 - entities . ent" ) ) ;
return new inputsource (
| | ( ( off + len ) > b . length ) | | ( ( off + len ) < 0 ) ) {
private static final string doctype = " < !doctype project system \ "m2 - entities . ent \ " > \ n" ;
throws saxexception , ioexception {
throws ioexception , saxexception {
inputstream stream , resource res , entityresolver entityresolver )
private static final int mark = 10000 ;
} else if ( ( off < 0 ) | | ( off > b . length ) | | ( len < 0 )
import java . io . linenumberreader ;
count + = nbrbytescopied ;
linenumberreader reader = new linenumberreader ( new inputstreamreader ( in , "utf - 8" ) ) ;
nbrbytescopied + = in . read ( b , off + nbrbytescopied , len - nbrbytescopied ) ;
uri uri = new uri ( url . getprotocol ( ) , url . getuserinfo ( ) , url . gethost ( ) ,
}
return url . toexternalform ( ) ;
return escape pattern . matcher ( uristring ) . replaceall ( " % $1" ) ;
parentelement = getfirstchildelement ( projectelement , parent ) ;
string firstline = reader . readline ( ) ;
throw new illegalargumentexception ( "the inputstream doesn't support mark" ) ;
import org . xml . sax . inputsource ;
private static final string doctype = " < !doctype project system \ "m2 - entities . ent \ " > \ n" ;
throw new indexoutofboundsexception ( ) ;
document pomdomdoc = xmlhelper . parsetodom ( stream , res , new entityresolver ( ) {
uristring = uristring . replaceall ( " \ \ + " , " % 2b" ) ;
if ( ( name = = null ) & & ( url = = null ) ) {
inputstream stream = new adddtdfilterinputstream ( descriptorurl . openstream ( ) ) ;
import java . net . uri ;
if ( stream ! = null ) {
super ( in ) ;
private static final pattern escape pattern = pattern . compile ( " % 25 ( [ 0 - 9a - fa - f ] [ 0 - 9a - fa - f ] ) " ) ;
} catch ( urisyntaxexception e ) {
} finally {
private static final class adddtdfilterinputstream extends filterinputstream {
return 0 ;
return nbrbytescopied ;
private adddtdfilterinputstream ( inputstream in ) throws ioexception {
try {
ioexception ioe = new malformedurlexception ( "couldn't convert '"
string trimmed = firstline . trim ( ) ;
url . getport ( ) , url . getpath ( ) , url . getquery ( ) , url . getref ( ) ) ;
return new url ( normalizetostring ( url ) ) ;
throws saxexception , ioexception {
in . read ( ) ;
public int read ( byte [ ] b , int off , int len ) throws ioexception {
if ( !in . marksupported ( ) ) {
res . getname ( ) , 0 , 0 ) ;
} else if ( len = = 0 ) {
public int read ( ) throws ioexception {
protected string normalizetostring ( url url ) throws ioexception {
if ( nbrbytescopied < len ) {
} ) ;
return url ;
if ( !project . equals ( projectelement . getnodename ( ) ) & & !model . equals ( projectelement . getnodename ( ) ) ) {
throw new saxparseexception ( "project must be the root tag" , res . getname ( ) ,
throw new nullpointerexception ( ) ;
return new inputsource (
string uristring = uri . tostring ( ) ;
import java . net . urisyntaxexception ;
in . mark ( mark ) ;
if ( count < prefix . length ) {
ioe . initcause ( e ) ;
int nbrbytesfromprefix = math . min ( prefix . length - count , len ) ;
for ( int i = 0 ; i < bytestoskip ; i + + ) {
return prefix [ count + + ] ;
protected url normalizetourl ( url url ) throws ioexception {
private static final int mark = 10000 ;
return ( e ! = null ) & & "true" . equalsignorecase ( gettextcontent ( e ) ) ;
pomreader . class . getresourceasstream ( "m2 - entities . ent" ) ) ;
string xmldecl = trimmed . substring ( 0 , endindex + 2 ) ;
int nbrbytescopied = 0 ;
in . reset ( ) ;
private static final string model = "model" ;
import java . io . inputstreamreader ;
private int count ;
} catch ( ioexception e ) {
} else if ( ( off < 0 ) | | ( off > b . length ) | | ( len < 0 )
import java . io . inputstream ;
public inputsource resolveentity ( string publicid , string systemid )
int bytestoskip = 0 ;
return null ;
int endindex = trimmed . indexof ( " ? > " ) ;
return result ;
continue ;
int result = super . read ( ) ;
element e = getfirstchildelement ( depelement , optional ) ;
if ( !"http" . equals ( url . getprotocol ( ) ) & & !"https" . equals ( url . getprotocol ( ) ) ) {
name = "unknown license" ;
system . arraycopy ( prefix , count , b , off , nbrbytesfromprefix ) ;
import org . xml . sax . entityresolver ;
prefix = ( xmldecl + " \ n" + doctype ) . getbytes ( ) ;
throw ioe ;
nbrbytescopied = nbrbytesfromprefix ;
import java . net . malformedurlexception ;
bytestoskip = xmldecl . getbytes ( ) . length ;
if ( ( systemid ! = null ) & & systemid . endswith ( "m2 - entities . ent" ) ) {
if ( firstline ! = null ) {
if ( trimmed . startswith ( " < ? xml " ) ) {
stream . close ( ) ;
projectelement = pomdomdoc . getdocumentelement ( ) ;
private byte [ ] prefix = doctype . getbytes ( ) ;
lics . add ( new license ( name , url ) ) ;
| | ( ( off + len ) > b . length ) | | ( ( off + len ) < 0 ) ) {
+ url . tostring ( ) + "' to a valid uri" ) ;
if ( name = = null ) {
import java . util . regex . pattern ;
if ( b = = null ) {
import java . io . filterinputstream ;
}
} catch ( exception ignore ) {
href = new url ( href ) . getpath ( ) ;
continue ;
if ( href . startswith ( "http : " ) | | href . startswith ( "https : " ) ) {
href = href . substring ( url . getpath ( ) . length ( ) ) ;
if ( !href . startswith ( url . getpath ( ) ) ) {
try {
string acceptnamepattern = " . * ? "
namepattern = namepattern . replaceall ( " \ \ . " , " \ \ \ \ . " ) ;
pattern p = pattern . compile ( acceptnamepattern ) ;
+ " ] + ) " ) + " ( $ | " + filesep + " . * ) " ;
}
import org . apache . ivy . plugins . matcher . exactpatternmatcher ;
return "buildnumber - matcher" ;
private patternmatcher exact = new exactpatternmatcher ( ) ;
import org . apache . ivy . plugins . matcher . matcher ;
| | ( expression = = module )
return exact . getmatcher ( expression ) ;
import org . apache . ivy . plugins . matcher . regexppatternmatcher ;
private patternmatcher regexp = new exactorregexppatternmatcher ( ) ;
} ) ;
import org . apache . ivy . plugins . matcher . exactorregexppatternmatcher ;
module , branch , " . * " ) , new patternmatcher ( ) {
if ( ( expression = = organisation )
public matcher getmatcher ( string expression ) {
public string getname ( ) {
| | ( expression = = branch ) ) {
return regexp . getmatcher ( expression ) ;
if ( excluded . isempty ( ) ) {
public list / * < moduleid > * / getexcludedmodules ( ) {
for ( final iterator entiter = extras . entryset ( ) . iterator ( ) ; entiter . hasnext ( ) ; ) {
final string exclusionprefix = getdependencymgtextrainfoprefixforexclusion (
continue ;
"had the wrong number of parts ( should have 2 ) " + exclusion parts . length + " : " + full exclusion ) ;
if ( key . startswith ( exclusionprefix ) ) {
ivymoduledescriptor . addextrainfo ( exclusionprefix + index ,
for ( iterator itexcl = excluded . iterator ( ) ; itexcl . hasnext ( ) ; ) {
return dependency management + extra info delimiter + groupid
int index = 0 ;
final map . entry / * < string , string > * / ent = ( map . entry ) entiter . next ( ) ;
dep . getgroupid ( ) , dep . getartifactid ( ) ) ;
}
if ( exclusion parts . length ! = 2 ) {
list / * < moduleid > * / exclusions = getdependencymgtexclusions ( md , parts [ 1 ] , parts [ 2 ] ) ;
final moduleid excludedmodule = ( moduleid ) iter . next ( ) ;
final string artifactid ) {
exclusionids . add ( moduleid . newinstance ( exclusion parts [ 0 ] , exclusion parts [ 1 ] ) ) ;
string groupid , string artifaceid ) {
excluded = getdependencymgtexclusions ( ivymoduledescriptor , dep . getgroupid ( ) , dep . getartifactid ( ) ) ;
private static list / * < moduleid > * / getdependencymgtexclusions (
final string groupid ,
final list / * < moduleid > * / exclusionids = new linkedlist / * < moduleid > * / ( ) ;
private static string getdependencymgtextrainfoprefixforexclusion (
return collections . empty list ;
list / * < moduleid > * / excluded = dep . getexcludedmodules ( ) ;
message . error ( "what seemed to be a dependency management extra info exclusion " +
final map / * < string , string > * / extras = descriptor . getextrainfo ( ) ;
return exclusionids ;
for ( final iterator iter = dep . getexcludedmodules ( ) . iterator ( ) ; iter . hasnext ( ) ; ) {
final string full exclusion = ( string ) ent . getvalue ( ) ;
+ extra info delimiter + artifaceid + extra info delimiter + "exclusion " ;
excludedmodule . getorganisation ( ) + extra info delimiter + excludedmodule . getname ( ) ) ;
index + = 1 ;
import java . util . linkedlist ;
result . add ( new defaultpomdependencymgt ( parts [ 1 ] , parts [ 2 ] , version , scope , exclusions ) ) ;
final string [ ] exclusion parts = full exclusion . split ( extra info delimiter ) ;
final string key = ( string ) ent . getkey ( ) ;
if ( !dep . getexcludedmodules ( ) . isempty ( ) ) {
groupid , artifactid ) ;
final moduledescriptor descriptor ,
}
if ( jar packagings . contains ( type ) ) {
string ext = type ;
ext = "jar" ;
type , ext , null , extraatt ) ;
& & (
| | node . hasanymergedusagewithtransitivedependency ( rootmoduleconf ) )
node . getdependencydescriptor ( getparentnode ( ) ) . istransitive ( )
}
configuration conf = md . getconfiguration ( callerconf ) ;
private void updateconfs ( string callerconf , string [ ] dependencyconfs ) {
if ( confextends ! = null ) {
import java . util . arraylist ;
updateconfs ( callerconf , dependencyconfs ) ;
import org . apache . ivy . core . module . descriptor . configuration ;
string [ ] confextends = conf . getextends ( ) ;
string requestedconf , string [ ] dependencyconfs , dependencydescriptor dd ) {
caller . addconfiguration ( requestedconf , dependencyconfs ) ;
addconfiguration ( confextends [ i ] , dependencyconfs ) ;
import java . util . list ;
for ( int i = 0 ; i < confextends . length ; i + + ) {
new authscope ( proxyhost , proxyport , proxyrealm ) ,
list authprefs = new arraylist ( 3 ) ;
new authscope ( c . gethost ( ) , authscope . any port , c . getrealm ( ) ) ,
proxyrealm = system . getproperty ( "http . auth . ntlm . domain" ) ;
"apache ivy / " + ivy . getivyversion ( ) ) ;
message . verbose ( "proxy configured : host = " + proxyhost + " port = " + proxyport + " user = "
+ " did not indicate a success . " + " see log for more detail . " ) ;
import org . apache . ivy . util . hostutil ;
hostutil . getlocalhostname ( ) , proxyrealm ) ) ;
+ "a proxy server that is not well configured . " ) ;
+ proxyusername ) ;
authprefs . add ( authpolicy . ntlm ) ;
throw new ioexception ( "the http response code for " + url
+ "use ivy with sufficient security permissions . " ) ;
+ "commons - httpclient headmethod . please use commons - httpclient 3 . 0 or "
import org . apache . commons . httpclient . ntcredentials ;
new ntcredentials ( proxyusername , proxypasswd ,
final multithreadedhttpconnectionmanager connmanager =
new multithreadedhttpconnectionmanager ( ) ;
import org . apache . commons . httpclient . auth . authscope ;
throw new ioexception ( "the http response code for " + src
httpclient . getparams ( ) . setparameter ( "http . useragent" ,
httpclient . getstate ( ) . setproxycredentials (
hostutil . getlocalhostname ( ) , c . getrealm ( ) ) ) ;
new ntcredentials ( c . getusername ( ) , c . getpasswd ( ) ,
}
pubdate = ivy . date format . parse ( pubdateattr ) ;
string pubdateattr = attributes . getvalue ( "pubdate" ) ;
if ( pubdateattr ! = null ) {
tokens . putall ( extraartifactattributes ) ;
return substitute ( pattern , mrid , artifact , ( string ) null , ( artifactorigin ) null ) ;
artifact , type , ext ) ) ;
string revision , string artifactname , string artifacttype , string artifactext ,
this . extraartifactattributes = extraartifactattributes ;
if ( origin = = null ) {
return substitute ( pattern , org , module , ( string ) null , revision , artifact , type , ext , ( string ) null ,
private map extramoduleattributes ;
}
revision , extramoduleattributes ) ;
return substitute ( pattern , artifact . getmodulerevisionid ( ) , artifact , conf , ( artifactorigin ) null ) ;
string artifact , string type , string ext , string conf , map extramoduleattributes ,
return substitute ( pattern , org , module , ( string ) null , revision , artifact , type , ext , conf ,
map extramoduleattributes , map extraartifactattributes ) {
private map extraartifactattributes ;
} else {
tokens . putall ( extramoduleattributes ) ;
null , modulerevision . getattributes ( ) , null ) ;
( artifactorigin ) null , extramoduleattributes , extraartifactattributes ) ;
map extraartifactattributes ) {
mrevid . getrevision ( ) , "" , "" , ext , conf , mrevid . getattributes ( ) , null ) ;
map tokens = new hashmap ( ) ;
this . extramoduleattributes = extramoduleattributes ;
if ( extraartifactattributes ! = null ) {
return substitute ( pattern , artifact . getmodulerevisionid ( ) , artifact , ( string ) null , origin ) ;
module , branch , revision , artifact , type , ext , extramoduleattributes , extraartifactattributes ) ) ;
( artifactorigin ) null , ( map ) null , ( map ) null ) ;
tokens . put ( original artifactname key , new originalartifactnamevalue ( origin ) ) ;
this . mr . getattributes ( ) , null ) ;
origin , mrid . getextraattributes ( ) , artifact . getextraattributes ( ) ) ;
tokens . put ( original artifactname key , new originalartifactnamevalue ( org ,
artifactorigin origin , map extramoduleattributes , map extraartifactattributes ) {
if ( extramoduleattributes ! = null ) {
artifactext , extraartifactattributes ) ;
}
if ( branchconstraint = = null ) {
import org . apache . ivy . core . settings . ivysettings ;
branchconstraint = settings . getdefaultbranch ( moduleid . newinstance ( org , name ) ) ;
}
instream . close ( ) ;
inputstream instream = null ;
instream = conn . getinputstream ( ) ;
} finally {
inputstream instream = conn . getinputstream ( ) ;
if ( instream ! = null ) {
} catch ( ioexception e ) {
inputstream errstream = conn . geterrorstream ( ) ;
errstream . close ( ) ;
try {
httpclient . getstate ( ) . setcredentials (
+ " ( resolved by " + rmr . getresolver ( ) . getname ( )
removesavedartifactorigin ( transformedartifact ) ;
cachedpublicationdate =
if ( !rmr . getdescriptor ( ) . isdefault ( ) & & replastmodified < = cachelastmodified ) {
if ( rmr . getdescriptor ( ) . isdefault ( ) & & rmr . getresolver ( ) ! = resolver ) {
moduledescriptorparser parser = moduledescriptorparserregistry
message . debug ( "deleting " + artfile ) ;
message . verbose ( mrid + " has changed : deleting old artifacts" ) ;
message . verbose ( "" + getname ( ) + " : revision in cache : " + mrid ) ;
if ( !moduleartifact . ismetadata ( ) ) {
if ( ischanging ( dd , mrid , options ) ) {
}
message . verbose ( "" + getname ( ) + " : found revision in cache : " + mrid
long cachelastmodified = rmr . getdescriptor ( ) . getlastmodified ( ) ;
+ mrid ) ;
file artfile = getarchivefileincache ( transformedartifact , origin , false ) ;
long replastmodified = mdref . getlastmodified ( ) ;
} else {
resolvedmodulerevision rmr = dofindmoduleincache ( mrid , options , null ) ;
if ( rmr ! = null ) {
. getinstance ( ) . getparser ( mdref . getresource ( ) ) ;
return rmr ;
backupresourcedownloader backupdownloader = new backupresourcedownloader ( downloader ) ;
moduleartifact , options . getnamespace ( ) . gettosystemtransformer ( ) ) ;
artifact originalmetadataartifact = getoriginalmetadataartifact ( moduleartifact ) ;
message . error ( "couldn't delete outdated artifact from cache : " + artfile ) ;
rmr . getdescriptor ( ) . getresolvedpublicationdate ( ) ;
message . verbose ( "" + getname ( ) + " : revision in cache ( not updated ) : "
if ( !ischeckmodified ( dd , mrid , options ) & & !ischanging ( dd , mrid , options ) ) {
return null ;
long repolastmodified = mdref . getlastmodified ( ) ;
if ( !artfile . delete ( ) ) {
message . verbose ( "" + getname ( ) + " : revision in cache is not up to date : "
artifactorigin origin = getsavedartifactorigin ( transformedartifact ) ;
artifact transformedartifact = namespacehelper . transform (
if ( artfile . exists ( ) & & repolastmodified > artfile . lastmodified ( ) ) {
rmr . getreport ( ) . setsearched ( true ) ;
+ " ) : but it's a default one , maybe we can find a better one" ) ;
try {
}
modulerevisionid current = ( modulerevisionid ) entry . getkey ( ) ;
& & current . getrevision ( ) . equals ( mrid . getrevision ( ) ) ) {
node = data . getnode ( ) ;
if ( current . getmoduleid ( ) . equals ( mrid . getmoduleid ( ) )
if ( node = = null ) {
break ;
for ( iterator it = visitdata . entryset ( ) . iterator ( ) ; it . hasnext ( ) ; ) {
map . entry entry = ( entry ) it . next ( ) ;
visitdata data = ( visitdata ) entry . getvalue ( ) ;
}
dep . getartifactid ( ) ,
list depmgt = pommoduledescriptorbuilder . getdependencymanagements ( importdescr ) ;
modulerevisionid importmodrevid = modulerevisionid . newinstance (
dep . getversion ( ) ) ;
moduledescriptor importdescr = importmodule . getdescriptor ( ) ;
if ( "import" . equals ( dep . getscope ( ) ) ) {
mdbuilder . adddependencymgt ( ( pomdependencymgt ) it2 . next ( ) ) ;
+ " import = " + importmodrevid ) ;
resolvedmodulerevision importmodule = parseotherpom ( ivysettings ,
throw new ioexception ( "impossible to import module for " + descriptorurl + " . "
dep . getgroupid ( ) ,
} else {
for ( iterator it2 = depmgt . iterator ( ) ; it2 . hasnext ( ) ; ) {
if ( importmodule ! = null ) {
importmodrevid ) ;
mdbuilder . adddependencymgt ( dep ) ;
}
private void classpathstarted ( map attributes ) throws ioexception {
throw new filenotfoundexception ( settingsfile . getabsolutepath ( ) ) ;
ivy . setsettingsvariables (
url = urlfromfileattribute ( file ) ;
} catch ( urisyntaxexception e ) {
throw new filenotfoundexception ( incfile . getabsolutepath ( ) ) ;
checks . checkabsolute ( settingsfile ,
"settings include path" ) ) ;
file settingsfile = new file ( new uri ( settingsurl . toexternalform ( ) ) ) ;
checks . checkabsolute ( settingsurl . getpath ( ) ,
try {
import java . lang . reflect . field ;
field f = authenticator . class . getdeclaredfield ( "theauthenticator" ) ;
if ( ! ( original instanceof ivyauthenticator ) ) {
}
original = ( authenticator ) f . get ( null ) ;
authenticator original = null ;
authenticator . setdefault ( new ivyauthenticator ( original ) ) ;
this . original = original ;
public static void install ( ) {
getrequestingsite ( ) , getrequestingport ( ) , getrequestingprotocol ( ) ,
authenticator . setdefault ( this ) ;
authenticator . setdefault ( original ) ;
private ivyauthenticator ( authenticator original ) {
result = authenticator . requestpasswordauthentication ( getrequestinghost ( ) ,
private authenticator original ;
} catch ( throwable t ) {
getrequestingprompt ( ) , getrequestingscheme ( ) ) ;
if ( ( result = = null ) & & ( original ! = null ) ) {
message . debug ( "error occured while getting the original authenticator!" ) ;
} finally {
try {
) ;
string parentrevision = attributes . getvalue ( "revision" ) ;
if ( dep . getdependencyrevisionid ( ) . getbranch ( ) ! = null ) {
if ( includes . length > 0 & & departifacts . length = = 0 ) {
moduledescriptor parent = null ;
moduledescriptor md ;
protected void mergewithothermoduledescriptor ( list / * < string > * / extendtypes ,
md . adddependency ( dependencydescriptor ) ;
+ " ; "
for ( int j = 0 ; j < exts . length ; j + + ) {
out . print ( " extendtypes = \ "" + stringutils . join ( parent . getextendstypes ( ) , " , " ) + " \ "" ) ;
dependencyartifactdescriptor [ ] departifacts = dep . getalldependencyartifacts ( ) ;
import java . util . linkedhashmap ;
out . print ( " changing = \ "" + dep . ischanging ( ) + " \ "" ) ;
}
dup . putall ( inherited ) ;
protected string getdefaultparentlocation ( ) {
resolvedata data = ivycontext . getcontext ( ) . getresolvedata ( ) ;
urlresource res = new urlresource ( url ) ;
moduleid pid = parent . getmodulerevisionid ( ) . getmoduleid ( ) ;
modulerevisionid parentmrid = new modulerevisionid ( parentmoduleid , parentrevision ) ;
out . print ( " description = \ ""
} else {
printconfiguration ( conf , out ) ;
dep . getdynamicconstraintdependencyrevisionid ( ) . getrevision ( ) ) + " \ "" ) ;
+ parentorganisation + "#" + parentmodule + " ; " + parentrevision
+ xmlhelper . escape ( dep . getdependencyrevisionid ( ) . getbranch ( ) ) + " \ "" ) ;
out . print ( " force = \ "" + dep . isforce ( ) + " \ "" ) ;
throw new parseexception ( "unable to parse included ivy file for "
protected void mergedescription ( string description ) {
if ( dep . ischanging ( ) ) {
+ parentorganisation + "#" + parentmodule + " ; " + parentrevision , 0 ) ;
includerule [ ] includes = dep . getallincluderules ( ) ;
excluderule [ ] excludes = dep . getallexcluderules ( ) ;
string current = getmd ( ) . getdescription ( ) ;
& & !parent . getresource ( ) . getname ( ) . equals ( ivyfileincache . touri ( ) . tostring ( ) ) ) {
moduledescriptor parent ) {
for ( int k = 0 ; k < depconfs . length ; k + + ) {
dependencydescriptor dependencydescriptor = dependencies [ i ] ;
out . print ( " visibility = \ ""
printwriter out ) {
descriptor . setnamespace ( ( ( defaultmoduledescriptor ) parent ) . getnamespace ( ) ) ;
parent . getmodulerevisionid ( ) ,
extendsstarted ( attributes ) ;
url parenturl = cachefile . touri ( ) . tourl ( ) ;
parent = parser . parsedescriptor ( getsettings ( ) , url , isvalidate ( ) ) ;
list / * < string > * / extendtypes = arrays . aslist ( extendtype . split ( " , " ) ) ;
out . println ( " > " ) ;
. equals ( dep . getdependencyrevisionid ( ) ) ) {
"trying to parse included ivy file by asking repository for module : "
out . print ( " < dependency" ) ;
getmd ( ) . setdescription ( description ) ;
for ( int i = 0 ; i < dependencies . length ; i + + ) {
return " . . / ivy . xml" ;
descriptor . setmodulerevisionid ( mergedmrid ) ;
parent = null ;
resolutioncachemanager cachemanager = settings . getresolutioncachemanager ( ) ;
if ( current = = null | | current . trim ( ) . length ( ) = = 0 ) {
out . print ( "" ) ;
try {
attributes . getvalue ( "location" ) ,
string parentorganisation = attributes . getvalue ( "organisation" ) ;
parent . toivyfile ( ivyfileincache ) ;
message . warn ( "unable to parse included ivy file for " + parentorganisation
printdependencyincluderules ( md , out , includes ) ;
for ( int i = 0 ; i < configurations . length ; i + + ) {
string [ ] modconfs = dep . getmoduleconfigurations ( ) ;
parent = parseotherivyfileonfilesystem ( location ) ;
out . println ( " < / dependency > " ) ;
data = new resolvedata ( engine , options ) ;
+ "#" + parentmodule + " ; " + parentrevision ) ;
protected static void printconfiguration ( configuration conf , printwriter out ) {
out . print ( " deprecated = \ "" + xmlhelper . escape ( conf . getdeprecated ( ) ) + " \ "" ) ;
out . print ( " \ "" ) ;
if ( dep . isforce ( ) ) {
import org . apache . ivy . core . resolve . resolvedata ;
url url = null ;
message . debug ( "merging configuration with : " + configuration . getname ( ) ) ;
getmd ( ) . addinheriteddescriptor ( ed ) ;
throw new parseexception ( "unable to find " + parentmrid . tostring ( ) , 0 ) ;
linkedhashmap dup = new linkedhashmap ( inherited . size ( ) + overrides . size ( ) ) ;
protected void mergeinfo ( moduledescriptor parent ) {
mergewithothermoduledescriptor ( extendtypes , parent ) ;
message . warn ( "unable to parse included ivy file " + location + " : "
mergevalue ( parentmrid . getbranch ( ) , currentmrid . getbranch ( ) ) ,
import org . apache . ivy . core . cache . resolutioncachemanager ;
out . print ( " , " ) ;
if ( includes . length + excludes . length + departifacts . length = = 0 ) {
return parent ;
message . debug ( "trying to load included ivy file from cache" ) ;
return dup ;
string location = parent . getlocation ( ) ;
modulerevisionid mrid = parent . getparentrevisionid ( ) ;
+ parentorganisation
resolveoptions options = new resolveoptions ( ) ;
if ( conf . getdeprecated ( ) ! = null ) {
+ xmlhelper . escape ( conf . getdescription ( ) ) + " \ "" ) ;
resolveengine engine = ivycontext . getcontext ( ) . getivy ( ) . getresolveengine ( ) ;
modulerevisionid . newinstance ( parentmrid , ivy . getworkingrevision ( ) ) ) ;
+ e . getmessage ( ) ) ;
out . print ( " branchconstraint = \ "" + xmlhelper . escape (
string parentmodule = attributes . getvalue ( "module" ) ;
| | md . getinheriteddescriptors ( ) . length > 0 ;
mergeinfo ( parent ) ;
if ( parent = = null ) {
out . print ( " name = \ ""
if ( parent . getresource ( ) ! = null
. getresolvedmodulerevisionid ( ) ) ;
resolvedmodulerevision othermodule = resolver . getdependency ( dd , data ) ;
dependencydescriptor dd = new defaultdependencydescriptor ( parentmrid , true ) ;
md = parseotherivyfileonfilesystem ( parenturl . tostring ( ) ) ;
if ( extendtypes . contains ( "description" ) ) {
defaultmoduledescriptor descriptor = getmd ( ) ;
mergeall ( parent ) ;
modulerevisionid parentmrid = parent . getmodulerevisionid ( ) ;
printextraattributes ( conf , out , " " ) ;
return md ;
if ( data = = null ) {
modulerevisionid mergedmrid = modulerevisionid . newinstance (
if ( exts . length > 0 ) {
+ "#"
if ( extendtypes . contains ( "dependencies" ) ) {
out . print ( " < extends organisation = \ "" + xmlhelper . escape ( mrid . getorganisation ( ) ) + " \ ""
for ( int j = 0 ; j < modconfs . length ; j + + ) {
printdependencyexcluderules ( md , out , excludes ) ;
res ) ;
+ " module = \ "" + xmlhelper . escape ( mrid . getname ( ) ) + " \ ""
if ( extendtypes . contains ( "configurations" ) ) {
message . debug ( "trying to parse included ivy file : " + location ) ;
for ( int i = 0 ; i < parents . length ; i + + ) {
printdependency ( md , dep , out ) ;
moduleid parentmoduleid = new moduleid ( parentorganisation , parentmodule ) ;
import org . apache . ivy . core . resolve . resolveoptions ;
mergevalue ( parentmrid . getrevision ( ) , currentmrid . getrevision ( ) ) ,
throw new parseexception ( "unable to create cache file for "
+ xmlhelper . escape ( dep . getdependencyrevisionid ( ) . getrevision ( ) ) + " \ "" ) ;
message . error ( e . getlocalizedmessage ( ) ) ;
out . print ( " revconstraint = \ "" + xmlhelper . escape (
printdependencyartefacts ( md , out , departifacts ) ;
import org . apache . ivy . util . stringutils ;
out . print ( " org = \ ""
return othermodule . getdescriptor ( ) ;
protected void mergeconfigurations ( modulerevisionid sourcemrid , configuration [ ] configurations ) {
printextraattributes ( dep , out , " " ) ;
if ( extendtypes . contains ( "all" ) ) {
+ parentmodule
mergedependencies ( parent . getdependencies ( ) ) ;
import org . apache . ivy . core . module . descriptor . * ;
descriptor . setresolvedmodulerevisionid ( mergedmrid ) ;
string extendtype = attributes . getvalue ( "extendtype" ) ! = null ? attributes . getvalue (
modulerevisionid currentmrid = descriptor . getmodulerevisionid ( ) ;
+ " reason : " + e . getlocalizedmessage ( ) , 0 ) ;
message . debug ( "trying to load included ivy file from " + url . tostring ( ) ) ;
| | md . getlicenses ( ) . length > 0
if ( !dep . getdynamicconstraintdependencyrevisionid ( )
modulerevisionid sourcemrid = parent . getmodulerevisionid ( ) ;
moduleid expected = new moduleid ( parentorganisation , parentmodule ) ;
protected moduledescriptor parseotherivyfile ( string parentorganisation ,
+ " revision = \ "" + xmlhelper . escape ( mrid . getrevision ( ) ) + " \ "" ) ;
if ( dep . getdynamicconstraintdependencyrevisionid ( ) . getbranch ( ) ! = null ) {
if ( k + 1 < depconfs . length ) {
import org . apache . ivy . plugins . parser . moduledescriptorparserregistry ;
protected void mergedependencies ( dependencydescriptor [ ] dependencies ) {
out . print ( " rev = \ ""
if ( !expected . equals ( pid ) ) {
out . print ( " conf = \ "" ) ;
throws parseexception , ioexception {
} catch ( ioexception e ) {
moduledescriptorparser parser = moduledescriptorparserregistry . getinstance ( ) . getparser (
string location = attributes . getvalue ( "location" ) ! = null ? attributes
if ( excludes . length > 0 & & includes . length = = 0 & & departifacts . length = = 0 ) {
options . setdownload ( false ) ;
message . debug ( "merging dependency with : "
import org . apache . ivy . core . resolve . resolvedmodulerevision ;
return null ;
if ( !dep . istransitive ( ) ) {
mergeconfigurations ( sourcemrid , parent . getconfigurations ( ) ) ;
out . print ( " location = \ "" + xmlhelper . escape ( location ) + " \ "" ) ;
file cachefile = settings . getresolutioncachemanager ( ) . getresolvedivyfileincache (
defaultextendsdescriptor ed = new defaultextendsdescriptor (
if ( descriptor . getnamespace ( ) = = null & & parent instanceof defaultmoduledescriptor ) {
} else if ( state = = state . info & & "extends" . equals ( qname ) ) {
out . print ( xmlhelper . escape ( exts [ j ] ) ) ;
+ dependencydescriptor . getdependencyrevisionid ( ) . tostring ( ) ) ;
extendsdescriptor [ ] parents = md . getinheriteddescriptors ( ) ;
} catch ( parseexception e ) {
protected void extendsstarted ( attributes attributes ) throws parseexception {
dup . putall ( overrides ) ;
out . print ( " extends = \ "" ) ;
+ parentrevision ) ;
descriptor . setstatus ( mergevalue ( parent . getstatus ( ) , descriptor . getstatus ( ) ) ) ;
if ( location ! = null ) {
currentmrid . getname ( ) ,
url = getsettings ( ) . getrelativeurlresolver ( ) . geturl ( descriptorurl , location ) ;
mergevalues ( parentmrid . getqualifiedextraattributes ( ) ,
dep . getdynamicconstraintdependencyrevisionid ( ) . getbranch ( ) ) + " \ "" ) ;
import org . apache . ivy . core . module . id . modulerevisionid ;
private static map mergevalues ( map inherited , map overrides ) {
mergevalue ( parentmrid . getorganisation ( ) , currentmrid . getorganisation ( ) ) ,
if ( othermodule = = null ) {
md . addconfiguration ( new configuration ( configuration , sourcemrid ) ) ;
if ( j + 1 < modconfs . length ) {
if ( resolver = = null ) {
message . verbose ( "ignoring parent ivy file " + location + " ; expected "
. getvalue ( "location" ) : getdefaultparentlocation ( ) ;
out . println ( " / > " ) ;
+ xmlhelper . escape ( conf . getvisibility ( ) . tostring ( ) ) + " \ "" ) ;
file ivyfileincache = cachemanager . getresolvedivyfileincache ( parent
string parentmodule , string parentrevision ) throws parseexception {
mergedescription ( parent . getdescription ( ) ) ;
currentmrid . getqualifiedextraattributes ( ) )
out . print ( " ; " ) ;
message . debug (
+ expected + " but found " + pid ) ;
configuration configuration = configurations [ i ] ;
protected static void printdependency ( moduledescriptor md , dependencydescriptor dep ,
string [ ] depconfs = dep . getdependencyconfigurations ( modconfs [ j ] ) ;
dependencydescriptor dep = dds [ i ] ;
configuration conf = confs [ i ] ;
import org . apache . ivy . plugins . resolver . dependencyresolver ;
mergeconfigurations ( parent . getmodulerevisionid ( ) , parent . getconfigurations ( ) ) ;
+ xmlhelper . escape ( dep . getdependencyrevisionid ( ) . getname ( ) ) + " \ "" ) ;
if ( j + 1 < exts . length ) {
out . print ( " branch = \ ""
"extendtype" ) . tolowercase ( ) : "all" ;
( string [ ] ) extendtypes . toarray ( new string [ extendtypes . size ( ) ] ) ) ;
+ xmlhelper . escape ( dep . getdependencyrevisionid ( ) . getorganisation ( ) ) + " \ "" ) ;
out . print ( " transitive = \ "" + dep . istransitive ( ) + " \ "" ) ;
out . print ( xmlhelper . escape ( depconfs [ k ] ) ) ;
parent = parseotherivyfile ( parentorganisation , parentmodule , parentrevision ) ;
out . print ( xmlhelper . escape ( modconfs [ j ] ) + " - > " ) ;
dependencyresolver resolver = getsettings ( ) . getresolver ( parentmrid ) ;
if ( extendtypes . contains ( "info" ) ) {
defaultmoduledescriptor md = getmd ( ) ;
if ( conf . getdescription ( ) ! = null ) {
return override = = null ? inherited : override ;
string [ ] exts = conf . getextends ( ) ;
out . print ( " name = \ "" + xmlhelper . escape ( conf . getname ( ) ) + " \ "" ) ;
parent . getresolvedmodulerevisionid ( ) ,
protected moduledescriptor parseotherivyfileonfilesystem ( string location )
if ( departifacts . length > 0 ) {
private static string mergevalue ( string inherited , string override ) {
extendsdescriptor parent = parents [ i ] ;
if ( cachefile . exists ( ) & & cachefile . length ( ) > 0 ) {
out . print ( " < conf" ) ;
import org . apache . ivy . core . resolve . resolveengine ;
protected void mergeall ( moduledescriptor parent ) {
return !latestlowest . equals ( askedmrid . getrevision ( ) ) ;
import org . apache . ivy . core . module . status . status ;
list statuses = statusmanager . getcurrent ( ) . getstatuses ( ) ;
import java . util . list ;
string latestlowest = "latest . " + lowest . getname ( ) ;
status lowest = ( status ) statuses . get ( statuses . size ( ) - 1 ) ;
if ( "gzip" . equals ( srcconn . getcontentencoding ( ) ) ) {
instream = srcconn . getinputstream ( ) ;
httpclient . getparams ( ) . setparameter ( httpmethodparams . user agent ,
inputstream instream ;
if ( ( contentencodings . length > 0 ) & & "gzip" . equals ( contentencodings [ 0 ] . getvalue ( ) ) ) {
fileutil . copy ( instream , dest , l ) ;
is = get . getresponsebodyasstream ( ) ;
}
conn . setrequestproperty ( "accept - encoding" , "gzip" ) ;
} else {
import java . util . zip . gzipinputstream ;
if ( "gzip" . equals ( conn . getcontentencoding ( ) ) ) {
instream = new gzipinputstream ( srcconn . getinputstream ( ) ) ;
instream = new gzipinputstream ( conn . getinputstream ( ) ) ;
header [ ] contentencodings = get . getresponseheaders ( "content - encoding" ) ;
inputstream is ;
get . setrequestheader ( "accept - encoding" , "gzip" ) ;
fileutil . copy ( is , dest , l ) ;
srcconn . setrequestproperty ( "accept - encoding" , "gzip" ) ;
is = new gzipinputstream ( get . getresponsebodyasstream ( ) ) ;
import org . apache . commons . httpclient . params . httpmethodparams ;
instream = conn . getinputstream ( ) ;
contentlength = - 1 ;
protected inputstream getdecodinginputstream ( string encoding , inputstream in )
} else if ( "deflate" . equals ( encoding ) ) {
return result ;
bstream . reset ( ) ;
import java . io . bufferedinputstream ;
inflater inflater = new inflater ( ) ;
inflater . end ( ) ;
}
import java . util . zip . dataformatexception ;
throws ioexception {
bstream . mark ( 100 ) ;
inflater . inflate ( new byte [ 1000 ] ) ;
} else {
import java . util . zip . inflaterinputstream ;
import java . util . zip . gzipinputstream ;
int nbbytes = bstream . read ( bytes ) ;
result = new inflaterinputstream ( bstream ) ;
result = in ;
byte [ ] bytes = new byte [ 100 ] ;
if ( "gzip" . equals ( encoding ) ) {
result = new gzipinputstream ( in ) ;
inputstream result = null ;
inflater . setinput ( bytes , 0 , nbbytes ) ;
import java . io . inputstream ;
result = new inflaterinputstream ( bstream , new inflater ( true ) ) ;
} finally {
} catch ( dataformatexception e ) {
import java . util . zip . inflater ;
bufferedinputstream bstream = new bufferedinputstream ( in ) ;
try {
inputstream stream = new adddtdfilterinputstream ( res . openstream ( ) ) ;
pomreader domreader = new pomreader ( res ) ;
public pomreader ( resource res ) throws ioexception , saxexception {
throw new ioexception ( "impossible to load parent for " + res . getname ( ) + " . "
throw new ioexception ( "impossible to import module for " + res . getname ( ) + " . "
map . entry / * < string , string > * / ent = ( map . entry ) entiter . next ( ) ;
list / * < moduleid > * / exclusionids = new linkedlist / * < moduleid > * / ( ) ;
string key = ( string ) ent . getkey ( ) ;
message . error ( wrong number of parts msg + exclusionparts . length + " : "
string artifactid ) {
if ( exclusionparts . length ! = 2 ) {
exclusionids . add ( moduleid . newinstance ( exclusionparts [ 0 ] , exclusionparts [ 1 ] ) ) ;
string groupid ,
map / * < string , string > * / extras = descriptor . getextrainfo ( ) ;
moduledescriptor descriptor ,
string fullexclusion = ( string ) ent . getvalue ( ) ;
string [ ] exclusionparts = fullexclusion . split ( extra info delimiter ) ;
+ fullexclusion ) ;
string exclusionprefix = getdependencymgtextrainfoprefixforexclusion (
}
forcedrevisions . put ( dependencies [ i ] . getmoduleid ( ) , dependencies [ i ] . getresolvedid ( ) ) ;
modulerevisionid forcedrevisionid = ( modulerevisionid )
& & dependencies [ i ] . getmodulerevision ( ) . isforce ( )
import java . util . map ;
depdescriptor = null ;
map forcedrevisions = new hashmap ( ) ;
for ( int i = 0 ; i < dependencies . length ; i + + ) {
props . put ( deprevisionid . encodetostring ( ) , rev + " " + status + " " + forcedrev ) ;
forcedrevisions . get ( dependencies [ i ] . getmoduleid ( ) ) ;
& & dependencies [ i ] . getmodulerevision ( ) . isforce ( ) ) {
& & !settings . getversionmatcher ( ) . isdynamic ( deprevisionid ) ) {
string forcedrev = forcedrevisionid = = null ? rev : forcedrevisionid . getrevision ( ) ;
depresolvedid = deprevisionid ;
& & !depresolvedid . equals ( deprevisionid )
import java . util . hashmap ;
if ( dependencies [ i ] . getmodulerevision ( ) ! = null
matcher m = p . matcher ( path . substring ( root . length ( ) + 1 ) ) ;
namepattern = ivypatternhelper . substitutetoken ( namepattern , token , " ( . + ) " ) ;
pattern p = pattern . compile ( namepattern ) ;
conf , artifact . getartifactorigin ( ) ) ;
}
string org = mrevid . getorganisation ( ) = = null ? null : mrevid . getorganisation ( ) . replace ( ' . ' , ' / ' ) ;
if ( retrieveoptions . dirmode flat . equals ( options . getdirmode ( ) ) ) {
destfilename = ivypatternhelper . substitute ( destpattern ,
artifact . getartifactorigin ( ) , mrid . getextraattributes ( ) , artifact . getartifact ( ) . getextraattributes ( ) ) ;
modulerevisionid mrevid = artifact . getartifact ( ) . getmodulerevisionid ( ) ;
mrevid . getrevision ( ) , artifact . getname ( ) , artifact . gettype ( ) , artifact . getext ( ) , conf ,
string destfilename ;
destfilename = ivypatternhelper . substitute ( destpattern , org , mrevid . getname ( ) , mrevid . getbranch ( ) ,
} else {
artifact . getartifact ( ) . getmodulerevisionid ( ) , artifact . getartifact ( ) ,
} else if ( retrieveoptions . dirmode tree . equals ( options . getdirmode ( ) ) ) {
throw new illegalargumentexception ( "unsupported dirmode : " + options . getdirmode ( ) ) ;
public static file normalize ( final string path ) {
root + = sep ;
while ( tok . hasmoretokens ( ) ) {
int colon = path . indexof ( ' : ' ) ;
} else if ( path . length ( ) > 1 & & path . charat ( 1 ) = = sep ) {
continue ;
int nextsep = path . indexof ( sep , 2 ) ;
root = path . substring ( 0 , next ) ;
for ( int i = 0 ; i < s . size ( ) ; i + + ) {
path = path . replace ( ' / ' , sep ) . replace ( ' \ \ ' , sep ) ;
next = ( ca [ next ] = = sep ) ? next + 1 : next ;
stack s = new stack ( ) ;
s . pop ( ) ;
stringtokenizer tok = new stringtokenizer ( dissect [ 1 ] , file . separator ) ;
}
char [ ] ca = path . tochararray ( ) ;
if ( ca [ i ] ! = sep | | ca [ i - 1 ] ! = sep ) {
path = path . substring ( root . length ( ) ) ;
int next = colon + 1 ;
import java . util . stringtokenizer ;
path = path . substring ( 1 ) ;
string [ ] dissect = dissect ( path ) ;
stringbuffer sbpath = new stringbuffer ( ) ;
file result = new file ( filename ) ;
} else {
nextsep = path . indexof ( sep , nextsep + 1 ) ;
/ / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /
string thistoken = tok . nexttoken ( ) ;
s . push ( dissect [ 0 ] ) ;
return new string [ ] { root , path } ;
if ( s . size ( ) < 2 ) {
stringbuffer sb = new stringbuffer ( ) ;
if ( colon > 0 ) {
root = file . separator ;
private static string [ ] dissect ( string path ) {
for ( int i = next ; i < ca . length ; i + + ) {
string root = null ;
if ( " . . " . equals ( thistoken ) ) {
return normalize ( result . getpath ( ) ) ;
sb . append ( file . separatorchar ) ;
import java . util . stack ;
char sep = file . separatorchar ;
path = sbpath . tostring ( ) ;
if ( " . " . equals ( thistoken ) ) {
sbpath . append ( ca [ i ] ) ;
if ( i > 1 ) {
return new file ( sb . tostring ( ) ) ;
root = ( nextsep > 2 ) ? path . substring ( 0 , nextsep + 1 ) : path ;
s . push ( thistoken ) ;
return new file ( path ) ;
if ( !result . isabsolute ( ) ) {
sb . append ( s . elementat ( i ) ) ;
result = new file ( file , filename ) ;
}
public static class pomdependencydescriptor extends defaultdependencydescriptor {
defaultdependencydescriptor dd = new pomdependencydescriptor ( dep , ivymoduledescriptor , modulerevid ) ;
moduledescriptor moduledescriptor , modulerevisionid revisionid ) {
this . pomdependencydata = pomdependencydata ;
private final pomdependencydata pomdependencydata ;
return pomdependencydata ;
super ( moduledescriptor , revisionid , true , false , true ) ;
public pomdependencydata getpomdependencydata ( ) {
private pomdependencydescriptor ( pomdependencydata pomdependencydata ,
public static class pommoduledescriptor extends defaultmoduledescriptor {
result . addall ( ( ( pommoduledescriptor ) md ) . getdependencymanagementmap ( ) . values ( ) ) ;
private final map / * < moduleid , pomdependencymgt > * / dependencymanagementmap = new hashmap ( ) ;
+ "doesn't match expected pattern : " + key ) ;
map . entry e = ( entry ) iterator . next ( ) ;
return dependencymgt . getexcludedmodules ( ) ;
string [ ] parts = key . split ( extra info delimiter ) ;
}
. getdependencymanagementmap ( ) . get ( moduleid . newinstance ( groupid , artifactid ) ) ;
list / * < moduleid > * / exclusions = getdependencymgtexclusions ( md , parts [ 1 ] , parts [ 2 ] ) ;
string scope = ( string ) md . getextrainfo ( ) . get ( scopekey ) ;
string key = getdependencymgtextrainfokeyforscope ( dep . getgroupid ( ) , dep . getartifactid ( ) ) ;
super ( parser , res ) ;
map . entry entry = ( map . entry ) iterator . next ( ) ;
if ( ( key ) . startswith ( dependency management ) ) {
return ( ( pomdependencymgt ) ivymoduledescriptor . getdependencymanagementmap ( ) . get (
ret . put ( moduleid . newinstance ( parts [ 1 ] , parts [ 2 ] ) , entry . getvalue ( ) ) ;
if ( md instanceof pommoduledescriptor ) {
public void adddependencymanagement ( pomdependencymgt dependencymgt ) {
for ( final iterator iterator = ( ( pommoduledescriptor ) md ) . getdependencymanagementmap ( ) . entryset ( ) . iterator ( ) ; iterator . hasnext ( ) ; ) {
} else {
moduleid ) ) . getversion ( ) ;
message . warn ( "what seem to be a dependency management extra info "
public pommoduledescriptor ( moduledescriptorparser parser , resource res ) {
return dependencymanagementmap ;
string result ;
result = ( ( pomdependencymgt ) ivymoduledescriptor . getdependencymanagementmap ( ) . get (
+ extra info delimiter + "version" ;
ivymoduledescriptor = new pommoduledescriptor ( parser , res ) ;
private final pommoduledescriptor ivymoduledescriptor ;
if ( dependencymgt ! = null ) {
result = ( string ) ivymoduledescriptor . getextrainfo ( ) . get ( key ) ;
moduleid moduleid = moduleid . newinstance ( dep . getgroupid ( ) , dep . getartifactid ( ) ) ;
for ( iterator iterator = md . getextrainfo ( ) . entryset ( ) . iterator ( ) ; iterator . hasnext ( ) ; ) {
public map getdependencymanagementmap ( ) {
string versionkey = dependency management + extra info delimiter + parts [ 1 ]
moduleid ) ) . getscope ( ) ;
dependencymanagementmap . put ( moduleid . newinstance ( dependencymgt . getgroupid ( ) , dependencymgt . getartifactid ( ) ) , dependencymgt ) ;
+ extra info delimiter + parts [ 2 ]
pomdependencymgt dependencymgt = ( pomdependencymgt ) ( ( pommoduledescriptor ) descriptor )
ivymoduledescriptor . adddependencymanagement ( dep ) ;
if ( ivymoduledescriptor . getdependencymanagementmap ( ) . containskey ( moduleid ) ) {
result . add ( new defaultpomdependencymgt ( parts [ 1 ] , parts [ 2 ] , version , scope , exclusions ) ) ;
if ( parts . length ! = dependency management key parts count ) {
string scopekey = dependency management + extra info delimiter + parts [ 1 ]
+ extra info delimiter + "scope" ;
string key = ( string ) entry . getkey ( ) ;
string version = ( string ) md . getextrainfo ( ) . get ( versionkey ) ;
pomdependencymgt dependencymgt = ( pomdependencymgt ) e . getvalue ( ) ;
ret . put ( e . getkey ( ) , dependencymgt . getversion ( ) ) ;
if ( descriptor instanceof pommoduledescriptor ) {
if ( sub = = null ) {
return false ;
} else if ( !handleincompatiblecaller ( callerstack , node , callernode , conflictparent ,
blacklisted . add ( new ivynodeblacklist ( conflictparent , selectednode , evictednode ,
}
boolean dynamiccaller ) {
blacklisted . addall ( sub ) ;
selectednode , evictednode , blacklisted , versionmatcher , true ) ) {
settings . getversionmatcher ( ) , parent , selected , evicted , callerstack , false ) ;
} else {
selectednode , evictednode , callerstack , dynamiccaller ) ;
return true ;
collection sub = blacklistincompatiblecaller ( versionmatcher , conflictparent ,
callerstack . pop ( ) ;
if ( node . isevicted ( rootmoduleconf )
node , rootmoduleconf ) ) ;
if ( callerstack . sublist ( 0 , callerstack . size ( ) - 1 ) . contains ( node ) ) {
callerstack . push ( callernode ) ;
if ( !dynamiccaller & & blacklisted . isempty ( )
ivynode conflictparent , ivynode selectednode , ivynode evictednode ,
private boolean handleincompatiblecaller ( stack callerstack , ivynode node , ivynode callernode ,
& & !handleincompatiblecaller ( callerstack , node , callernode , conflictparent ,
collection blacklisted , versionmatcher versionmatcher , boolean dynamiccaller ) {
selectednode , evictednode , blacklisted , versionmatcher , false ) ) {
return null ;
stack / * < ivynode > * / callerstack ,
collections . singletonmap ( capability extra attr , md . getmodulerevisionid ( ) . tostring ( ) ) ) ;
return ret ;
report . setdownloadstatus ( downloadstatus . no ) ;
exportpackage . getversion ( ) . tostring ( ) ) ;
import org . apache . ivy . core . module . descriptor . defaultmoduledescriptor ;
* moduledescriptor
report . setsearched ( true ) ;
int c = vmatcher . compare ( mrid2 , mrid1 , mridcomparator ) ;
if ( !bundleinfo . bundle type . equals ( osgitype ) ) {
. findmodule ( osgitype , name ) ;
string name = dd . getdependencyrevisionid ( ) . getname ( ) ;
}
capabilitymd . addconfiguration ( bundleinfoadapter . conf optional ) ;
bundleinfoadapter . conf name transitive optional ) ;
string rev = ( string ) md . getextrainfo ( ) . get (
int c = vmatcher . compare ( mrid1 , mrid2 , mridcomparator ) ;
message . error ( "ambiguity for the '" + osgitype + "' requirement "
if ( osgitype = = null | | osgitype . equals ( bundleinfo . bundle type ) ) {
} else {
public static final string extra info export prefix = " osgi export " ;
. get ( osgitype ) ;
if ( node ! = null ) {
resolvedresource [ ] ret ;
return found ;
public int compare ( artifactinfo o1 , artifactinfo o2 ) {
public boolean isallownomd ( ) {
set / * < moduledescriptor > * / mds = getrepodescriptor ( ) . findmodule ( osgitype , id ) ;
return v1 . compareto ( v2 ) ;
private static final string capability extra attr = "osgi bundle" ;
public static modulerevisionid asmrid ( string type , string name , version v ) {
return collections . emptyset ( ) ;
versionmatcher vmatcher = ivycontext . getcontext ( ) . getsettings ( ) . getversionmatcher ( ) ;
if ( osgitype ! = null ) {
import java . util . comparator ;
string osgitype = ( string ) criteria . get ( ivypatternhelper . organisation key ) ;
import java . text . parseexception ;
import org . apache . ivy . osgi . util . version ;
values . put ( ivypatternhelper . organisation key , osgitype ) ;
string rev1 = o1 . getrevision ( ) ;
defaultmoduledescriptor capabilitymd = new defaultmoduledescriptor ( capabilityrev ,
tokenset . remove ( ivypatternhelper . organisation key ) ;
private mdresolvedresource buildresolvedcapabilitymd ( dependencydescriptor dd ,
resolvedresource [ ] ret = new resolvedresource [ mds . size ( ) ] ;
try {
return modulerevisionid . newinstance ( type , name , revision ) ;
return c > = 0 ? 1 : - 1 ;
newcriteria . put ( ivypatternhelper . organisation key , bundleinfo . bundle type ) ;
* <
set / * < string > * / capabilities = getrepodescriptor ( ) . getcapabilityvalues ( osgitype ) ;
capabilitydd . adddependencyconfiguration ( bundleinfoadapter . conf name optional ,
v2 = new version ( o2 . getrevision ( ) ) ;
* >
capabilitymd . addconfiguration ( bundleinfoadapter . conf transitive optional ) ;
string osgitype = mrid . getorganisation ( ) ;
capabilitydd . adddependencyconfiguration ( bundleinfoadapter . conf name default ,
modulerevisionid mrid2 = modulerevisionid . newinstance ( "" , "" , rev2 ) ;
md . getmodulerevisionid ( ) , false ) ;
version v1 ;
return new resolvedresource [ ] { buildresolvedcapabilitymd ( dd , node . getdescriptor ( ) ) } ;
} else if ( vmatcher . isdynamic ( mrid2 ) ) {
return collections . emptylist ( ) ;
md . getextrainfo ( ) . put ( extra info export prefix + exportpackage . getname ( ) ,
string useconf = bundleinfoadapter . conf use prefix + dd . getdependencyrevisionid ( ) . getname ( ) ;
v1 = new version ( o1 . getrevision ( ) ) ;
return c > = 0 ? - 1 : 1 ;
import org . apache . ivy . plugins . version . versionmatcher ;
if ( osgitype = = null | | osgitype . length ( ) = = 0 ) {
import org . apache . ivy . plugins . latest . comparatorlateststrategy ;
while ( itmd . hasnext ( ) ) {
iterator itmd = mds . iterator ( ) ;
final class artifactinfocomparator implements comparator / * < artifactinfo > * / {
return compare ( ( artifactinfo ) o1 , ( artifactinfo ) o2 ) ;
modulerevisionid capabilityrev = modulerevisionid . newinstance ( org , name , rev ,
newcriteria . put ( ivypatternhelper . organisation key , bundleinfo . package type ) ;
string org = dd . getdependencyrevisionid ( ) . getorganisation ( ) ;
metadataartifactdownloadreport report = new metadataartifactdownloadreport ( null ) ;
* / mds ) {
capabilitymd . adddependency ( capabilitydd ) ;
message . debug ( "" + getname ( ) + " : no resource found for " + mrid ) ;
modulerevisionid mrid1 = modulerevisionid . newinstance ( "" , "" , rev1 ) ;
defaultdependencydescriptor capabilitydd = new defaultdependencydescriptor (
if ( found = = null ) {
version v2 ;
newcriteria . put ( ivypatternhelper . organisation key , bundleinfo . service type ) ;
int i = 0 ;
moduledescriptor md = ( moduledescriptor ) itmd . next ( ) ;
if ( vmatcher . isdynamic ( mrid1 ) ) {
ivynode node = data . getnode ( md . getmodulerevisionid ( ) ) ;
bundleinfoadapter . conf name default ) ;
string osgitype = systemmrid . getorganisation ( ) ;
message . warn ( "ambiguity for the '" + osgitype + "' requirement "
public osgilateststrategy ( ) {
public int compare ( modulerevisionid o1 , modulerevisionid o2 ) {
public resolvedresource [ ] findbundle ( dependencydescriptor dd , resolvedata data , set / *
public int compare ( object o1 , object o2 ) {
return mridcomparator . compare ( mrid1 , mrid2 ) ;
package org . apache . ivy . osgi . core ;
exactorregexppatternmatcher . instance , null ) ;
setname ( "latest - osgi" ) ;
artifactid id = new artifactid ( moduleid . newinstance ( bundleinfo . bundle type , pkg ) ,
data . getdate ( ) ) ;
string rev2 = o2 . getrevision ( ) ;
return modulebycapabilityvalue . keyset ( ) ;
capabilitydd . adddependencyconfiguration ( useconf , useconf ) ;
+ o2 . getrevision ( ) + " ( " + e . getmessage ( ) + " ) " ) ;
private final comparator / * < modulerevisionid > * / mridcomparator = new mridcomparator ( ) ;
throw new runtimeexception ( "unsupported osgi module id : " + mrid . getmoduleid ( ) ) ;
modulerevisionid ddmrid = asmrid ( type , name , requirement . getversion ( ) ) ;
public resolvedresource [ ] findcapability ( dependencydescriptor dd , resolvedata data , set / *
bundleinfoadapter . extra info export prefix + name ) ;
private final comparator / * < artifactinfo > * / artifactinfocomparator = new artifactinfocomparator ( ) ;
public class osgilateststrategy extends comparatorlateststrategy {
import org . apache . ivy . core . module . descriptor . defaultdependencydescriptor ;
} catch ( parseexception e ) {
string osgitype = ( string ) tokenvalues . get ( ivypatternhelper . organisation key ) ;
import org . apache . ivy . core . module . descriptor . configuration ;
return getrepodescriptor ( ) . getmodulebycapbilities ( ) . keyset ( ) ;
set / * < moduledescriptor > * / mds = getrepodescriptor ( ) . findmodule ( osgitype , name ) ;
return compare ( ( modulerevisionid ) o1 , ( modulerevisionid ) o2 ) ;
import org . apache . ivy . core . module . id . modulerevisionid ;
throw new runtimeexception ( "uncomparable versions : " + o1 . getrevision ( ) + " and "
public static modulerevisionid asmrid ( string type , string name , versionrange v ) {
modulerevisionid mrid = asmrid ( bundleinfo . bundle type , bundle . getsymbolicname ( ) , bundle . getversion ( ) ) ;
ret = findbundle ( dd , data , mds ) ;
return modulerevisionid . newinstance ( type , name , v = = null ? null : v . tostring ( ) ) ;
capabilitymd . addconfiguration ( new configuration ( useconf ) ) ;
if ( bundleinfo . bundle type . equals ( osgitype ) ) {
. findmodule ( osgitype , module ) ;
. getdescriptor ( ) . getextraattribute ( capability extra attr ) ;
capabilitydd . adddependencyconfiguration ( bundleinfoadapter . conf name transitive optional ,
message . info ( "'" + osgitype + "' requirement " + mrid . getname ( ) + " ; version = "
import org . apache . ivy . core . resolve . ivynode ;
"release" , new date ( ) ) ;
capabilitymd . addconfiguration ( bundleinfoadapter . conf default ) ;
if ( osgitype . equals ( bundleinfo . package type ) ) {
resolvedresource found = findresource ( ret , getdefaultrmdparser ( dd . getdependencyid ( ) ) , mrid ,
ret [ i + + ] = buildresolvedcapabilitymd ( dd , md ) ;
import org . apache . ivy . core . ivycontext ;
final class mridcomparator implements comparator / * < modulerevisionid > * / {
return new mdresolvedresource ( null , capabilitymd . getrevision ( ) , rmr ) ;
ret = findcapability ( dd , data , mds ) ;
moduledescriptor md ) {
resolvedmodulerevision rmr = new resolvedmodulerevision ( this , this , capabilitymd , report ) ;
import org . apache . ivy . plugins . latest . artifactinfo ;
setcomparator ( artifactinfocomparator ) ;
bundleinfoadapter . conf name optional ) ;
if ( osgitype = = null ) {
string charset = urlhandler . geturlinfo ( url ) . getbodycharset ( ) ;
inputstream contentstream = urlhandler . openstream ( url ) ;
string bodycharset = basicurlhandler . getcharsetfromcontenttype ( contenttype ) ;
import java . io . inputstream ;
urlhandler urlhandler = urlhandlerregistry . getdefault ( ) ;
return new urlinfo ( true , getresponsecontentlength ( method ) , getlastmodified ( method ) , bodycharset ) ;
bufferedreader r = new bufferedreader ( new inputstreamreader ( contentstream , charset ) ) ;
string contenttype = method . getresponseheader ( "content - type" ) . getvalue ( ) ;
filesystemresolver parentmoduleresolver = new filesystemresolver ( ) ;
if ( !"file" . equals ( descriptorurl . getprotocol ( ) ) ) {
if ( !file . isabsolute ( ) ) {
parentmoduleresolver . setsettings ( ivycontext . getcontext ( ) . getsettings ( ) ) ;
file = fileutil . normalize ( new file ( url . getpath ( ) ) . getabsolutepath ( ) ) ;
return parentmoduleresolver ;
if ( parentresolver ! = null ) {
parentmoduleresolver . addivypattern ( file . getabsolutepath ( ) ) ;
}
dependencyresolver parentresolver = checkparentmoduleonfilesystem ( location , parentmrid ) ;
protected dependencyresolver checkparentmoduleonfilesystem ( string location , modulerevisionid parentmrid ) throws ioexception {
file file = fileutil . normalize ( location ) ;
parentmoduleresolver . setname ( getmoduleinheritancerepositoryparentresolvername ( parentmrid ) ) ;
import org . apache . ivy . plugins . resolver . filesystemresolver ;
parent = resolveparentfrommoduleinheritancerepository ( parentresolver , parentmrid ) ;
import java . net . uri ;
import java . net . urisyntaxexception ;
protected moduledescriptor resolveparentfrommoduleinheritancerepository ( dependencyresolver resolver , modulerevisionid parentmrid ) throws parseexception {
} catch ( urisyntaxexception e ) {
return null ;
import org . apache . ivy . util . fileutil ;
if ( !file . exists ( ) ) {
file = fileutil . normalize ( new file ( new uri ( url . toexternalform ( ) ) ) . getabsolutepath ( ) ) ;
url url = new url ( descriptorurl , location ) ;
try {
public static synchronized moduleid intern ( moduleid moduleid ) {
public static synchronized moduleid intern ( moduleid moduleid ) {
"jbi - component" , "jbi - shared - library" , "orbit" } ) ;
ext = "phar" ;
} else if ( "pear" . equals ( packaging ) ) {
"jbi - component" , "jbi - shared - library" , "orbit" } ) ;
ext = "phar" ;
} else if ( "pear" . equals ( packaging ) ) {
+ partiallyresolvedpattern , e ) ;
message . verbose ( "problem while listing resources in " + root + " with " + rep , e ) ;
message . warn ( "problem while listing resources in " + parent + " with " + rep , e ) ;
message . warn ( "problem while listing files in " + root , e ) ;
message . warn ( "problem while listing directories in " + root , e ) ;
message . verbose ( "problem while listing resources in " + parent + " with " + rep , e ) ;
+ rres , e ) ;
message . warn ( "problem while listing resources in " + root + " with " + rep , e ) ;
file = new file ( new uri ( url . toexternalform ( ) ) ) ;
file = fileutil . normalize ( file . getabsolutepath ( ) ) ;
file file = new file ( location ) ;
file = new file ( url . getpath ( ) ) ;
totalcopiedsize + = fileutil . getfilelength ( destfile ) ;
}
if ( !"ivy" . equals ( adr . gettype ( ) )
archive = artifact . getuncompressedlocaldir ( ) ;
> >
if ( options . isuncompressed ( ) & & artifact . getuncompressedlocaldir ( ) ! = null ) {
& & !options . getartifactfilter ( ) . accept ( adr . getartifact ( ) ) ) {
set dest = ( set ) artifactstocopy . get ( adr ) ;
string destpattern = "ivy" . equals ( adr . gettype ( ) ) ? destivypattern
if ( options . isuncompressed ( ) & & adr . getuncompressedlocaldir ( ) ! = null ) {
if ( conflicts . add ( artifact . getid ( ) ) ) {
artifact artifact = adr . getartifact ( ) ;
artifact . getmodulerevisionid ( ) , artifact , conf , adr . getartifactorigin ( ) ) ;
artifactstocopy . put ( adr , dest ) ;
artifactdownloadreport adr = ( artifactdownloadreport ) iter . next ( ) ;
conflictsreports . add ( adr ) ;
artifact = adr . builduncompressedartifact ( ) ;
modulerevisionid amrid = artifact . getmodulerevisionid ( ) ;
amrid . getorganisation ( ) , amrid . getname ( ) , amrid . getbranch ( ) ,
artifact . getqualifiedextraattributes ( ) ) ;
amrid . getrevision ( ) , artifact . getname ( ) , artifact . gettype ( ) , ext , conf ,
ext = "" ;
adr . getartifactorigin ( ) , amrid . getqualifiedextraattributes ( ) ,
string ext = artifact . getext ( ) ;
return parser . parsedescriptor ( getsettings ( ) , file . tourl ( ) , res , isvalidate ( ) ) ;
if ( parent ! = null ) {
moduledescriptorparser parser = moduledescriptorparserregistry . getinstance ( ) . getparser ( res ) ;
url url = settings . getrelativeurlresolver ( ) . geturl ( descriptorurl , location ) ;
import org . apache . ivy . plugins . repository . file . fileresource ;
local = parent ! = null ;
}
local ) ;
boolean local = false ;
( string [ ] ) extendtypes . toarray ( new string [ extendtypes . size ( ) ] ) ,
+ " . this parent module will be ignored . " ) ;
fileresource res = new fileresource ( null , file ) ;
import org . apache . ivy . plugins . parser . moduledescriptorparserregistry ;
parent = parseparentmoduleonfilesystem ( location ) ;
private moduledescriptor parseparentmoduleonfilesystem ( string location ) throws ioexception , parseexception {
if ( !foundmid . equals ( parentmid ) ) {
parent ,
moduleid foundmid = parent . getresolvedmodulerevisionid ( ) . getmoduleid ( ) ;
parent = null ;
message . info ( "found a parent module with unexpected modulerevisionid at source location "
+ location + "! expected : " + parentmid + " . found : " + foundmid
for ( int i = 0 ; i < indent ; i + + ) {
sb . append ( " \ \ - " ) ;
populatedependencytree ( dependency , mrid , report ) ;
public void doexecute ( ) throws buildexception {
private void populatedependencytree ( ivynode dependency , modulerevisionid currentmrid ,
list . add ( dependency ) ;
continue ;
sb . append ( " " ) . append ( evicteddata . getdetail ( ) ) ;
evictiondata evicteddata = dependency . getevicteddata ( getconf ( ) ) ;
prepareandcheck ( ) ;
import org . apache . ivy . core . resolve . ivynodecallers . caller ;
list dependencylist = ( list ) dependencies . get ( dependency . getid ( ) ) ;
* / ( ) ;
sb . append ( " + - " ) ;
private boolean hasdependencies ( ivynode dependency ) {
import java . util . list ;
import java . util . hashmap ;
printdependencies ( ( list ) dependencies . get ( dependency . getid ( ) ) , indent + 1 ) ;
boolean evicted = dependency . isevicted ( getconf ( ) ) ;
}
import java . util . iterator ;
public class ivydependencytree extends ivypostresolvetask {
for ( iterator iterator = report . getdependencies ( ) . iterator ( ) ; iterator . hasnext ( ) ; ) {
modulerevisionid mrid = report . getmoduledescriptor ( ) . getmodulerevisionid ( ) ;
registernodeifnecessary ( modulerevisionid ) ;
if ( evicteddata . istransitivelyevicted ( ) ) {
if ( evicteddata . getdetail ( ) ! = null ) {
sb . append ( " | " ) ;
import org . apache . ivy . core . module . id . modulerevisionid ;
if ( iterator . hasnext ( ) ) {
if ( evicted & & !showevicted ) {
package org . apache . ivy . ant ;
sb . append ( " transitively" ) ;
private map / * < modulerevisionid , list < ivynode > > * / dependencies = new hashmap / *
} else {
ivynode dependency = ( ivynode ) iterator . next ( ) ;
sb . append ( " evicted by " ) ;
list / * < ivynode > * / list = ( list ) dependencies . get ( modulerevisionid ) ;
if ( indent > 0 ) {
private boolean showevicted = false ;
log ( sb . tostring ( ) ) ;
return dependencylist . size ( ) > 0 ;
import org . apache . tools . ant . buildexception ;
adddependency ( caller . getmodulerevisionid ( ) , dependency ) ;
if ( i = = indent - 1 & & !iterator . hasnext ( ) & & !hasdependencies ( dependency ) ) {
* list < ivynode > >
for ( int i = 0 ; i < dependency . getallcallers ( ) . length ; i + + ) {
caller caller = dependency . getallcallers ( ) [ i ] ;
resolvereport report ) {
import org . apache . ivy . core . resolve . ivynodeeviction . evictiondata ;
import org . apache . ivy . core . resolve . ivynode ;
* < modulerevisionid ,
if ( evicted & & showevicted ) {
sb . append ( evicteddata . getselected ( ) ) ;
public void setshowevicted ( boolean showevicted ) {
sb . append ( " in " ) . append ( evicteddata . getparent ( ) ) ;
dependencies . put ( modulerevisionid , new arraylist / * < ivynode > * / ( ) ) ;
registernodeifnecessary ( dependency . getid ( ) ) ;
private void adddependency ( modulerevisionid modulerevisionid , ivynode dependency ) {
private void printdependencies ( list / * < ivynode > * / dependencylist , int indent ) {
this . showevicted = showevicted ;
for ( iterator iterator = dependencylist . iterator ( ) ; iterator . hasnext ( ) ; ) {
import org . apache . ivy . core . report . resolvereport ;
import java . util . map ;
public boolean isshowevicted ( ) {
log ( "dependency tree for " + report . getresolveid ( ) ) ;
import java . util . arraylist ;
return showevicted ;
sb . append ( " " ) ;
printdependencies ( ( list ) dependencies . get ( mrid ) , 0 ) ;
private void registernodeifnecessary ( modulerevisionid modulerevisionid ) {
sb . append ( dependency . getid ( ) . tostring ( ) ) ;
if ( !dependencies . containskey ( modulerevisionid ) ) {
stringbuilder sb = new stringbuilder ( ) ;
resolvereport report = getresolvedreport ( ) ;
stringbuffer sb = new stringbuffer ( ) ;
}
return ;
string optional = ( string ) attributes . get ( "optional" ) ;
if ( "true" . equals ( optional ) & & !settingsfile . exists ( ) ) {
}
file pemfile , string pempassword , file passfile , boolean allowedagentuse )
} catch ( agentproxyexception e ) {
throws ioexception {
import com . jcraft . jsch . agentproxy . agentproxyexception ;
return false ;
import com . jcraft . jsch . agentproxy . remoteidentityrepository ;
jsch . setidentityrepository ( new remoteidentityrepository ( con ) ) ;
connector con = connectorfactory . getdefault ( ) . createconnector ( ) ;
if ( allowedagentuse ) {
import com . jcraft . jsch . agentproxy . connectorfactory ;
import com . jcraft . jsch . agentproxy . connector ;
private boolean attemptagentuse ( jsch jsch ) {
attemptagentuse ( jsch ) ;
return true ;
message . verbose ( " : : ssh : : failure connecting to agent : : " + e . tostring ( ) ) ;
try {
import java . io . file ;
artifact unpacked = packagingmanager . getunpackedartifact ( artifact ) ;
import org . apache . ivy . core . pack . streampacking ;
this . settings = settings ;
if ( packing = = null ) {
inputstream in = null ;
adr . setdownloaddetails ( "the packed artifact " + artifact . getid ( )
import org . apache . ivy . core . settings . ivysettings ;
file archivefile = getarchivefileincache ( unpacked , null , false ) ;
import org . apache . ivy . core . pack . packagingmanager ;
throw new illegalstateexception ( "unsupported archive only packing type '"
+ "' in the packing chain : " + packaging ) ;
public void unpackartifact ( artifact artifact , file localfile , file archivefile ) throws ioexception {
import org . apache . ivy . plugins . ivysettingsaware ;
string ext = artifact . getext ( ) ;
}
packing . unpack ( in , archivefile ) ;
message . debug ( e ) ;
ext = ( ( streampacking ) packing ) . getunpackedextension ( ext ) ;
throw new illegalstateexception ( "unknown packing type '" + packings [ 0 ]
return unpacked ;
artifact . gettype ( ) + " unpacked" , ext ) ;
import org . apache . ivy . util . message ;
import org . apache . ivy . core . module . descriptor . defaultartifact ;
defaultartifact unpacked = new defaultartifact ( artifact . getmodulerevisionid ( ) ,
public artifact getunpackedartifact ( artifact artifact ) {
if ( artifact . getunpackedlocalfile ( ) ! = null ) {
if ( adr . getunpackedlocalfile ( ) ! = null ) {
message . info ( "unpacking " + artifact . getid ( ) ) ;
import java . io . ioexception ;
private packagingmanager packagingmanager = new packagingmanager ( ) ;
adr . setunpackedlocalfile ( archivefile ) ;
string [ ] packings = packaging . split ( " , " ) ;
+ " could not be unpacked ( " + e . getmessage ( ) + " ) " ) ;
artifact . getpublicationdate ( ) , artifact . getname ( ) ,
in . close ( ) ;
unpackartifact ( artifact , adr , options ) ;
string packaging = artifact . getextraattribute ( "packaging" ) ;
import org . apache . ivy . core . pack . archivepacking ;
import org . apache . ivy . core . module . descriptor . artifact ;
return ;
public class packagingmanager implements ivysettingsaware {
package org . apache . ivy . core . pack ;
if ( adr . getdownloadstatus ( ) ! = downloadstatus . failed ) {
private void unpackartifact ( artifact artifact , artifactdownloadreport adr ,
} catch ( ioexception e ) {
if ( ! ( packing instanceof streampacking ) ) {
import java . io . fileinputstream ;
+ packings [ i ] + "' in the streamed chain : " + packaging ) ;
public void setsettings ( ivysettings settings ) {
for ( int i = packings . length - 1 ; i > = 1 ; i - - ) {
private ivysettings settings ;
in = new fileinputstream ( localfile ) ;
import java . io . inputstream ;
throw new illegalstateexception ( "unknown packing type '" + packings [ i ]
if ( in ! = null ) {
packagingmanager . setsettings ( settings ) ;
archivepacking packing = settings . getpackingregistry ( ) . get ( packings [ 0 ] ) ;
if ( packaging = = null ) {
in = ( ( streampacking ) packing ) . unpack ( in ) ;
archive = artifact . getunpackedlocalfile ( ) ;
return null ;
} finally {
} catch ( exception e ) {
if ( unpacked = = null ) {
archivepacking packing = settings . getpackingregistry ( ) . get ( packings [ i ] ) ;
ext = packing . getunpackedextension ( ext ) ;
packagingmanager . unpackartifact ( artifact , adr . getlocalfile ( ) , archivefile ) ;
try {
import com . jcraft . jsch . agentproxy . connectorfactory ;
import com . jcraft . jsch . agentproxy . agentproxyexception ;
import com . jcraft . jsch . agentproxy . connector ;
import com . jcraft . jsch . agentproxy . remoteidentityrepository ;
indent ( out , indent * 4 ) ;
indent ( out , indent * 3 ) ;
indent ( out , indent * 5 ) ;
if ( !istransitive ) {
dds [ i ] . istransitive ( ) , excludes ) ;
out . println ( " < groupid > * < / groupid > " ) ;
dep . getclassifier ( ) , dep . getscope ( ) , dep . isoptional ( ) , true , null ) ;
out . println ( " < / exclusion > " ) ;
out . println ( " < exclusion > " ) ;
out . println ( " < exclusions > " ) ;
mrid . getrevision ( ) , null , null , scope , optional , dds [ i ] . istransitive ( ) ,
mrid . getrevision ( ) , type , classifier , scope , optional ,
} else if ( excludes ! = null ) {
boolean isoptional , boolean istransitive , excluderule [ ] excludes ) {
out . println ( " < artifactid > * < / artifactid > " ) ;
excludes ) ;
out . println ( " < / exclusions > " ) ;
}
string foundbranch = foundmd . getmodulerevisionid ( ) . getbranch ( ) ;
boolean samebranch = ( askedbranch = = null ) ? foundbranch = = null
: askedbranch . equals ( foundbranch ) ;
string askedbranch = askedmrid . getbranch ( ) ;
return false ;
list < status > statuses = statusmanager . getcurrent ( ) . getstatuses ( ) ;
if ( !samebranch ) {
return true ;
if ( askedmrid . getbranch ( ) ! = null ) {
}
if ( dependencylist ! = null ) {
list dependencylist = ( list ) dependencies . get ( mrid ) ;
printdependencies ( dependencylist , 0 ) ;
final stringbuffer sbpath = new stringbuffer ( ) ;
continue ;
for ( int i = 0 ; i < rest . length ( ) ; i + + ) {
s . push ( dissectedpath . root ) ;
final stack < string > s = new stack < string > ( ) ;
private final string remainingpath ;
if ( filesystemroots ! = null ) {
final file [ ] filesystemroots = file . listroots ( ) ;
}
final stringbuffer sb = new stringbuffer ( ) ;
public string tostring ( ) {
. append ( remainingpath ) . append ( " ] " ) . tostring ( ) ;
if ( pathtodissect . startswith ( filesystemroot . getpath ( ) ) ) {
final char currentchar = rest . charat ( i ) ;
final char previouschar = rest . charat ( i - 1 ) ;
this . remainingpath = remainingpath ;
if ( pathtodissect . length ( ) > 1 & & pathtodissect . charat ( 1 ) = = sep ) {
private static dissectedpath dissect ( final string path ) {
nextsep = pathtodissect . indexof ( sep , nextsep + 1 ) ;
final string pathtodissect = path . replace ( ' / ' , sep ) . replace ( ' \ \ ' , sep ) . trim ( ) ;
final string root = filesystemroot . getpath ( ) ;
return new dissectedpath ( root , sbpath . tostring ( ) ) ;
return new stringbuilder ( "dissected path [ root = " ) . append ( root ) . append ( " , remainingpath = " )
sbpath . append ( currentchar ) ;
if ( i = = 0 ) {
final string rest = pathtodissect . substring ( root . length ( ) ) ;
final char sep = file . separatorchar ;
int nextsep = pathtodissect . indexof ( sep , 2 ) ;
private final string root ;
@ override
return new dissectedpath ( file . separator , pathtodissect . substring ( 1 ) ) ;
final dissectedpath dissectedpath = dissect ( path ) ;
private static final class dissectedpath {
final stringtokenizer tok = new stringtokenizer ( dissectedpath . remainingpath , file . separator ) ;
this . root = root ;
return new dissectedpath ( root , rest ) ;
private dissectedpath ( final string root , final string remainingpath ) {
if ( currentchar ! = sep | | previouschar ! = sep ) {
for ( final file filesystemroot : filesystemroots ) {
final string root = ( nextsep > 2 ) ? pathtodissect . substring ( 0 , nextsep + 1 ) : pathtodissect ;
resolveengine engine = getcontext ( ) . getivy ( ) . getresolveengine ( ) ;
resolvedata data = getcontext ( ) . getresolvedata ( ) ;
if ( public . equals ( m2conf . getvisibility ( ) ) ) {
import static org . apache . ivy . plugins . namespace . namespacehelper . tosystem ;
import static org . apache . ivy . core . module . descriptor . configuration . visibility . public ;
dd = tosystem ( dd , ivysettings . getcontextnamespace ( ) ) ;
import static org . apache . ivy . core . ivycontext . getcontext ;
for ( string lh : splittoarray ( ops [ 0 ] ) ) {
import static org . apache . ivy . util . stringutils . isnullorempty ;
if ( list = = null ) {
if ( !confs . contains ( lh ) ) {
if ( isnullorempty ( value ) ) {
write ( " branch = \ "" + newbranch + " \ "" ) ;
public static boolean isnullorempty ( string s ) {
public static string [ ] splittoarray ( string list ) {
string listsep = groups . contains ( " " ) ? " , " : " , " ;
string listsep = list . contains ( " " ) ? " , " : " , " ;
public static void assertnotnullnotempty ( final string value , final string errormessage ) {
for ( string current : splittoarray ( list ) ) {
newmapping . append ( " - > " ) . append ( joinarray ( splittoarray ( ops [ 1 ] ) , sep ) ) ;
}
if ( confs . contains ( tok ) ) {
throw new illegalargumentexception (
@ deprecated
buffer . setdefaultprint ( isnullorempty ( attributes . getvalue ( "conf" ) ) ) ;
if ( !confstoremove . contains ( current ) ) {
public static string join ( object [ ] objs , string sep ) {
string [ ] ops = groups . split ( " - > " ) ;
newlist . append ( sep ) . append ( current ) ;
import static org . apache . ivy . util . stringutils . splittoarray ;
return s = = null | | s . trim ( ) . isempty ( ) ;
if ( ! ( objs [ i ] instanceof string ) ) {
if ( org = = null ) {
for ( string tok : splittoarray ( extend ) ) {
objs [ i ] = objs [ i ] . tostring ( ) ;
string sep = "" ;
throw new illegalargumentexception ( errormessage ) ;
for ( string obj : objs ) {
sep = listsep ;
return parts ;
assertnotnullnorempty ( value , errormessage ) ;
parts [ i ] = parts [ i ] . trim ( ) ;
public static string joinarray ( string [ ] objs , string sep ) {
return joinarray ( context . toarray ( new string [ context . size ( ) ] ) , " / " ) ;
string [ ] parts = list . split ( " , " ) ;
org = organisation ;
for ( string groups : mapping . split ( " ; " ) ) {
for ( int i = 0 ; i < parts . length ; i + + ) {
return joinarray ( ( string [ ] ) objs , sep ) ;
return null ;
"cannot exclude a configuration which is extended . " ) ;
for ( int i = 0 ; i < objs . length ; i + + ) {
public static void assertnotnullnorempty ( final string value , final string errormessage ) {
string result = ( ivy = = null ) ? value : ivy . substitute ( value ) ;
import static org . apache . ivy . util . stringutils . joinarray ;
if ( !isnullorempty ( newbranch ) ) {
if ( getiterations ( ) > 0 ) {
incrementiterationcount ( ) ;
final long bits = double . doubletorawlongbits ( d ) ;
long bits = double . doubletorawlongbits ( x ) ;
long xl = double . doubletorawlongbits ( d ) ;
inbits = double . doubletorawlongbits ( x ) ;
final long m = double . doubletorawlongbits ( magnitude ) ;
return ( ( float . floattorawintbits ( f ) > > > 23 ) & 0xff ) - 127 ;
long inbits = double . doubletorawlongbits ( x ) ;
final long s = double . doubletorawlongbits ( sign ) ;
final int m = float . floattorawintbits ( magnitude ) ;
return abs ( x - double . longbitstodouble ( double . doubletorawlongbits ( x ) ^ 1 ) ) ;
return ( int ) ( ( double . doubletorawlongbits ( d ) > > > 52 ) & 0x7ff ) - 1023 ;
final int s = float . floattorawintbits ( sign ) ;
}
state . stepaccepted ( eventt , eventyprimary ) ;
final double [ ] eventycomplete = new double [ y . length ] ;
expandable . getprimarymapper ( ) . insertequationdata ( interpolator . getinterpolatedstate ( ) ,
protected incrementor getevaluationscounter ( ) {
eventycomplete ) ;
return evaluations ;
needreset = needreset | | state . reset ( eventt , eventycomplete ) ;
secondary . insertequationdata ( interpolator . getinterpolatedsecondarystate ( index + + ) ,
return expandable ;
system . arraycopy ( eventycomplete , 0 , y , 0 , y . length ) ;
protected expandablestatefulode getexpandable ( ) {
int index = 0 ;
for ( equationsmapper secondary : expandable . getsecondarymappers ( ) ) {
interpolator . setinterpolatedtime ( eventt ) ;
final double [ ] eventyprimary = interpolator . getinterpolatedstate ( ) . clone ( ) ;
double [ ] arr = new double [ center . getpoint ( ) . length ] ;
clusters . get ( newcluster ) . addpoint ( point ) ;
if ( size = = 0 ) {
final double [ ] [ ] oldmatrix = new double [ size ] [ k ] ;
final list < centroidcluster < t > > newclusters = new arraylist < centroidcluster < t > > ( k ) ;
return fuzzyness ;
j + + ;
membershipmatrix = new double [ size ] [ k ] ;
if ( size < k ) {
}
import org . apache . commons . math3 . exception . numberistoosmallexception ;
return objfunction ;
final double dista = fastmath . abs ( distance ( point , clusters . get ( j ) . getcenter ( ) ) ) ;
throw new numberistoosmallexception ( fuzzyness , 1 . 0 , false ) ;
private void updateclustercenters ( ) {
int j = 0 ;
if ( fuzzyness < = 1 . 0d ) {
public fuzzykmeansclusterer ( final int k , final double fuzzyness ,
import java . util . collections ;
private double calculatemaxmembershipchange ( final double [ ] [ ] matrix ) {
initializemembershipmatrix ( ) ;
public class fuzzykmeansclusterer < t extends clusterable > extends clusterer < t > {
private void savemembershipmatrix ( final double [ ] [ ] matrix ) {
import org . apache . commons . math3 . random . jdkrandomgenerator ;
final int maxiterations , final distancemeasure measure ,
super ( measure ) ;
double maxmembership = 0 . 0 ;
public double getfuzzyness ( ) {
final double epsilon , final randomgenerator random )
return 0 ;
this . clusters = null ;
matharrays . scaleinplace ( 1 . 0 / sum , arr ) ;
membershipmatrix [ i ] [ j ] = 1 . 0 / sum ;
clusters = newclusters ;
public int getk ( ) {
} while ( difference > epsilon & & + + iteration < max ) ;
for ( final t point : points ) {
public randomgenerator getrandomgenerator ( ) {
clusters . clear ( ) ;
import org . apache . commons . math3 . exception . mathillegalargumentexception ;
this . membershipmatrix = null ;
return clusters ;
this ( k , fuzzyness , maxiterations , measure , default epsilon , new jdkrandomgenerator ( ) ) ;
return points ;
private final randomgenerator random ;
double objfunction = 0 . 0 ;
mathutils . checknotnull ( datapoints ) ;
double difference = 0 . 0 ;
for ( int idx = 0 ; idx < arr . length ; idx + + ) {
double dist = distance ( point , cluster . getcenter ( ) ) ;
int newcluster = - 1 ;
final t point = points . get ( i ) ;
import org . apache . commons . math3 . linear . matrixutils ;
this . random = random ;
import java . util . arraylist ;
return maxiterations ;
final double [ ] pointarr = point . getpoint ( ) ;
i + + ;
private final double fuzzyness ;
final double distb = fastmath . abs ( distance ( point , c . getcenter ( ) ) ) ;
private static final double default epsilon = 1e - 3 ;
return k ;
this ( k , fuzzyness , - 1 , new euclideandistance ( ) ) ;
int i = 0 ;
public int getmaxiterations ( ) {
import java . util . list ;
for ( int i = 0 ; i < k ; i + + ) {
points = collections . unmodifiablelist ( new arraylist < t > ( datapoints ) ) ;
private final int k ;
return maxmembership ;
import org . apache . commons . math3 . util . matharrays ;
private final int maxiterations ;
return random ;
public list < centroidcluster < t > > cluster ( final collection < t > datapoints )
import org . apache . commons . math3 . ml . distance . euclideandistance ;
this . fuzzyness = fuzzyness ;
public realmatrix getmembershipmatrix ( ) {
public list < t > getdatapoints ( ) {
for ( final centroidcluster < t > c : clusters ) {
final int max = ( maxiterations < 0 ) ? integer . max value : maxiterations ;
for ( final centroidcluster < t > cluster : clusters ) {
final clusterable center = cluster . getcenter ( ) ;
private void updatemembershipmatrix ( ) {
for ( int j = 0 ; j < k ; j + + ) {
private list < centroidcluster < t > > clusters ;
return matrixutils . createrealmatrix ( membershipmatrix ) ;
do {
import java . util . collection ;
final int size = datapoints . size ( ) ;
maxmembership = fastmath . max ( v , maxmembership ) ;
savemembershipmatrix ( oldmatrix ) ;
if ( membershipmatrix [ i ] [ j ] > maxmembership ) {
public list < centroidcluster < t > > getclusters ( ) {
membershipmatrix [ i ] [ j ] = random . nextdouble ( ) ;
this . epsilon = epsilon ;
this . points = null ;
if ( points = = null | | clusters = = null ) {
private double [ ] [ ] membershipmatrix ;
throw new numberistoosmallexception ( size , k , false ) ;
public double getobjectivefunctionvalue ( ) {
objfunction + = ( dist * dist ) * fastmath . pow ( membershipmatrix [ i ] [ j ] , fuzzyness ) ;
updatemembershipmatrix ( ) ;
final double u = fastmath . pow ( membershipmatrix [ i ] [ j ] , fuzzyness ) ;
public fuzzykmeansclusterer ( final int k , final double fuzzyness ) throws numberistoosmallexception {
maxmembership = membershipmatrix [ i ] [ j ] ;
final int maxiterations , final distancemeasure measure )
private final double epsilon ;
private list < t > points ;
for ( int j = 0 ; j < clusters . size ( ) ; j + + ) {
arr [ idx ] + = u * pointarr [ idx ] ;
import org . apache . commons . math3 . util . fastmath ;
import org . apache . commons . math3 . ml . distance . distancemeasure ;
package org . apache . commons . math3 . ml . clustering ;
this . maxiterations = maxiterations ;
int iteration = 0 ;
double v = fastmath . abs ( membershipmatrix [ i ] [ j ] - matrix [ i ] [ j ] ) ;
updateclustercenters ( ) ;
clusters . add ( new centroidcluster < t > ( new doublepoint ( new double [ pointdimension ] ) ) ) ;
system . arraycopy ( membershipmatrix [ i ] , 0 , matrix [ i ] , 0 , clusters . size ( ) ) ;
newcluster = j ;
sum + = u ;
newclusters . add ( new centroidcluster < t > ( new doublepoint ( arr ) ) ) ;
import org . apache . commons . math3 . util . mathutils ;
for ( int i = 0 ; i < points . size ( ) ; i + + ) {
import org . apache . commons . math3 . random . randomgenerator ;
clusters = new arraylist < centroidcluster < t > > ( ) ;
import org . apache . commons . math3 . linear . realmatrix ;
membershipmatrix [ i ] = matharrays . normalizearray ( membershipmatrix [ i ] , 1 . 0 ) ;
difference = calculatemaxmembershipchange ( oldmatrix ) ;
throws numberistoosmallexception {
sum + = fastmath . pow ( dista / distb , 2 . 0 / ( fuzzyness - 1 . 0 ) ) ;
private void initializemembershipmatrix ( ) {
final int pointdimension = points . get ( 0 ) . getpoint ( ) . length ;
this . k = k ;
throws mathillegalargumentexception {
double sum = 0 . 0 ;
this . fuzziness = fuzziness ;
throw new mathillegalstateexception ( ) ;
if ( fuzziness < = 1 . 0d ) {
public double getepsilon ( ) {
return fuzziness ;
return epsilon ;
if ( membershipmatrix = = null ) {
this ( k , fuzziness , maxiterations , measure , default epsilon , new jdkrandomgenerator ( ) ) ;
}
public fuzzykmeansclusterer ( final int k , final double fuzziness ,
throw new numberistoosmallexception ( fuzziness , 1 . 0 , false ) ;
public double getfuzziness ( ) {
public fuzzykmeansclusterer ( final int k , final double fuzziness ) throws numberistoosmallexception {
this ( k , fuzziness , - 1 , new euclideandistance ( ) ) ;
objfunction + = ( dist * dist ) * fastmath . pow ( membershipmatrix [ i ] [ j ] , fuzziness ) ;
final double dist = distance ( point , cluster . getcenter ( ) ) ;
import org . apache . commons . math3 . exception . mathillegalstateexception ;
sum + = fastmath . pow ( dista / distb , 2 . 0 / ( fuzziness - 1 . 0 ) ) ;
final double u = fastmath . pow ( membershipmatrix [ i ] [ j ] , fuzziness ) ;
private final double fuzziness ;
for ( int j = 0 ; j < num coeff ; j + + ) {
private static final int num coeff = 16 ;
final double [ ] a = new double [ num coeff ] ;
for ( int i = 0 ; i < num coeff ; i + + ) {
}
throw new outofrangeexception ( c , val [ 0 ] , val [ val . length - 1 ] ) ;
import java . util . arrays ;
if ( r = = last ) {
return r ;
return last - 1 ;
if ( r = = - 1 | |
return - r - 2 ;
r = = - val . length ) {
if ( r < 0 ) {
final int r = arrays . binarysearch ( val , c ) ;
final int last = val . length - 1 ;
this . a [ i ] [ j ] = a [ i * n + j ] ;
r = = - val . length - 1 ) {
}
x > xval [ xval . length - 1 ] | |
y < yval [ 0 ] | |
y > yval [ yval . length - 1 ] ) {
return false ;
if ( x < xval [ 0 ] | |
} else {
return true ;
public boolean isvalidpoint ( double x , double y ) {
getmaxiterations ( ) ,
public gaussnewtonoptimizer withmodelandjacobian ( multivariatevectorfunction model ,
0 , 0 , true ) ;
multivariatematrixfunction jacobian ) {
checker ,
import org . apache . commons . math3 . linear . blockrealmatrix ;
new qrdecomposition ( ma ) . getsolver ( ) ;
}
realmatrix weightsqrt ,
public gaussnewtonoptimizer withstartpoint ( double [ ] start ) {
getconvergencechecker ( ) ,
withmaxevaluations < gaussnewtonoptimizer > {
for ( int i = 0 ; i < nr ; + + i ) {
import org . apache . commons . math3 . linear . ludecomposition ;
package org . apache . commons . math3 . fitting . leastsquares ;
return new gaussnewtonoptimizer ( null , null , null , null , null , null , null ,
for ( int i = 0 ; i < nr ; i + + ) {
return current ;
realmatrix weight ,
public gaussnewtonoptimizer withtarget ( double [ ] target ) {
} catch ( singularmatrixexception e ) {
getmaxevaluations ( ) ,
current = new pointvectorvaluepair ( currentpoint , currentobjective ) ;
withlu ) ;
withmodelandjacobian < gaussnewtonoptimizer > ,
import org . apache . commons . math3 . linear . qrdecomposition ;
residualsweights [ i ] = weightmatrix . getentry ( i , i ) ;
b [ j ] + = wr * grad [ j ] ;
public class gaussnewtonoptimizer extends abstractleastsquaresoptimizer
realmatrix ma = new blockrealmatrix ( a ) ;
throw new mathinternalerror ( ) ;
jacobian ,
= getconvergencechecker ( ) ;
withweight < gaussnewtonoptimizer > ,
for ( boolean converged = false ; !converged ; ) {
model ,
public gaussnewtonoptimizer withconvergencechecker ( convergencechecker < pointvectorvaluepair > checker ) {
import org . apache . commons . math3 . optim . convergencechecker ;
try {
getweightsquarerootinternal ( ) ,
public gaussnewtonoptimizer withmaxiterations ( int maxiter ) {
boolean uselu ) {
pointvectorvaluepair previous = current ;
final double residual = currentresiduals [ i ] ;
final realmatrix weightedjacobian = computeweightedjacobian ( currentpoint ) ;
final double [ ] residualsweights = new double [ nr ] ;
for ( int i = 0 ; i < nc ; + + i ) {
decompositionsolver solver = uselu ?
final int nr = targetvalues . length ;
withstartpoint < gaussnewtonoptimizer > ,
import org . apache . commons . math3 . exception . util . localizedformats ;
for ( int j = 0 ; j < nc ; + + j ) {
public pointvectorvaluepair dooptimize ( ) {
public gaussnewtonoptimizer withweight ( realmatrix weight ) {
final int nc = currentpoint . length ;
final double weight = residualsweights [ i ] ;
for ( int l = 0 ; l < nc ; + + l ) {
final double [ ] dx = solver . solve ( new arrayrealvector ( b , false ) ) . toarray ( ) ;
return new gaussnewtonoptimizer ( gettargetinternal ( ) ,
final convergencechecker < pointvectorvaluepair > checker
convergencechecker < pointvectorvaluepair > checker ,
multivariatematrixfunction jacobian ,
withconvergencechecker < gaussnewtonoptimizer > ,
final double [ ] targetvalues = gettarget ( ) ;
ak [ l ] + = wgk * grad [ l ] ;
import org . apache . commons . math3 . exception . dimensionmismatchexception ;
if ( previous ! = null ) {
final double [ ] [ ] a = new double [ nc ] [ nc ] ;
getstart ( ) ,
double wgk = weight * grad [ k ] ;
final double wr = weight * residual ;
double [ ] start ,
private gaussnewtonoptimizer ( double [ ] target ,
getmodel ( ) ,
if ( converged ) {
public gaussnewtonoptimizer withlu ( boolean withlu ) {
throw new convergenceexception ( localizedformats . unable to solve singular problem ) ;
uselu ) ;
int maxiter ,
maxiter ,
final realmatrix weightmatrix = getweight ( ) ;
implements withtarget < gaussnewtonoptimizer > ,
final double [ ] currentobjective = computeobjectivevalue ( currentpoint ) ;
final double [ ] b = new double [ nc ] ;
return new gaussnewtonoptimizer ( target ,
getweightinternal ( ) ,
final double [ ] grad = weightedjacobian . getrow ( i ) ;
@ override
import org . apache . commons . math3 . linear . singularmatrixexception ;
getjacobian ( ) ,
import org . apache . commons . math3 . analysis . multivariatematrixfunction ;
incrementiterationcount ( ) ;
throw new dimensionmismatchexception ( weightmatrix . getrowdimension ( ) , nr ) ;
weight ,
if ( weightmatrix . getrowdimension ( ) ! = nr ) {
withmaxiterations < gaussnewtonoptimizer > ,
null ,
maxeval ,
import org . apache . commons . math3 . linear . decompositionsolver ;
import org . apache . commons . math3 . exception . nullargumentexception ;
double [ ] ak = a [ k ] ;
converged = checker . converged ( getiterations ( ) , previous , current ) ;
if ( checker = = null ) {
start ,
multivariatevectorfunction model ,
pointvectorvaluepair current = null ;
import org . apache . commons . math3 . linear . arrayrealvector ;
int maxeval ,
import org . apache . commons . math3 . analysis . multivariatevectorfunction ;
import org . apache . commons . math3 . exception . convergenceexception ;
public gaussnewtonoptimizer withmaxevaluations ( int maxeval ) {
throw new nullargumentexception ( ) ;
import org . apache . commons . math3 . optim . pointvectorvaluepair ;
throw new dimensionmismatchexception ( weightmatrix . getcolumndimension ( ) , nr ) ;
this . uselu = uselu ;
final double [ ] currentpoint = getstart ( ) ;
public static gaussnewtonoptimizer create ( ) {
private final boolean uselu ;
for ( int k = 0 ; k < nc ; + + k ) {
final double [ ] currentresiduals = computeresiduals ( currentobjective ) ;
currentpoint [ i ] + = dx [ i ] ;
super ( target , weight , weightsqrt , model , jacobian , checker , start , maxeval , maxiter ) ;
import org . apache . commons . math3 . linear . realmatrix ;
return new gaussnewtonoptimizer ( gettarget ( ) ,
new ludecomposition ( ma ) . getsolver ( ) :
if ( weightmatrix . getcolumndimension ( ) ! = nr ) {
import org . apache . commons . math3 . exception . mathinternalerror ;
}
return fastmath . sqrt ( re * re + im * im ) ;
if ( largesteigenvaluenorm = = 0 . 0 ) {
return false ;
double largesteigenvaluenorm = eigenvaluenorm ( 0 ) ;
if ( precision . equals ( eigenvaluenorm ( i ) / largesteigenvaluenorm , 0 , epsilon ) ) {
private double eigenvaluenorm ( int i ) {
final double re = realeigenvalues [ i ] ;
final double im = imageigenvalues [ i ] ;
int consecutivexyties = 1 ;
import org . apache . commons . math3 . linear . blockrealmatrix ;
if ( j < jend ) {
final int jend = fastmath . min ( j + segmentsize , n ) ;
pairs [ i ] = new pair < double , double > ( xarray [ i ] , yarray [ i ] ) ;
j + + ;
consecutiveyties + + ;
}
public class kendallscorrelation {
for ( int i = 1 ; i < n ; i + + ) {
import org . apache . commons . math3 . util . pair ;
final pair < double , double > curr = pairs [ i ] ;
outmatrix . setentry ( j , i , corr ) ;
int swaps = 0 ;
for ( int j = 0 ; j < i ; j + + ) {
} else {
for ( int offset = 0 ; offset < n ; offset + = 2 * segmentsize ) {
private final realmatrix correlationmatrix ;
for ( int i = 0 ; i < n ; i + + ) {
pairs = pairsdestination ;
return concordantminusdiscordant / fastmath . sqrt ( ( numpairs - tiedxpairs ) * ( numpairs - tiedypairs ) ) ;
int copylocation = offset ;
if ( curr . getsecond ( ) . equals ( prev . getsecond ( ) ) ) {
import java . util . comparator ;
int consecutiveyties = 1 ;
int j = iend ;
public kendallscorrelation ( ) {
consecutivexyties + + ;
if ( i < iend ) {
@ suppresswarnings ( "unchecked" )
consecutivexties + + ;
realmatrix outmatrix = new blockrealmatrix ( nvars , nvars ) ;
pairsdestination [ copylocation ] = pairs [ j ] ;
} ) ;
public kendallscorrelation ( realmatrix matrix ) {
tiedxypairs + = consecutivexyties * ( consecutivexyties - 1 ) / 2 ;
final int iend = fastmath . min ( i + segmentsize , n ) ;
return computecorrelationmatrix ( new blockrealmatrix ( matrix ) ) ;
public kendallscorrelation ( double [ ] [ ] data ) {
if ( xarray . length ! = yarray . length ) {
int concordantminusdiscordant = numpairs - tiedxpairs - tiedypairs + tiedxypairs - 2 * swaps ;
outmatrix . setentry ( i , i , 1d ) ;
int tiedxypairs = 0 ;
import org . apache . commons . math3 . linear . matrixutils ;
correlationmatrix = computecorrelationmatrix ( matrix ) ;
throws dimensionmismatchexception {
i + + ;
import org . apache . commons . math3 . exception . dimensionmismatchexception ;
pairsdestination [ copylocation ] = pairs [ i ] ;
int comparefirst = pair1 . getfirst ( ) . compareto ( pair2 . getfirst ( ) ) ;
tiedxpairs + = consecutivexties * ( consecutivexties - 1 ) / 2 ;
package org . apache . commons . math3 . stat . correlation ;
throw new dimensionmismatchexception ( xarray . length , yarray . length ) ;
this ( matrixutils . createrealmatrix ( data ) ) ;
pairsdestination = pairstemp ;
public realmatrix computecorrelationmatrix ( final realmatrix matrix ) {
public int compare ( pair < double , double > pair1 , pair < double , double > pair2 ) {
copylocation + + ;
prev = pairs [ 0 ] ;
return correlationmatrix ;
return outmatrix ;
swaps + = iend - i ;
@ override
while ( i < iend | | j < jend ) {
if ( pairs [ i ] . getsecond ( ) . compareto ( pairs [ j ] . getsecond ( ) ) < = 0 ) {
int consecutivexties = 1 ;
int tiedypairs = 0 ;
int tiedxpairs = 0 ;
tiedypairs + = consecutiveyties * ( consecutiveyties - 1 ) / 2 ;
public realmatrix computecorrelationmatrix ( final double [ ] [ ] matrix ) {
final pair < double , double > [ ] pairstemp = pairs ;
final int n = xarray . length ;
double corr = correlation ( matrix . getcolumn ( i ) , matrix . getcolumn ( j ) ) ;
outmatrix . setentry ( i , j , corr ) ;
consecutivexyties = 1 ;
consecutiveyties = 1 ;
correlationmatrix = null ;
int i = offset ;
prev = curr ;
pair < double , double > prev = pairs [ 0 ] ;
final int numpairs = n * ( n - 1 ) / 2 ;
for ( int i = 0 ; i < nvars ; i + + ) {
import org . apache . commons . math3 . util . fastmath ;
import java . util . arrays ;
for ( int segmentsize = 1 ; segmentsize < n ; segmentsize < < = 1 ) {
return comparefirst ! = 0 ? comparefirst : pair1 . getsecond ( ) . compareto ( pair2 . getsecond ( ) ) ;
public realmatrix getcorrelationmatrix ( ) {
pair < double , double > [ ] pairs = new pair [ n ] ;
int nvars = matrix . getcolumndimension ( ) ;
import org . apache . commons . math3 . linear . realmatrix ;
arrays . sort ( pairs , new comparator < pair < double , double > > ( ) {
if ( curr . getfirst ( ) . equals ( prev . getfirst ( ) ) ) {
pair < double , double > [ ] pairsdestination = new pair [ n ] ;
public double correlation ( final double [ ] xarray , final double [ ] yarray )
consecutivexties = 1 ;
mantissa | = 1l < < 52 ;
result * = x ;
b - = sintb * sinepsa + sinta * sinepsb + sintb * sinepsb ;
random > > = 8 ;
mantissa & = 0x000fffffffffffffl ;
za * = ya ;
bits | = ( ( long ) next ( 32 ) ) & 0xffffffffl ;
xl & = mask 30bits ;
b + = sinta * cosepsb + costa * sinepsb ;
prodb + = bc < < 32 ;
lnza * = epsilon ;
ac + = ( bc + ad ) > > > 32 ;
z * = lnb ;
b + = d ;
b + = costb * cosepsa + costa * cosepsb + costb * cosepsb ;
b + = sintb + costb * sinepsa + sintb * cosepsb + costb * sinepsb ;
mantissa & = 0x007fffff ;
mantissa < < = 1 ;
prod2a + = bc > > > 32 ;
e > > = 1 ;
mantissa > > > = 1 - scaledexponent ;
proda + = bc > > > 32 ;
mantissa | = 1 < < 23 ;
zb * = epsilon ;
prod2b + = bc < < 32 ;
final long numpairs = n * ( n - 1l ) / 2l ;
final double nontiedpairsmultiplied = ( numpairs - tiedxpairs ) * ( double ) ( numpairs - tiedypairs ) ;
return concordantminusdiscordant / fastmath . sqrt ( nontiedpairsmultiplied ) ;
final long concordantminusdiscordant = numpairs - tiedxpairs - tiedypairs + tiedxypairs - 2 * swaps ;
pivotselection = ( pivotselectionrule ) data ;
private pivotselectionrule pivotselection ;
for ( int i = tableau . getnumobjectivefunctions ( ) ; i < tableau . getheight ( ) ; i + + ) {
continue ;
final double entry = tableau . getentry ( i , col ) ;
return false ;
int minindex = tableau . getwidth ( ) ;
if ( pivotselection = = pivotselectionrule . bland & & isvalidpivotcolumn ( tableau , i ) ) {
}
if ( precision . compareto ( entry , 0d , maxulps ) > 0 ) {
for ( integer row : minratiopositions ) {
private boolean isvalidpivotcolumn ( simplextableau tableau , int col ) {
if ( basicrow ! = null & & basicrow . equals ( row ) & & i < minindex ) {
return true ;
if ( data instanceof pivotselectionrule ) {
minrow = row ;
final int varend = tableau . getwidth ( ) - 1 ;
final integer basicrow = tableau . getbasicrow ( i ) ;
integer minrow = null ;
final int varstart = tableau . getnumobjectivefunctions ( ) ;
return minrow ;
this . pivotselection = pivotselectionrule . dantzig ;
minindex = i ;
for ( int i = varstart ; i < varend & & !row . equals ( minrow ) ; i + + ) {
break ;
import org . apache . commons . math3 . util . fastmath ;
if ( fastmath . abs ( entry ) < cutoff ) {
maxulps ) ;
tableau . setentry ( i , col , 0 ) ;
} else if ( precision . compareto ( entry , 0d , maxulps ) > 0 ) {
static final double default cut off = 1e - 10 ;
if ( crossings = = null ) {
final subhyperplane < sphere1d > subplus = new chord ( x , !direct ) . wholehyperplane ( ) ;
final double global = othercircle . getoffset ( thiscircle . getpointat ( 0 . 0 ) ) ;
import org . apache . commons . math3 . geometry . spherical . oned . s1point ;
final bsptree < sphere1d > plustree = getremainingregion ( ) . isempty ( splittree . getplus ( ) ) ?
final subhyperplane < sphere1d > subminus = new chord ( x , direct ) . wholehyperplane ( ) ;
}
public class subcircle extends abstractsubhyperplane < sphere2d , sphere1d > {
import org . apache . commons . math3 . geometry . partitioning . abstractsubhyperplane ;
new bsptree < sphere1d > ( boolean . false ) :
package org . apache . commons . math3 . geometry . spherical . twod ;
import org . apache . commons . math3 . geometry . partitioning . region ;
import org . apache . commons . math3 . geometry . spherical . oned . sphere1d ;
return new subcircle ( hyperplane , remainingregion ) ;
import org . apache . commons . math3 . geometry . partitioning . side ;
new bsptree < sphere1d > ( subminus , new bsptree < sphere1d > ( boolean . false ) ,
import org . apache . commons . math3 . geometry . partitioning . subhyperplane ;
import org . apache . commons . math3 . util . fastmath ;
splittree . getminus ( ) , null ) ;
final circle thiscircle = ( circle ) gethyperplane ( ) ;
final bsptree < sphere1d > minustree = getremainingregion ( ) . isempty ( splittree . getminus ( ) ) ?
super ( hyperplane , remainingregion ) ;
import org . apache . commons . math3 . geometry . spherical . oned . chord ;
return getremainingregion ( ) . side ( new chord ( x , direct ) ) ;
return new splitsubhyperplane < sphere2d > ( new subcircle ( thiscircle . copyself ( ) , new arcsset ( plustree ) ) ,
new bsptree < sphere1d > ( subplus , new bsptree < sphere1d > ( boolean . false ) ,
public side side ( final hyperplane < sphere2d > hyperplane ) {
final circle othercircle = ( circle ) hyperplane ;
new splitsubhyperplane < sphere2d > ( this , null ) ;
final region < sphere1d > remainingregion ) {
import org . apache . commons . math3 . geometry . partitioning . hyperplane ;
public splitsubhyperplane < sphere2d > split ( final hyperplane < sphere2d > hyperplane ) {
@ override
splittree . getplus ( ) , null ) ;
new splitsubhyperplane < sphere2d > ( null , this ) :
final s1point x = thiscircle . tosubspace ( crossings ) ;
final s2point [ ] crossings = thiscircle . intersection ( othercircle ) ;
final boolean direct = fastmath . sin ( thiscircle . getangle ( ) - othercircle . getangle ( ) ) < 0 ;
final bsptree < sphere1d > splittree = getremainingregion ( ) . gettree ( false ) . split ( subminus ) ;
return ( global < - 1 . 0e - 10 ) ? side . minus : ( ( global > 1 . 0e - 10 ) ? side . plus : side . hyper ) ;
public subcircle ( final hyperplane < sphere2d > hyperplane ,
return ( global < - 1 . 0e - 10 ) ?
protected abstractsubhyperplane < sphere2d , sphere1d > buildnew ( final hyperplane < sphere2d > hyperplane ,
import org . apache . commons . math3 . geometry . partitioning . bsptree ;
import org . apache . commons . math3 . geometry . spherical . oned . arcsset ;
new subcircle ( thiscircle . copyself ( ) , new arcsset ( minustree ) ) ) ;
final vector1d x = thisline . tosubspace ( ( point < euclidean2d > ) crossing ) ;
import org . apache . commons . math3 . geometry . euclidean . oned . euclidean1d ;
location loc1 = getremainingregion ( ) . checkpoint ( line1 . tosubspace ( ( point < euclidean2d > ) v2d ) ) ;
location loc2 = subline . getremainingregion ( ) . checkpoint ( line2 . tosubspace ( ( point < euclidean2d > ) v2d ) ) ;
line . tosubspace ( ( point < euclidean2d > ) end ) . getx ( ) ) ;
final vector2d start = line . tospace ( ( point < euclidean1d > ) new vector1d ( interval . getinf ( ) ) ) ;
vector2d p = thisplane . tosubspace ( ( point < euclidean3d > ) inter . tospace ( ( point < euclidean1d > ) vector1d . zero ) ) ;
final vector2d end = line . tospace ( ( point < euclidean1d > ) new vector1d ( interval . getsup ( ) ) ) ;
vector2d q = thisplane . tosubspace ( ( point < euclidean3d > ) inter . tospace ( ( point < euclidean1d > ) vector1d . one ) ) ;
return new intervalsset ( line . tosubspace ( ( point < euclidean2d > ) start ) . getx ( ) ,
import org . apache . commons . math3 . geometry . point ;
package org . apache . commons . math3 . geometry . spherical . oned ;
return false ;
new splitsubhyperplane < sphere1d > ( this , null ) ;
public splitsubhyperplane < sphere1d > split ( final hyperplane < sphere1d > hyperplane ) {
public class sublimitangle extends abstractsubhyperplane < sphere1d , sphere1d > {
}
public side side ( final hyperplane < sphere1d > hyperplane ) {
import org . apache . commons . math3 . geometry . partitioning . abstractsubhyperplane ;
import org . apache . commons . math3 . geometry . partitioning . region ;
protected abstractsubhyperplane < sphere1d , sphere1d > buildnew ( final hyperplane < sphere1d > hyperplane ,
import org . apache . commons . math3 . geometry . partitioning . side ;
super ( hyperplane , remainingregion ) ;
public boolean isempty ( ) {
final region < sphere1d > remainingregion ) {
import org . apache . commons . math3 . geometry . partitioning . hyperplane ;
@ override
new splitsubhyperplane < sphere1d > ( null , this ) :
public sublimitangle ( final hyperplane < sphere1d > hyperplane ,
return ( global < - 1 . 0e - 10 ) ? side . minus : ( ( global > 1 . 0e - 10 ) ? side . plus : side . hyper ) ;
return ( global < - 1 . 0e - 10 ) ?
return new sublimitangle ( hyperplane , remainingregion ) ;
return 0 ;
public double getsize ( ) {
final double global = hyperplane . getoffset ( ( ( limitangle ) gethyperplane ( ) ) . getlocation ( ) ) ;
minus . isempty ( ) ? null : new arcsset ( minus , tolerance ) ) ;
if ( end = = null ) {
private bsptree < sphere1d > nextinternalnode ( bsptree < sphere1d > node ) {
if ( syncedstart < arclength | | syncedend > mathutils . two pi ) {
return side . hyper ;
bsptree < sphere1d > previous = previousinternalnode ( node ) ;
public arcsset getplus ( ) {
return new subarcsiterator ( ) ;
plus . add ( arcend ( a [ 1 ] ) ) ;
}
while ( end ! = null & & !isarcend ( end ) ) {
minus . add ( arcstart ( a [ 0 ] ) ) ;
minus . add ( arcend ( minustoplus ) ) ;
minus . add ( arcstart ( plustominus ) ) ;
if ( firststart = = null ) {
final double [ ] next = pending ;
private final bsptree < sphere1d > firststart ;
public class arcsset extends abstractregion < sphere1d , sphere1d > implements iterable < double [ ] > {
} else {
size + = length ;
end = nextinternalnode ( end ) ;
private double [ ] pending ;
pending = new double [ ] {
node = null ;
previous = previousinternalnode ( node ) ;
private final arcsset minus ;
return side . both ;
throw new mathinternalerror ( ) ;
return plus ;
return list ;
if ( syncedend > mathutils . two pi ) {
inminus = true ;
list . add ( new arc ( a [ 0 ] , a [ 1 ] , tolerance ) ) ;
private bsptree < sphere1d > previousinternalnode ( bsptree < sphere1d > node ) {
throw new nosuchelementexception ( ) ;
bsptree < sphere1d > start = node ;
this . minus = minus ;
setbarycenter ( new s1point ( sum / ( 2 * size ) ) ) ;
0 , mathutils . two pi
private bsptree < sphere1d > node ;
return pending ! = null ;
if ( end ! = null ) {
return side . minus ;
return side . plus ;
public double [ ] next ( ) {
public split split ( final arc arc ) {
} else if ( previousinternalnode ( firststart ) = = null & & nextinternalnode ( firststart ) = = null ) {
node = firststart ;
end = firststart ;
private sublimitangle arcend ( final double alpha ) {
getangle ( start ) , getangle ( end )
public iterator < double [ ] > iterator ( ) {
final double syncedstart = mathutils . normalizeangle ( a [ 0 ] , reference ) - arc . getinf ( ) ;
final list < subhyperplane < sphere1d > > plus = new arraylist < subhyperplane < sphere1d > > ( ) ;
return next ;
private sublimitangle arcstart ( final double alpha ) {
public side side ( final arc arc ) {
import java . util . nosuchelementexception ;
while ( start ! = null & & !isarcstart ( start ) ) {
for ( final double [ ] a : this ) {
public subarcsiterator ( ) {
final double arclength = arc . getsup ( ) - arc . getinf ( ) ;
start = nextinternalnode ( start ) ;
boolean inminus = false ;
if ( syncedend > arclength ) {
node = nextinternalnode ( node ) ;
bsptree < sphere1d > end = start ;
minus . add ( arcend ( a [ 1 ] ) ) ;
import java . util . iterator ;
this . plus = plus ;
if ( start = = null ) {
if ( pending = = null ) {
return minus ;
return new limitangle ( new s1point ( alpha ) , false , tolerance ) . wholehyperplane ( ) ;
if ( inminus ) {
end = previousinternalnode ( end ) ;
public arcsset getminus ( ) {
if ( syncedend > mathutils . two pi + arclength ) {
plus . add ( arcstart ( minustoplus ) ) ;
pending = null ;
return new split ( plus . isempty ( ) ? null : new arcsset ( plus , tolerance ) ,
public void remove ( ) {
sum + = length * ( a [ 0 ] + a [ 1 ] ) ;
private final arcsset plus ;
final double minustoplus = arclength + arcoffset ;
node = childafter ( node ) ;
public boolean hasnext ( ) {
private split ( final arcsset plus , final arcsset minus ) {
return new limitangle ( new s1point ( alpha ) , true , tolerance ) . wholehyperplane ( ) ;
boolean inplus = false ;
public static class split {
getangle ( start ) , getangle ( end ) + mathutils . two pi
final double length = a [ 1 ] - a [ 0 ] ;
if ( inplus ) {
final double syncedend = a [ 1 ] - arcoffset ;
import org . apache . commons . math3 . geometry . partitioning . side ;
final double plustominus = mathutils . two pi + arcoffset ;
private void selectpending ( ) {
if ( syncedstart < arclength ) {
node = end ;
node = childbefore ( node ) ;
plus . add ( arcstart ( a [ 0 ] ) ) ;
return ;
final double reference = fastmath . pi + arc . getinf ( ) ;
plus . add ( arcend ( plustominus ) ) ;
final double minustoplus = mathutils . two pi + arclength + arcoffset ;
throw new unsupportedoperationexception ( ) ;
} ;
selectpending ( ) ;
inplus = true ;
private class subarcsiterator implements iterator < double [ ] > {
firststart = getfirstarcstart ( ) ;
final list < subhyperplane < sphere1d > > minus = new arraylist < subhyperplane < sphere1d > > ( ) ;
final double arcoffset = a [ 0 ] - syncedstart ;
return new splitsubhyperplane < sphere2d > ( this , null ) ;
}
import org . apache . commons . math3 . util . fastmath ;
final arcsset . split split = ( ( arcsset ) getremainingregion ( ) ) . split ( arc ) ;
import org . apache . commons . math3 . geometry . euclidean . threed . vector3d ;
return new splitsubhyperplane < sphere2d > ( null , this ) ;
return new splitsubhyperplane < sphere2d > ( plus = = null ? null : new subcircle ( thiscircle . copyself ( ) , plus ) ,
} else if ( angle > fastmath . pi - thiscircle . gettolerance ( ) ) {
minus = = null ? null : new subcircle ( thiscircle . copyself ( ) , minus ) ) ;
final arcsset minus = split . getminus ( ) ;
return side . hyper ;
final double angle = vector3d . angle ( thiscircle . getpole ( ) , othercircle . getpole ( ) ) ;
final arc arc = thiscircle . getinsidearc ( othercircle ) ;
} else {
if ( angle < thiscircle . gettolerance ( ) ) {
if ( angle < thiscircle . gettolerance ( ) | | angle > fastmath . pi - thiscircle . gettolerance ( ) ) {
return ( ( arcsset ) getremainingregion ( ) ) . side ( thiscircle . getinsidearc ( othercircle ) ) ;
final arcsset plus = split . getplus ( ) ;
return getremainingregion ( ) . side ( new orientedpoint ( x , direct , thisline . gettolerance ( ) ) ) ;
final subhyperplane < euclidean1d > subplus =
super ( new line ( start , end , tolerance ) , buildintervalset ( start , end , tolerance ) ) ;
new org . apache . commons . math3 . geometry . euclidean . twod . line ( q , p , tolerance ) . wholehyperplane ( ) ;
public subline ( final vector2d start , final vector2d end , final double tolerance ) {
* /
new org . apache . commons . math3 . geometry . euclidean . twod . line ( p , q , tolerance ) . wholehyperplane ( ) ;
new orientedpoint ( x , direct , tolerance ) . wholehyperplane ( ) ;
tolerance ) ;
final line line = new line ( start , end , tolerance ) ;
new org . apache . commons . math3 . geometry . euclidean . twod . line ( p , q , tolerance ) ;
return new splitsubhyperplane < euclidean2d > ( new subline ( thisline . copyself ( ) , new intervalsset ( plustree , tolerance ) ) ,
final subhyperplane < euclidean1d > subminus =
line . tosubspace ( ( point < euclidean2d > ) end ) . getx ( ) ,
final double tolerance = thisline . gettolerance ( ) ;
final double tolerance = thisplane . gettolerance ( ) ;
new orientedpoint ( x , !direct , tolerance ) . wholehyperplane ( ) ;
new subplane ( thisplane . copyself ( ) , new polygonsset ( minustree , tolerance ) ) ) ;
buildintervalset ( segment . getstart ( ) , segment . getend ( ) , segment . getline ( ) . gettolerance ( ) ) ) ;
private static intervalsset buildintervalset ( final vector2d start , final vector2d end , final double tolerance ) {
return new splitsubhyperplane < euclidean3d > ( new subplane ( thisplane . copyself ( ) , new polygonsset ( plustree , tolerance ) ) ,
final vector2d crossing = thisline . intersection ( otherline ) ;
super ( segment . getline ( ) ,
new subline ( thisline . copyself ( ) , new intervalsset ( minustree , tolerance ) ) ) ;
private boolean addarcstart ( final bsptree < sphere1d > tree , final double alpha , final boolean ignored ) {
final bsptree < sphere1d > minus = new bsptree < sphere1d > ( ) ;
return root ;
private bsptree < sphere1d > current ;
minusignored = addarcend ( minus , a [ 1 ] , minusignored ) ;
super ( localizedformats . inconsistent state at 2 pi wrapping ) ;
node = getfirstleaf ( node ) . getparent ( ) ;
if ( alpha > = mathutils . two pi - gettolerance ( ) ) {
for ( bsptree < sphere1d > n = root ; n ! = null ; n = previousinternalnode ( n ) ) {
current = end ;
minusignored = addarcstart ( minus , a [ 0 ] , minusignored ) ;
if ( ignored ) {
if ( alpha < = gettolerance ( ) ) {
throws inconsistentstateat2piwrapping {
private arcsset createsplitpart ( final bsptree < sphere1d > tree , final boolean ignored ) {
check2piconsistency ( ) ;
public arcsset ( final bsptree < sphere1d > tree , final double tolerance )
bsptree < sphere1d > largest = null ;
smallest = n ;
boolean minusignored = false ;
public static class inconsistentstateat2piwrapping extends mathillegalargumentexception {
boolean plusignored = false ;
if ( firststate ^ laststate ) {
if ( statebefore ^ stateafter ) {
final boolean stateafter = ( boolean ) getlastleaf ( root ) . getattribute ( ) ;
return leafbefore ( smallest ) ;
}
last . getplus ( ) . setattribute ( boolean . false ) ;
last . insertcut ( new limitangle ( new s1point ( alpha ) , false , gettolerance ( ) ) ) ;
current = firststart ;
private bsptree < sphere1d > getfirstleaf ( final bsptree < sphere1d > root ) {
minus . setattribute ( boolean . false ) ;
minusignored = addarcstart ( minus , plustominus , minusignored ) ;
final boolean laststate = ( boolean ) last . getattribute ( ) ;
import org . apache . commons . math3 . exception . mathillegalargumentexception ;
first . getminus ( ) . setattribute ( laststate ) ;
plusignored = addarcstart ( plus , minustoplus , plusignored ) ;
minusignored = addarcend ( minus , minustoplus , minusignored ) ;
} else {
final boolean firststate = ( boolean ) first . getattribute ( ) ;
return true ;
bsptree < sphere1d > root = gettree ( false ) ;
plusignored = addarcend ( plus , plustominus , plusignored ) ;
first . insertcut ( new limitangle ( new s1point ( 0 . 0 ) , true , gettolerance ( ) ) ) ;
first . getplus ( ) . setattribute ( firststate ) ;
current = null ;
private void check2piconsistency ( ) throws inconsistentstateat2piwrapping {
final bsptree < sphere1d > plus = new bsptree < sphere1d > ( ) ;
if ( tree . getcut ( ) = = null & & ! ( boolean ) tree . getattribute ( ) ) {
private static final long serialversionuid = 20140107l ;
private boolean addarcend ( final bsptree < sphere1d > tree , final double alpha , final boolean ignored ) {
for ( bsptree < sphere1d > n = root ; n ! = null ; n = nextinternalnode ( n ) ) {
last . getminus ( ) . setattribute ( boolean . true ) ;
return new split ( createsplitpart ( plus , plusignored ) , createsplitpart ( minus , minusignored ) ) ;
return ;
return leafafter ( largest ) ;
throw new inconsistentstateat2piwrapping ( ) ;
plusignored = addarcend ( plus , a [ 1 ] , plusignored ) ;
public inconsistentstateat2piwrapping ( ) {
bsptree < sphere1d > start = current ;
last . setattribute ( null ) ;
last . insertcut ( new limitangle ( new s1point ( alpha ) , true , gettolerance ( ) ) ) ;
plusignored = addarcstart ( plus , a [ 0 ] , plusignored ) ;
plus . setattribute ( boolean . false ) ;
final bsptree < sphere1d > first = getfirstleaf ( tree ) ;
return new arcsset ( tree , gettolerance ( ) ) ;
public arcsset ( final collection < subhyperplane < sphere1d > > boundary , final double tolerance )
bsptree < sphere1d > smallest = null ;
final bsptree < sphere1d > last = getlastleaf ( tree ) ;
return null ;
final boolean statebefore = ( boolean ) getfirstleaf ( root ) . getattribute ( ) ;
last . setattribute ( boolean . true ) ;
if ( root . getcut ( ) = = null ) {
return ignored ;
largest = n ;
private bsptree < sphere1d > getlastleaf ( final bsptree < sphere1d > root ) {
exclude . addall ( neighbours ) ;
final gaussian neighbourhooddecay
updateneighbouringneuron ( n , features , neighbourhooddecay . value ( radius ) ) ;
final neuron best = findandupdatebestneuron ( net ,
for ( neuron n : neighbours ) {
import org . apache . commons . math3 . ml . neuralnet . neuron ;
private final distancemeasure distance ;
double [ ] features ) {
package org . apache . commons . math3 . ml . neuralnet . sofm ;
return s . subtract ( c ) . mapmultiplytoself ( learningrate ) . add ( c ) . toarray ( ) ;
final double [ ] expect = n . getfeatures ( ) ;
private void updateneighbouringneuron ( neuron n ,
this . learningfactor = learningfactor ;
} while ( radius < = currentneighbourhood ) ;
neighbours . add ( best ) ;
public kohonenupdateaction ( distancemeasure distance ,
learningrate ) ;
double [ ] features ,
import java . util . concurrent . atomic . atomiclong ;
}
double learningrate ) {
import java . util . hashset ;
if ( best . compareandsetfeatures ( expect , update ) ) {
1d / currentneighbourhood ) ;
public void update ( network net ,
= new gaussian ( currentlearning ,
import org . apache . commons . math3 . linear . arrayrealvector ;
+ + radius ;
if ( currentneighbourhood > 0 ) {
public class kohonenupdateaction implements updateaction {
final double [ ] expect = best . getfeatures ( ) ;
final int currentneighbourhood = neighbourhoodsize . value ( numcalls ) ;
private neuron findandupdatebestneuron ( network net ,
0 ,
public long getnumberofcalls ( ) {
import org . apache . commons . math3 . ml . neuralnet . network ;
return best ;
import org . apache . commons . math3 . ml . distance . distancemeasure ;
learningfactorfunction learningfactor ,
import org . apache . commons . math3 . ml . neuralnet . maputils ;
int radius = 1 ;
final neuron best = maputils . findbest ( features , net , distance ) ;
final double currentlearning = learningfactor . value ( numcalls ) ;
neighbours = net . getneighbours ( neighbours , exclude ) ;
currentlearning ) ;
private final learningfactorfunction learningfactor ;
final long numcalls = numberofcalls . incrementandget ( ) ;
final double [ ] update = computefeatures ( expect ,
return numberofcalls . get ( ) ;
collection < neuron > neighbours = new hashset < neuron > ( ) ;
private final neighbourhoodsizefunction neighbourhoodsize ;
@ override
do {
private double [ ] computefeatures ( double [ ] current ,
import java . util . collection ;
import org . apache . commons . math3 . ml . neuralnet . updateaction ;
double [ ] sample ,
if ( n . compareandsetfeatures ( expect , update ) ) {
final arrayrealvector s = new arrayrealvector ( sample , false ) ;
while ( true ) {
this . distance = distance ;
this . neighbourhoodsize = neighbourhoodsize ;
exclude . add ( best ) ;
break ;
features ,
neighbourhoodsizefunction neighbourhoodsize ) {
final hashset < neuron > exclude = new hashset < neuron > ( ) ;
import org . apache . commons . math3 . analysis . function . gaussian ;
final arrayrealvector c = new arrayrealvector ( current , false ) ;
private final atomiclong numberofcalls = new atomiclong ( - 1 ) ;
addarclimit ( minus , minustoplus , false ) ;
addarclimit ( minus , a [ 1 ] , false ) ;
addarclimit ( minus , plustominus , true ) ;
addarclimit ( minus , a [ 0 ] , true ) ;
node . setattribute ( null ) ;
node . getplus ( ) . setattribute ( boolean . false ) ;
node . insertcut ( limit ) ;
private void addarclimit ( final bsptree < sphere1d > tree , final double alpha , final boolean isstart ) {
leafbefore ( node ) . setattribute ( boolean . valueof ( !isstart ) ) ;
private arcsset createsplitpart ( final bsptree < sphere1d > tree ) {
if ( node . getcut ( ) ! = null ) {
addarclimit ( plus , a [ 0 ] , true ) ;
addarclimit ( plus , a [ 1 ] , false ) ;
node . getminus ( ) . setattribute ( boolean . true ) ;
final limitangle limit = new limitangle ( new s1point ( alpha ) , !isstart , gettolerance ( ) ) ;
final bsptree < sphere1d > node = tree . getcell ( limit . getlocation ( ) , gettolerance ( ) ) ;
addarclimit ( plus , minustoplus , true ) ;
return new split ( createsplitpart ( plus ) , createsplitpart ( minus ) ) ;
addarclimit ( plus , plustominus , false ) ;
final double rounded = ( new bigdecimal ( double . tostring ( x ) )
return rounded = = 0 . 0 ? rounded * fastmath . copysign ( 1d , x ) : rounded ;
final normaldistribution normaldistribution = new normaldistribution ( ) ;
final double lowerbound = factor * ( modifiedsuccessratio - difference ) ;
import org . apache . commons . math3 . distribution . normaldistribution ;
public class wilsonscoreinterval implements binomialconfidenceinterval {
final double alpha = ( 1 . 0 - confidencelevel ) / 2 ;
final double zsquared = fastmath . pow ( z , 2 ) ;
}
intervalutils . checkparameters ( numberoftrials , numberofsuccesses , confidencelevel ) ;
public confidenceinterval createinterval ( int numberoftrials , int numberofsuccesses , double confidencelevel ) {
final double upperbound = factor * ( modifiedsuccessratio + difference ) ;
final double factor = 1 . 0 / ( 1 + ( 1 . 0 / numberoftrials ) * zsquared ) ;
import org . apache . commons . math3 . util . fastmath ;
final double modifiedsuccessratio = mean + ( 1 . 0 / ( 2 * numberoftrials ) ) * zsquared ;
final double difference = z *
fastmath . sqrt ( 1 . 0 / numberoftrials * mean * ( 1 - mean ) +
final double z = normaldistribution . inversecumulativeprobability ( 1 - alpha ) ;
@ override
final double mean = ( double ) numberofsuccesses / ( double ) numberoftrials ;
return new confidenceinterval ( lowerbound , upperbound , confidencelevel ) ;
( 1 . 0 / ( 4 * fastmath . pow ( numberoftrials , 2 ) ) * zsquared ) ) ;
package org . apache . commons . math3 . stat . interval ;
final int j = ( i + 1 ) % limits . size ( ) ;
plus . add ( a [ 0 ] ) ;
minus . add ( a [ 1 ] ) ;
if ( fastmath . abs ( lb - la ) < = gettolerance ( ) ) {
addarclimit ( tree , limits . get ( i + 1 ) , false ) ;
i = i - 1 ;
for ( int i = 0 ; i < limits . size ( ) ; + + i ) {
for ( int i = 0 ; i < limits . size ( ) - 1 ; i + = 2 ) {
node . setattribute ( null ) ;
}
final double lb = mathutils . normalizeangle ( limits . get ( j ) , la ) ;
node . getplus ( ) . setattribute ( boolean . false ) ;
if ( syncedstart < = arclength - gettolerance ( ) | | syncedend > = mathutils . two pi + gettolerance ( ) ) {
final list < double > minus = new arraylist < double > ( ) ;
plus . add ( plustominus ) ;
if ( syncedend > = arclength + gettolerance ( ) ) {
minus . add ( a [ 0 ] ) ;
if ( limits . isempty ( ) ) {
} else {
node . insertcut ( limit ) ;
final list < double > plus = new arraylist < double > ( ) ;
plus . add ( a [ 1 ] ) ;
limits . remove ( j ) ;
if ( tree . getcut ( ) = = null ) {
limits . remove ( i ) ;
addarclimit ( tree , limits . get ( i ) , true ) ;
final double la = limits . get ( i ) ;
minus . add ( plustominus ) ;
return new arcsset ( new bsptree < sphere1d > ( boolean . true ) , gettolerance ( ) ) ;
node . getminus ( ) . setattribute ( boolean . true ) ;
minus . add ( minustoplus ) ;
bsptree < sphere1d > tree = new bsptree < sphere1d > ( boolean . false ) ;
throw new mathinternalerror ( ) ;
return null ;
private arcsset createsplitpart ( final list < double > limits ) {
final double lstart = limits . remove ( 0 ) ;
limits . add ( limits . remove ( 0 ) + mathutils . two pi ) ;
plus . add ( minustoplus ) ;
final double lend = limits . remove ( limits . size ( ) - 1 ) ;
if ( lend - lstart > fastmath . pi ) {
if ( j > 0 ) {
return ball ;
private final int max ;
private enclosingball < s , p > movetofrontball ( final list < p > extreme , final list < p > support ) {
double dmax = - 1 . 0 ;
ball = movetofrontball ( extreme , support ) ;
for ( int i = 0 ; i < extreme . size ( ) ; + + i ) {
extreme . add ( points . get ( 0 ) ) ;
list < p > support = new arraylist < p > ( max ) ;
this . max = dimension + 1 ;
enclosingball < s , p > ball = movetofrontball ( extreme , support ) ;
if ( ball . contains ( farthest , tolerance ) ) {
extreme . set ( j , extreme . get ( j - 1 ) ) ;
private final supportballgenerator < s , p > generator ;
import java . util . list ;
return generator . ballonsupport ( new arraylist < p > ( ) ) ;
public p selectfarthest ( final list < p > points , final enclosingball < s , p > ball ) {
}
support . add ( farthest ) ;
public interface encloser < s extends space , p extends point < s > > {
for ( int j = i ; j > 1 ; - - j ) {
enclosingball < s , p > ball = generator . ballonsupport ( support ) ;
final supportballgenerator < s , p > generator ) {
for ( final p point : points ) {
this . tolerance = tolerance ;
protected welzlencloser ( final double tolerance , final int dimension ,
public enclosingball < s , p > enclose ( final list < p > points ) {
p farthest = null ;
private final double tolerance ;
dmax = d ;
return pivotingball ( points ) ;
public class welzlencloser < s extends space , p extends point < s > > implements encloser < s , p > {
extreme . add ( 0 , farthest ) ;
final double d = point . distance ( center ) ;
if ( !ball . contains ( pi , tolerance ) ) {
support . clear ( ) ;
if ( points = = null | | points . isempty ( ) ) {
farthest = point ;
if ( d > dmax ) {
support . add ( pi ) ;
final p center = ball . getcenter ( ) ;
list < p > extreme = new arraylist < p > ( max ) ;
package org . apache . commons . math3 . geometry . enclosing ;
import org . apache . commons . math3 . geometry . point ;
if ( ball . getsupportsize ( ) < max ) {
this . generator = generator ;
private enclosingball < s , p > pivotingball ( final list < p > points ) {
import java . util . arraylist ;
enclosingball < s , p > enclose ( list < p > points ) ;
extreme . sublist ( ball . getsupportsize ( ) , extreme . size ( ) ) . clear ( ) ;
final p farthest = selectfarthest ( points , ball ) ;
extreme . set ( 0 , pi ) ;
while ( true ) {
final p pi = extreme . get ( i ) ;
ball = movetofrontball ( extreme . sublist ( i + 1 , extreme . size ( ) ) , support ) ;
return farthest ;
import org . apache . commons . math3 . geometry . space ;
m12 , m01 , m20 , m10 , m21 , m02
if ( support . size ( ) < 4 ) {
final double [ ] c4 = new double [ ] {
final double [ ] c1 = new double [ ] {
}
c1 [ 1 ] , c1 [ 0 ] , c1 [ 3 ] , - c1 [ 0 ] , - c1 [ 3 ] , - c1 [ 1 ] ,
package org . apache . commons . math3 . geometry . euclidean . threed ;
va . getnormsq ( ) , vb . getnormsq ( ) , vc . getnormsq ( )
final vector2d va = support . get ( 0 ) ;
m13 , m32 , m21 , m23 , m12 , m31 ,
} else {
final double m03 = c2 [ 0 ] * c3 [ 3 ] ;
m03 , m31 , m10 , m13 , m01 , m30 ,
final vector3d center = new vector3d ( 0 . 5 * m12 / m11 , - 0 . 5 * m13 / m11 , 0 . 5 * m14 / m11 ) ;
return new enclosingball < euclidean3d , vector3d > ( vector3d . zero , - 1 . 0 ) ;
private double minor ( final double [ ] c1 , final double [ ] c2 , final double [ ] c3 ) {
final double m01 = c2 [ 0 ] * c3 [ 1 ] ;
va . gety ( ) , vb . gety ( ) , vc . gety ( ) , vd . gety ( )
final vector3d vd = support . get ( 3 ) ;
va , vb ) ;
c1 [ 2 ] , c1 [ 1 ] , c1 [ 3 ] , - c1 [ 1 ] , - c1 [ 3 ] , - c1 [ 2 ] ,
new diskgenerator ( ) . ballonsupport ( arrays . aslist ( p . tosubspace ( va ) ,
final double m23 = c2 [ 2 ] * c3 [ 3 ] ;
final vector3d vb = support . get ( 1 ) ;
public class spheregenerator implements supportballgenerator < euclidean3d , vector3d > {
import org . apache . commons . math3 . geometry . enclosing . supportballgenerator ;
final double [ ] c2 = new double [ ] {
final double m13 = minor ( c1 , c2 , c4 ) ;
final double m11 = minor ( c2 , c3 , c4 ) ;
public class diskgenerator implements supportballgenerator < euclidean2d , vector2d > {
} ) ;
final plane p = new plane ( va , vb , vc ,
p . tosubspace ( vb ) ,
} ,
final double m32 = c2 [ 3 ] * c3 [ 2 ] ;
final vector3d vc = support . get ( 2 ) ;
final double m20 = c2 [ 2 ] * c3 [ 0 ] ;
final vector3d va = support . get ( 0 ) ;
p . tosubspace ( vc ) ) ) ;
final enclosingball < euclidean2d , vector2d > disk =
1 . 0e - 10 * ( va . getnorm1 ( ) + vb . getnorm1 ( ) + vc . getnorm1 ( ) ) ) ;
final double m13 = c2 [ 1 ] * c3 [ 3 ] ;
final double m12 = minor ( c1 , c3 , c4 ) ;
final double m31 = c2 [ 3 ] * c3 [ 1 ] ;
package org . apache . commons . math3 . geometry . euclidean . twod ;
return new enclosingball < euclidean3d , vector3d > ( va , 0 , va ) ;
va . getx ( ) , vb . getx ( ) , vc . getx ( ) , vd . getx ( )
va . getz ( ) , vb . getz ( ) , vc . getz ( ) , vd . getz ( )
import org . apache . commons . math3 . geometry . enclosing . enclosingball ;
return new enclosingball < euclidean2d , vector2d > ( va , 0 , va ) ;
import java . util . list ;
public enclosingball < euclidean2d , vector2d > ballonsupport ( final list < vector2d > support ) {
disk . getradius ( ) , va , vb , vc ) ;
public enclosingball < euclidean3d , vector3d > ballonsupport ( final list < vector3d > support ) {
import org . apache . commons . math3 . util . matharrays ;
va . gety ( ) , vb . gety ( ) , vc . gety ( )
final double m10 = c2 [ 1 ] * c3 [ 0 ] ;
return new enclosingball < euclidean3d , vector3d > ( new vector3d ( 0 . 5 , va , 0 . 5 , vb ) ,
return new enclosingball < euclidean2d , vector2d > ( vector2d . zero , - 1 . 0 ) ;
c2 [ 1 ] , c2 [ 0 ] , c2 [ 2 ] , c2 [ 1 ] , c2 [ 2 ] , c2 [ 0 ]
final vector2d vb = support . get ( 1 ) ;
final vector2d center = new vector2d ( 0 . 5 * m12 / m11 , - 0 . 5 * m13 / m11 ) ;
return new enclosingball < euclidean2d , vector2d > ( center , center . distance ( va ) , va , vb , vc ) ;
final double m12 = minor ( c1 , c3 ) ;
new double [ ] {
if ( support . size ( ) < 3 ) {
final double m21 = c2 [ 2 ] * c3 [ 1 ] ;
final double m14 = minor ( c1 , c2 , c3 ) ;
import org . apache . commons . math3 . geometry . euclidean . twod . euclidean2d ;
c1 [ 0 ] , c1 [ 3 ] , c1 [ 2 ] , - c1 [ 3 ] , - c1 [ 0 ] , - c1 [ 2 ] ,
import org . apache . commons . math3 . geometry . euclidean . twod . diskgenerator ;
c1 [ 0 ] , c1 [ 2 ] , c1 [ 1 ] , - c1 [ 2 ] , - c1 [ 0 ] , - c1 [ 1 ]
return new enclosingball < euclidean2d , vector2d > ( new vector2d ( 0 . 5 , va , 0 . 5 , vb ) ,
va , vb , vc , vd ) ;
import org . apache . commons . math3 . geometry . euclidean . twod . vector2d ;
final double m13 = minor ( c1 , c2 ) ;
import java . util . arrays ;
0 . 5 * va . distance ( vb ) ,
final vector2d vc = support . get ( 2 ) ;
return new enclosingball < euclidean3d , vector3d > ( center , center . distance ( va ) ,
va . getx ( ) , vb . getx ( ) , vc . getx ( )
va . getnormsq ( ) , vb . getnormsq ( ) , vc . getnormsq ( ) , vd . getnormsq ( )
private double minor ( final double [ ] c1 , final double [ ] c2 ) {
return new enclosingball < euclidean3d , vector3d > ( p . tospace ( disk . getcenter ( ) ) ,
m23 , m02 , m30 , m20 , m32 , m03 ,
final double [ ] c3 = new double [ ] {
} ;
final double m12 = c2 [ 1 ] * c3 [ 2 ] ;
final double m02 = c2 [ 0 ] * c3 [ 2 ] ;
if ( support . size ( ) < 1 ) {
return matharrays . linearcombination ( new double [ ] {
final double m11 = minor ( c2 , c3 ) ;
if ( support . size ( ) < 2 ) {
final double m30 = c2 [ 3 ] * c3 [ 0 ] ;
final list < vector2d > pointssortedbyxaxis = new arraylist < vector2d > ( points ) ;
public monotonechain ( ) {
final vector2d p = pointssortedbyxaxis . get ( idx ) ;
final vector2d linepoint2 ) {
return ( linepoint2 . getx ( ) - linepoint1 . getx ( ) ) * ( point . gety ( ) - linepoint1 . gety ( ) ) -
public class monotonechain implements convexhullgenerator2d {
return new convexhull2d ( hullvertices , tolerance ) ;
import org . apache . commons . math3 . exception . nullargumentexception ;
for ( int idx = 0 ; idx < lowerhull . size ( ) - 1 ; idx + + ) {
hullvertices . add ( upperhull . get ( idx ) ) ;
final vector2d p1 = upperhull . get ( size - 2 ) ;
final list < vector2d > upperhull = new arraylist < vector2d > ( ) ;
import java . util . list ;
}
package org . apache . commons . math3 . geometry . euclidean . twod . hull ;
while ( lowerhull . size ( ) > = 2 ) {
lowerhull . add ( p ) ;
private static final double default tolerance = 1e - 10 ;
} ) ;
for ( int idx = pointssortedbyxaxis . size ( ) - 1 ; idx > = 0 ; idx - - ) {
for ( int idx = 0 ; idx < upperhull . size ( ) - 1 ; idx + + ) {
} else {
public monotonechain ( final double tolerance ) {
public int compare ( final vector2d o1 , final vector2d o2 ) {
this . tolerance = tolerance ;
import org . apache . commons . math3 . geometry . euclidean . twod . vector2d ;
private final double tolerance ;
return new convexhull2d ( points , tolerance ) ;
final vector2d linepoint1 ,
final int size = lowerhull . size ( ) ;
import org . apache . commons . math3 . util . fastmath ;
return ( int ) fastmath . signum ( o1 . gety ( ) - o2 . gety ( ) ) ;
if ( getlocation ( p , p1 , p2 ) < = 0 ) {
if ( diff = = 0 ) {
final int diff = ( int ) fastmath . signum ( o1 . getx ( ) - o2 . getx ( ) ) ;
collections . sort ( pointssortedbyxaxis , new comparator < vector2d > ( ) {
import java . util . collections ;
public convexhull2d generate ( final collection < vector2d > points ) throws nullargumentexception {
private double getlocation ( final vector2d point ,
( point . getx ( ) - linepoint1 . getx ( ) ) * ( linepoint2 . gety ( ) - linepoint1 . gety ( ) ) ;
lowerhull . remove ( size - 1 ) ;
this ( default tolerance ) ;
final int size = upperhull . size ( ) ;
mathutils . checknotnull ( points ) ;
while ( upperhull . size ( ) > = 2 ) {
upperhull . remove ( size - 1 ) ;
final vector2d p1 = lowerhull . get ( size - 2 ) ;
import java . util . comparator ;
list < vector2d > hullvertices = new arraylist < vector2d > ( lowerhull . size ( ) + upperhull . size ( ) - 2 ) ;
import org . apache . commons . math3 . util . mathutils ;
import java . util . collection ;
upperhull . add ( p ) ;
import java . util . arraylist ;
final vector2d p2 = lowerhull . get ( size - 1 ) ;
final list < vector2d > lowerhull = new arraylist < vector2d > ( ) ;
if ( points . size ( ) < 3 ) {
hullvertices . add ( lowerhull . get ( idx ) ) ;
break ;
for ( vector2d p : pointssortedbyxaxis ) {
return diff ;
final vector2d p2 = upperhull . get ( size - 1 ) ;
final int size = hull . size ( ) ;
public monotonechain ( ) {
super ( ) ;
while ( hull . size ( ) > = 2 ) {
public monotonechain ( final boolean includecollinearpoints ) {
final vector2d p2 = hull . get ( size - 1 ) ;
final list < vector2d > hullvertices = new arraylist < vector2d > ( lowerhull . size ( ) + upperhull . size ( ) - 2 ) ;
if ( distancetocurrent > distancetolast ) {
updatehull ( p , lowerhull ) ;
super ( includecollinearpoints , tolerance ) ;
if ( hull . size ( ) = = 1 ) {
public collection < vector2d > generatehull ( final collection < vector2d > points ) {
}
if ( distancetocurrent < tolerance | | p2 . distance ( point ) < tolerance ) {
hull . add ( index , point ) ;
final double offset = point . crossproduct ( p1 , p2 ) ;
} else {
updatehull ( p , upperhull ) ;
final vector2d p1 = hull . get ( 0 ) ;
final double tolerance = gettolerance ( ) ;
super ( includecollinearpoints ) ;
final double distancetocurrent = p1 . distance ( point ) ;
} else if ( offset < 0 ) {
final double distancetolast = p1 . distance ( p2 ) ;
hull . remove ( size - 1 ) ;
return ;
if ( isincludecollinearpoints ( ) ) {
hull . add ( point ) ;
@ override
public monotonechain ( final boolean includecollinearpoints , final double tolerance ) {
private void updatehull ( final vector2d point , final list < vector2d > hull ) {
if ( fastmath . abs ( offset ) < tolerance ) {
if ( p1 . distance ( point ) < tolerance ) {
break ;
final vector2d p1 = hull . get ( size - 2 ) ;
public class monotonechain extends abstractconvexhullgenerator2d {
final int index = distancetocurrent < distancetolast ? size - 1 : size ;
return hullvertices ;
vector2d lastpoint = null ;
} else if ( size = = 2 ) {
for ( vector2d point : vertices ) {
if ( size < = 1 ) {
if ( linesegments = = null ) {
this . linesegments [ index ] =
final int size = vertices . length ;
int index = 0 ;
new segment ( lastpoint , point , new line ( lastpoint , point , tolerance ) ) ;
}
private segment [ ] retrievelinesegments ( ) {
this . linesegments = new segment [ 0 ] ;
} else {
this . tolerance = tolerance ;
private final double tolerance ;
firstpoint = point ;
final vector2d p1 = vertices [ 0 ] ;
this . linesegments = new segment [ size ] ;
final line [ ] linearray = new line [ segments . length ] ;
final segment [ ] segments = retrievelinesegments ( ) ;
if ( lastpoint = = null ) {
return linesegments ;
vector2d firstpoint = null ;
this . linesegments [ index + + ] =
lastpoint = point ;
private transient segment [ ] linesegments ;
this . linesegments = new segment [ 1 ] ;
this . linesegments [ 0 ] = new segment ( p1 , p2 , new line ( p1 , p2 , tolerance ) ) ;
new segment ( lastpoint , firstpoint , new line ( lastpoint , firstpoint , tolerance ) ) ;
return retrievelinesegments ( ) . clone ( ) ;
linearray [ i ] = segments [ i ] . getline ( ) ;
final vector2d p2 = vertices [ 1 ] ;
for ( int i = 0 ; i < segments . length ; i + + ) {
double sign = 0 . 0 ;
final double cross = fastmath . signum ( matharrays . linearcombination ( d1 . getx ( ) , d2 . gety ( ) ,
return false ;
final vector2d p3 = hullvertices [ i = = hullvertices . length - 1 ? 0 : i + 1 ] ;
final vector2d d1 = p2 . subtract ( p1 ) ;
final vector2d d2 = p3 . subtract ( p2 ) ;
}
import org . apache . commons . math3 . exception . mathillegalargumentexception ;
public convexhull2d ( final vector2d [ ] vertices , final double tolerance )
import org . apache . commons . math3 . util . matharrays ;
- d1 . gety ( ) , d2 . getx ( ) ) ) ;
import org . apache . commons . math3 . exception . util . localizedformats ;
return true ;
import org . apache . commons . math3 . util . fastmath ;
final vector2d p2 = hullvertices [ i ] ;
sign = cross ;
if ( !isconvex ( vertices ) ) {
this . vertices = vertices . clone ( ) ;
private boolean isconvex ( final vector2d [ ] hullvertices ) {
for ( int i = 0 ; i < hullvertices . length ; i + + ) {
final vector2d p1 = hullvertices [ i = = 0 ? hullvertices . length - 1 : i - 1 ] ;
if ( sign ! = 0 . 0 & & cross ! = sign ) {
throw new mathillegalargumentexception ( localizedformats . not convex ) ;
if ( hullvertices . length < 3 ) {
if ( cross ! = 0 . 0 ) {
throws mathillegalargumentexception {
ak [ l ] + = grad [ k ] * grad [ l ] ;
final realmatrix weightedjacobian = value . computejacobian ( ) ;
b [ j ] + = residual * grad [ j ] ;
final int nc = lsp . getparametersize ( ) ;
final int nr = lsp . getobservationsize ( ) ;
final convergencechecker < evaluation > checker
current ,
current = previous ;
evaluation current = problem . evaluate ( currentpoint ) ;
current = problem . evaluate ( currentpoint ) ;
double currentcost = current . computecost ( ) ;
currentcost = current . computecost ( ) ;
double [ ] currentresiduals = current . computeresiduals ( ) ;
return new optimumimpl ( current , iterationcounter . getcount ( ) , evaluationcounter . getcount ( ) ) ;
final evaluation previous = current ;
= qrdecomposition ( current . computejacobian ( ) , solvedcols ) ;
currentresiduals = current . computeresiduals ( ) ;
currentpoint . setentry ( i , currentpoint . getentry ( i ) + dx . getentry ( i ) ) ;
final double residual = currentresiduals . getentry ( i ) ;
final realvector currentpoint = lsp . getstart ( ) ;
final double [ ] currentpoint = problem . getstart ( ) . toarray ( ) ;
double [ ] currentresiduals = current . computeresiduals ( ) . toarray ( ) ;
currentresiduals = current . computeresiduals ( ) . toarray ( ) ;
current = problem . evaluate ( new arrayrealvector ( currentpoint , false ) ) ;
final realvector dx = solver . solve ( new arrayrealvector ( b , false ) ) ;
import org . apache . commons . math3 . linear . arrayrealvector ;
evaluation current = problem . evaluate ( new arrayrealvector ( currentpoint , false ) ) ;
import org . apache . commons . math3 . linear . realvector ;
final realvector currentresiduals = current . computeresiduals ( ) ;
private unweightedevaluation ( final realvector values ,
public realvector getvalue ( ) {
final realvector target ,
return start . getdimension ( ) ;
this . values = values ;
return this . values ;
super ( target . getdimension ( ) ) ;
private final realvector point ;
public realvector getpoint ( ) {
return new unweightedevaluation (
value . getfirst ( ) ,
super ( maxevaluations , maxiterations , checker ) ;
this . target = target ;
}
localleastsquaresproblem ( final multivariatejacobianfunction model ,
public evaluation evaluate ( final realvector point ) {
extends abstractoptimizationproblem < evaluation >
return this . jacobian ;
public int getobservationsize ( ) {
final realmatrix jacobian ,
final int maxiterations ) {
point ) ;
this . target ,
public realvector getstart ( ) {
final convergencechecker < evaluation > checker ,
private final realvector values ;
public realvector getresiduals ( ) {
return this . point ;
final realvector start ,
private static class localleastsquaresproblem
value . getsecond ( ) ,
return start = = null ? null : start . copy ( ) ;
this . model = model ;
return new localleastsquaresproblem (
public int getparametersize ( ) {
this . jacobian = jacobian ;
return target . getdimension ( ) ;
private final realmatrix jacobian ;
import org . apache . commons . math3 . optim . abstractoptimizationproblem ;
private static class unweightedevaluation extends abstractevaluation {
this . start = start ;
return target . subtract ( this . getvalue ( ) ) ;
final pair < realvector , realmatrix > value = this . model . value ( point ) ;
private realvector start ;
private multivariatejacobianfunction model ;
private realvector target ;
implements leastsquaresproblem {
this . point = point ;
private final realvector target ;
final int maxevaluations ,
public realmatrix getjacobian ( ) {
final realvector point ) {
h = new bigfraction ( hdouble , 1 . 0e - 20 , 10000 ) ;
if ( i - j + 1 > 0 ) {
hdata [ m - 1 ] [ i ] = hdata [ m - 1 ] [ i ] . subtract ( hpowers [ m - i - 1 ] ) ;
return d ;
final double cdf y = yindex > = 0 ? ( yindex + 1d ) / m : ( - yindex - 1d ) / m ;
public double kolmogorovsmirnovtest ( double [ ] x , double [ ] y ) {
bigfraction pfrac = hpower . getentry ( k - 1 , k - 1 ) ;
final int m = hbigfraction . getrowdimension ( ) ;
final int n = sx . length ;
private void copypartition ( double [ ] nset , double [ ] mset , int [ ] nseti , int n , int m ) {
mset [ k + + ] = i ;
}
final double nd = n ;
final bigfraction [ ] hpowers = new bigfraction [ m ] ;
if ( curd > supd ) {
return exact ? exactk ( d , n ) : roundedk ( d , n ) ;
public double cdf ( double d , int n )
return res ;
hdata [ i ] [ j ] = bigfraction . zero ;
h = new bigfraction ( hdouble , 1 . 0e - 5 , 10000 ) ;
if ( x . length * y . length < small sample product ) {
protected static final int large sample product = 10000 ;
} else {
return montecarlop ( kolmogorovsmirnovstatistic ( x , y ) , x . length , y . length , strict , monte carlo iterations ) ;
int sign = - 1 ;
int tail = 0 ;
hdata [ m - 1 ] [ 0 ] = hdata [ m - 1 ] [ 0 ] . add ( h . multiply ( 2 ) . subtract ( 1 ) . pow ( m ) ) ;
for ( int i = 0 ; i < n ; i + + ) {
throws matharithmeticexception {
private fieldmatrix < bigfraction > createh ( double d , int n )
int j = 0 ;
import org . apache . commons . math3 . exception . toomanyiterationsexception ;
final double currd = fastmath . max ( yi - ( i - 1 ) / nd , i / nd - yi ) ;
long tail = 0 ;
protected static final int small sample product = 200 ;
iterator < int [ ] > combinationsiterator = combinatoricsutils . combinationsiterator ( n + m , n ) ;
arrays . sort ( sx ) ;
return cdf ( d , n , true ) ;
return kolmogorovsmirnovtest ( distribution , data ) < alpha ;
final double cdf x = xindex > = 0 ? ( xindex + 1d ) / n : ( - xindex - 1d ) / n ;
throw new outofrangeexception ( localizedformats . out of bound significance level , alpha , 0 , 0 . 5 ) ;
return 0 ;
for ( int i = 1 ; i < m ; + + i ) {
return pfrac ;
final double cdf y = ( i + 1d ) / m ;
if ( i = = maxiterations ) {
private final randomgenerator rng ;
system . arraycopy ( data , 0 , datacopy , 0 , n ) ;
if ( ( alpha < = 0 ) | | ( alpha > 0 . 5 ) ) {
try {
final double dm = m ;
for ( int j = 0 ; j < i + 1 ; + + j ) {
import org . apache . commons . math3 . distribution . realdistribution ;
copypartition ( nset , mset , nplusmset , n , m ) ;
if ( h . compareto ( bigfraction . one half ) = = 1 ) {
public class kolmogorovsmirnovtest {
checkarray ( data ) ;
protected static final double ks sum cauchy criterion = 1e - 20 ;
protected static final int monte carlo iterations = 1000000 ;
public double kolmogorovsmirnovtest ( realdistribution distribution , double [ ] data ) {
import org . apache . commons . math3 . exception . matharithmeticexception ;
public double kssum ( double t , double tolerance , int maxiterations ) {
final int n = data . length ;
final double [ ] mset = new double [ m ] ;
this . rng = rng ;
final int yindex = arrays . binarysearch ( sy , sx [ i ] ) ;
checkarray ( y ) ;
protected static final int maximum partial sum count = 100000 ;
partialsum + = sign * delta ;
final double ninv = 1 / ( ( double ) n ) ;
h = new bigfraction ( hdouble , 1 . 0e - 10 , 10000 ) ;
import org . apache . commons . math3 . exception . util . localizedformats ;
hpowers [ i ] = h . multiply ( hpowers [ i - 1 ] ) ;
final double cdf x = ( i + 1d ) / n ;
import org . apache . commons . math3 . fraction . fractionconversionexception ;
public double exactp ( double d , int n , int m , boolean strict ) {
final int xindex = arrays . binarysearch ( sx , sy [ i ] ) ;
for ( int i = 1 ; i < = n ; i + + ) {
pfrac = pfrac . multiply ( i ) . divide ( n ) ;
if ( hdouble > = 1 ) {
d = currd ;
final double yi = distribution . cumulativeprobability ( datacopy [ i - 1 ] ) ;
import org . apache . commons . math3 . exception . insufficientdataexception ;
public kolmogorovsmirnovtest ( ) {
return supd ;
return 1 - 2 * math . pow ( 1 - d , n ) ;
if ( currd > d ) {
if ( x . length * y . length < large sample product ) {
if ( d < = ninvhalf ) {
throw new nullargumentexception ( localizedformats . null not allowed ) ;
public double kolmogorovsmirnovtest ( realdistribution distribution , double [ ] data , boolean exact ) {
import java . math . bigdecimal ;
final double x = - 2 * t * t ;
public double cdfexact ( double d , int n )
import org . apache . commons . math3 . util . combinatoricsutils ;
if ( curd > d ) {
sign * = - 1 ;
res * = i * f ;
final double hdouble = k - n * d ;
import org . apache . commons . math3 . linear . array2drowrealmatrix ;
} else if ( ninvhalf < d & & d < = ninv ) {
final realmatrix h = new array2drowrealmatrix ( m , m ) ;
arrays . sort ( sy ) ;
return kolmogorovsmirnovtest ( distribution , data , false ) ;
matharrays . shuffle ( nplusmset , rng ) ;
double supd = 0d ;
for ( int i = 0 ; i < m ; i + + ) {
i + + ;
for ( int i = 0 ; i < n + m ; i + + ) {
double res = 1 ;
import org . apache . commons . math3 . fraction . bigfractionfield ;
while ( delta > tolerance & & i < maxiterations ) {
import org . apache . commons . math3 . exception . outofrangeexception ;
for ( int g = 2 ; g < = i - j + 1 ; + + g ) {
throw new numberistoolargeexception ( hdouble , 1 . 0 , false ) ;
final fieldmatrix < bigfraction > h = this . createh ( d , n ) ;
public kolmogorovsmirnovtest ( randomgenerator rng ) {
return new array2drowfieldmatrix < bigfraction > ( bigfractionfield . getinstance ( ) , hdata ) ;
private double roundedk ( double d , int n )
public double kolmogorovsmirnovstatistic ( realdistribution distribution , double [ ] data ) {
return 1d - cdf ( kolmogorovsmirnovstatistic ( distribution , data ) , data . length , exact ) ;
import java . util . iterator ;
return exactp ( kolmogorovsmirnovstatistic ( x , y ) , x . length , y . length , strict ) ;
arrays . sort ( datacopy ) ;
checkarray ( x ) ;
} catch ( final fractionconversionexception e2 ) {
return cdf ( d , n , false ) ;
import org . apache . commons . math3 . util . matharrays ;
return approximatep ( kolmogorovsmirnovstatistic ( x , y ) , x . length , y . length ) ;
final int [ ] nplusmset = matharrays . natural ( m + n ) ;
delta = fastmath . exp ( x * i * i ) ;
return 1 ;
final double f = 2 * d - ninv ;
final double curd = fastmath . abs ( cdf x - cdf y ) ;
int k = 0 ;
for ( int i = 1 ; i < = n ; + + i ) {
if ( j < n & & nseti [ j ] = = i ) {
pfrac * = ( double ) i / ( double ) n ;
tail + + ;
import org . apache . commons . math3 . fraction . bigfraction ;
final double [ ] nset = new double [ n ] ;
final double curd = kolmogorovsmirnovstatistic ( nset , mset ) ;
final realmatrix hpower = h . power ( n ) ;
double pfrac = hpower . getentry ( k - 1 , k - 1 ) ;
final int m = sy . length ;
throws numberistoolargeexception , fractionconversionexception {
final fieldmatrix < bigfraction > hbigfraction = this . createh ( d , n ) ;
} else if ( curd = = d & & !strict ) {
import org . apache . commons . math3 . exception . numberistoolargeexception ;
package org . apache . commons . math3 . stat . inference ;
return pfrac . bigdecimalvalue ( 20 , bigdecimal . round half up ) . doublevalue ( ) ;
return kolmogorovsmirnovtest ( x , y , true ) ;
final double [ ] sy = matharrays . copyof ( y ) ;
return ( double ) tail / ( double ) combinatoricsutils . binomialcoefficient ( n + m , n ) ;
import org . apache . commons . math3 . exception . nullargumentexception ;
import org . apache . commons . math3 . linear . fieldmatrix ;
} else if ( 1 - ninv < = d & & d < 1 ) {
return ( double ) tail / iterations ;
} else if ( 1 < = d ) {
long i = 1 ;
public double montecarlop ( double d , int n , int m , boolean strict , int iterations ) {
import org . apache . commons . math3 . random . well19937c ;
final double ninvhalf = 0 . 5 * ninv ;
for ( int j = 0 ; j < m ; + + j ) {
final int [ ] nseti = combinationsiterator . next ( ) ;
arrays . sort ( nplusmset , 0 , n ) ;
import org . apache . commons . math3 . linear . array2drowfieldmatrix ;
if ( array = = null ) {
throw new toomanyiterationsexception ( maxiterations ) ;
hdata [ i ] [ j ] = hdata [ i ] [ j ] . divide ( g ) ;
return partialsum * 2 ;
supd = curd ;
private void checkarray ( double [ ] array ) {
return 1 - kssum ( d * fastmath . sqrt ( ( dm * dn ) / ( dm + dn ) ) , ks sum cauchy criterion , maximum partial sum count ) ;
final double [ ] datacopy = new double [ n ] ;
if ( i - j + 1 < 0 ) {
public double kolmogorovsmirnovstatistic ( double [ ] x , double [ ] y ) {
for ( int i = 0 ; i < m ; + + i ) {
hdata [ i ] [ 0 ] = hdata [ i ] [ 0 ] . subtract ( hpowers [ i ] ) ;
import org . apache . commons . math3 . util . fastmath ;
throw new insufficientdataexception ( localizedformats . insufficient observed points in sample , array . length ,
private double exactk ( double d , int n )
import java . util . arrays ;
public double approximatep ( double d , int n , int m ) {
public boolean kolmogorovsmirnovtest ( realdistribution distribution , double [ ] data , double alpha ) {
while ( combinationsiterator . hasnext ( ) ) {
final bigfraction [ ] [ ] hdata = new bigfraction [ m ] [ m ] ;
public double kolmogorovsmirnovtest ( double [ ] x , double [ ] y , boolean strict ) {
} catch ( final fractionconversionexception e1 ) {
if ( array . length < 2 ) {
rng = new well19937c ( ) ;
hpowers [ 0 ] = h ;
final int k = ( int ) math . ceil ( n * d ) ;
double delta = 1 ;
import org . apache . commons . math3 . random . randomgenerator ;
import org . apache . commons . math3 . linear . realmatrix ;
for ( int i = 0 ; i < iterations ; i + + ) {
nset [ j + + ] = i ;
final double [ ] sx = matharrays . copyof ( x ) ;
h . setentry ( i , j , hbigfraction . getentry ( i , j ) . doublevalue ( ) ) ;
hdata [ i ] [ j ] = bigfraction . one ;
final int m = 2 * k - 1 ;
double d = 0d ;
2 ) ;
public double cdf ( double d , int n , boolean exact )
final double dn = n ;
final fieldmatrix < bigfraction > hpower = h . power ( n ) ;
double partialsum = 0 . 5d ;
bigfraction h = null ;
import org . apache . commons . math3 . optim . univariate . searchinterval ;
absolutetolerance ,
import org . apache . commons . math3 . optim . univariate . univariateobjectivefunction ;
import org . apache . commons . math3 . optim . nonlinear . scalar . linesearch ;
import org . apache . commons . math3 . optim . univariate . brentoptimizer ;
import org . apache . commons . math3 . optim . univariate . bracketfinder ;
double absolutetolerance ) {
1e - 8 ,
convergencechecker < pointvaluepair > checker ,
absolutetolerance ) ) ;
double absolutetolerance ,
final double [ ] direction ) {
checker ,
x [ i ] = startpoint [ i ] + alpha * direction [ i ] ;
* /
private static final double rel tol unused = 1e - 15 ;
new simpleunivariatevaluechecker ( relativetolerance ,
import org . apache . commons . math3 . optim . univariate . univariateoptimizer ;
}
private final bracketfinder bracket = new bracketfinder ( ) ;
@ deprecated
abs tol unused ,
public univariatepointvaluepair search ( final double [ ] startpoint ,
package org . apache . commons . math3 . optim . nonlinear . scalar ;
linesearchsolver . getabsoluteaccuracy ( ) ,
public linesearch ( multivariateoptimizer optimizer ,
absolutetolerance ) ;
public class linesearch {
import org . apache . commons . math3 . optim . univariate . univariatepointvaluepair ;
new identitypreconditioner ( ) ) ;
double relativetolerance ,
public nonlinearconjugategradientoptimizer ( final formula updateformula ,
import org . apache . commons . math3 . optim . maxeval ;
return obj ;
goal ,
preconditioner ) ;
mainoptimizer = optimizer ;
final preconditioner preconditioner ) {
lineoptimizer = new brentoptimizer ( rel tol unused ,
public double value ( double alpha ) {
for ( int i = 0 ; i < n ; i + + ) {
return lineoptimizer . optimize ( new maxeval ( integer . max value ) ,
import org . apache . commons . math3 . optim . univariate . simpleunivariatevaluechecker ;
final double obj = mainoptimizer . computeobjectivevalue ( x ) ;
bracket . search ( f , goal , 0 , 1 ) ;
this ( updateformula ,
final goaltype goal = mainoptimizer . getgoaltype ( ) ;
final univariatefunction f = new univariatefunction ( ) {
bracket . gethi ( ) ,
relativetolerance ,
linesearchsolver . getrelativeaccuracy ( ) ,
final int n = startpoint . length ;
convergencechecker < pointvaluepair > checker ) {
new univariateobjectivefunction ( f ) ,
private final multivariateoptimizer mainoptimizer ;
} ;
line = new linesearch ( this ,
private static final double abs tol unused = double . min value ;
final double step = line . search ( point , searchdirection ) . getpoint ( ) ;
final double [ ] x = new double [ n ] ;
bracket . getmid ( ) ) ) ;
import org . apache . commons . math3 . analysis . univariatefunction ;
private final univariateoptimizer lineoptimizer ;
private final linesearch line ;
new searchinterval ( bracket . getlo ( ) ,
return new double [ ] { a , previousa } ;
public static double [ ] bracket ( final univariatefunction function , final double initial ,
final double q , final double r , final int maximumiterations )
a = fastmath . max ( initial - delta , lowerbound ) ;
fa = function . value ( a ) ;
}
if ( numiterations = = 0 ) {
final double previousa = a ;
if ( q < = 0 ) {
double fa = double . nan ;
throws nobracketingexception {
if ( fa * fb < = 0 ) {
} else {
( numiterations < maximumiterations ) & & ( a > lowerbound | | b > upperbound ) ;
double a = initial ;
final double previousfa = fa ;
delta = r * delta + q ;
final double previousfb = fb ;
return bracket ( function , initial , lowerbound , upperbound , 1 . 0 , 1 . 0 , maximumiterations ) ;
+ + numiterations ) {
throw new notstrictlypositiveexception ( q ) ;
final double lowerbound , final double upperbound ,
double delta = 0 ;
return new double [ ] { a , b } ;
} else if ( fb * previousfb < = 0 ) {
return bracket ( function , initial , lowerbound , upperbound , 1 . 0 , 1 . 0 , integer . max value ) ;
final double previousb = b ;
if ( fa * previousfa < = 0 ) {
double fb = double . nan ;
throw new nobracketingexception ( a , b , fa , fb ) ;
double b = initial ;
b = fastmath . min ( initial + delta , upperbound ) ;
fb = function . value ( b ) ;
return new double [ ] { previousb , b } ;
for ( int numiterations = 0 ;
}
super ( randomdata . getrandomgenerator ( ) ) ;
return sample ( ) ;
return new normaldistribution ( randomdata . getrandomgenerator ( ) ,
} else {
if ( bstats . getn ( ) = = 1 ) {
return new constantrealdistribution ( bstats . getmean ( ) ) ;
import org . apache . commons . math3 . distribution . constantrealdistribution ;
return result ;
public bivariatefunction partialderivativex ( ) {
double result = 0 ;
partialderivatives [ 4 ] [ i ] [ j ] = bcs . partialderivativexy ( ) ;
partialderivatives [ 0 ] [ i ] [ j ] = bcs . partialderivativex ( ) ;
private final bivariatefunction partialderivativex ;
private double apply ( double [ ] px , double [ ] py , double [ ] [ ] coeff ) {
if ( x < 0 | | x > 1 ) {
for ( int j = 0 ; j < lastj ; j + + ) {
final double [ ] px = { 1 , x , x2 , x3 } ;
return apply ( px , py , a ) ;
}
final double y2 = y * y ;
for ( int j = 0 ; j < n ; j + + ) {
private final bivariatefunction [ ] [ ] [ ] partialderivatives ;
private final bivariatefunction partialderivativeyy ;
return partialderivativexy ;
return partialderivativeyy ;
public bivariatefunction partialderivativexy ( ) {
final double [ ] py = { 1 , y , y2 , y3 } ;
for ( int i = 0 ; i < lasti ; i + + ) {
partialderivatives = new bivariatefunction [ 5 ] [ lasti ] [ lastj ] ;
public bivariatefunction partialderivativey ( ) {
private final bivariatefunction partialderivativey ;
a [ i ] [ j ] = coeff [ i * n + j ] ;
if ( y < 0 | | y > 1 ) {
final bicubicsplinefunction bcs = splines [ i ] [ j ] ;
result + = coeff [ i ] [ j ] * px [ i ] * py [ j ] ;
for ( int i = 0 ; i < n ; i + + ) {
public double value ( double x , double y ) {
partialderivatives [ 1 ] [ i ] [ j ] = bcs . partialderivativey ( ) ;
a = new double [ n ] [ n ] ;
final double y3 = y2 * y ;
public bivariatefunction partialderivativeyy ( ) {
return partialderivativey ;
private final bivariatefunction partialderivativexy ;
return partialderivativexx ;
partialderivatives [ 2 ] [ i ] [ j ] = bcs . partialderivativexx ( ) ;
final double x2 = x * x ;
class bicubicsplinefunction implements bivariatefunction {
partialderivatives [ 3 ] [ i ] [ j ] = bcs . partialderivativeyy ( ) ;
final double x3 = x2 * x ;
return partialderivativex ;
throw new outofrangeexception ( y , 0 , 1 ) ;
public bivariatefunction partialderivativexx ( ) {
public bicubicsplinefunction ( double [ ] coeff ) {
private final bivariatefunction partialderivativexx ;
throw new outofrangeexception ( x , 0 , 1 ) ;
public bicubicsplineinterpolator ( boolean initializederivatives ) {
return apply ( px , py , axy ) ;
public bicubicsplinefunction ( double [ ] coeff ,
partialderivativex = new bivariatefunction ( ) {
boolean initializederivatives ) {
this ( coeff , false ) ;
if ( initializederivatives ) {
partialderivativeyy = null ;
partialderivatives [ 4 ] [ i ] [ j ] = bcs . partialderivativexy ( ) ;
partialderivatives [ 0 ] [ i ] [ j ] = bcs . partialderivativex ( ) ;
final bicubicsplineinterpolator bsi = new bicubicsplineinterpolator ( true ) ;
final double [ ] py = { 0 , 1 , y , y2 } ;
return apply ( px , py , ayy ) ;
axx [ i ] [ j ] = ( i - 1 ) * ax [ i ] [ j ] ;
partialderivativexx = new bivariatefunction ( ) {
axy [ i ] [ j ] = j * ax [ i ] [ j ] ;
partialderivativey = new bivariatefunction ( ) {
final double [ ] px = { 1 , x , x2 , x3 } ;
}
for ( int j = 0 ; j < lastj ; j + + ) {
double [ ] [ ] f ,
final double y2 = y * y ;
for ( int j = 0 ; j < n ; j + + ) {
double [ ] [ ] d2fdxdy ,
partialderivativexx = null ;
nonmonotonicsequenceexception {
final double [ ] px = { 0 , 1 , x , x2 } ;
ayy [ i ] [ j ] = ( j - 1 ) * ay [ i ] [ j ] ;
this . initializederivatives = initializederivatives ;
return apply ( px , py , ay ) ;
final double [ ] py = { 1 , y , y2 , y3 } ;
ax [ i ] [ j ] = i * c ;
for ( int i = 0 ; i < lasti ; i + + ) {
partialderivatives = new bivariatefunction [ 5 ] [ lasti ] [ lastj ] ;
dfdx , dfdy , d2fdxdy ,
} else {
partialderivativexy = new bivariatefunction ( ) {
private final boolean initializederivatives ;
final double [ ] [ ] ax = new double [ n ] [ n ] ;
public bicubicsplineinterpolator ( ) {
final bicubicsplinefunction bcs = splines [ i ] [ j ] ;
for ( int i = 0 ; i < n ; i + + ) {
public double value ( double x , double y ) {
partialderivatives [ 1 ] [ i ] [ j ] = bcs . partialderivativey ( ) ;
this ( false ) ;
final double y3 = y2 * y ;
final double [ ] [ ] axx = new double [ n ] [ n ] ;
initializederivatives ) ;
final double [ ] px = { 0 , 0 , 1 , x } ;
splines [ i ] [ j ] = new bicubicsplinefunction ( computesplinecoefficients ( beta ) ,
this ( x , y , f , dfdx , dfdy , d2fdxdy , false ) ;
throws dimensionmismatchexception ,
partialderivatives [ 2 ] [ i ] [ j ] = bcs . partialderivativexx ( ) ;
double [ ] [ ] dfdy ,
final double x2 = x * x ;
partialderivatives [ 3 ] [ i ] [ j ] = bcs . partialderivativeyy ( ) ;
final double x3 = x2 * x ;
nodataexception ,
partialderivativexy = null ;
boolean initializederivatives )
final double [ ] [ ] ay = new double [ n ] [ n ] ;
partialderivativeyy = new bivariatefunction ( ) {
partialderivatives = null ;
} ;
return apply ( px , py , ax ) ;
ay [ i ] [ j ] = j * c ;
return apply ( px , py , axx ) ;
partialderivativex = null ;
partialderivativey = null ;
final double c = a [ i ] [ j ] ;
double [ ] y ,
final double [ ] py = { 0 , 0 , 1 , y } ;
final double [ ] [ ] ayy = new double [ n ] [ n ] ;
public bicubicsplineinterpolatingfunction ( double [ ] x ,
final double [ ] [ ] axy = new double [ n ] [ n ] ;
double [ ] [ ] dfdx ,
if ( data ! = null & & data . length > 1 ) {
return beta ;
private static final long serialversionuid = 20141003 ;
return fastmath . exp ( ( x - mu ) / beta ) / 2 . 0 ;
public double getsupportupperbound ( ) {
throw new notstrictlypositiveexception ( localizedformats . not positive scale , beta ) ;
public double getlocation ( ) {
import org . apache . commons . math3 . exception . outofrangeexception ;
throw new outofrangeexception ( p , 0 . 0 , 1 . 0 ) ;
return false ;
public laplacedistribution ( double mu , double beta ) {
import org . apache . commons . math3 . random . well19937c ;
public double getnumericalmean ( ) {
}
return mu + beta * x ;
public boolean issupportupperboundinclusive ( ) {
return double . positive infinity ;
super ( rng ) ;
double x = ( p > 0 . 5 ) ? - math . log ( 2 . 0 - 2 . 0 * p ) : math . log ( 2 . 0 * p ) ;
private final double beta ;
} else if ( p = = 1 ) {
return fastmath . exp ( - fastmath . abs ( x - mu ) / beta ) / ( 2 . 0 * beta ) ;
public class laplacedistribution extends abstractrealdistribution {
import org . apache . commons . math3 . exception . util . localizedformats ;
return 0 . 0 ;
return double . negative infinity ;
return true ;
} else {
} else if ( p = = 0 ) {
public double getnumericalvariance ( ) {
package org . apache . commons . math3 . distribution ;
import org . apache . commons . math3 . exception . notstrictlypositiveexception ;
import org . apache . commons . math3 . util . fastmath ;
private final double mu ;
return 1 . 0 - fastmath . exp ( ( mu - x ) / beta ) / 2 . 0 ;
this . mu = mu ;
public double cumulativeprobability ( double x ) {
this . beta = beta ;
public boolean issupportconnected ( ) {
if ( x < = mu ) {
@ override
if ( p < 0 . 0 | | p > 1 . 0 ) {
public double getscale ( ) {
return 2 . 0 * beta * beta ;
public boolean issupportlowerboundinclusive ( ) {
public double density ( double x ) {
if ( beta < = 0 . 0 ) {
import org . apache . commons . math3 . random . randomgenerator ;
return mu ;
this ( new well19937c ( ) , mu , beta ) ;
public laplacedistribution ( randomgenerator rng , double mu , double beta ) {
public double getsupportlowerbound ( ) {
public double inversecumulativeprobability ( double p ) throws outofrangeexception {
double zarray [ ] = new double [ count ] ;
spline = interpolator . interpolate ( xarray , zarray ) ;
zarray [ index ] = fval [ i + index ] [ j + zindex ] ;
fval = f . clone ( ) ;
if ( ylen ! = f [ 0 ] . length ) {
import org . apache . commons . math3 . exception . nullargumentexception ;
double [ ] [ ] f )
r = - r - offset - 1 ;
final bicubicsplineinterpolator bsi = new bicubicsplineinterpolator ( ) ;
throw new nodataexception ( ) ;
polynomialsplinefunction spline ;
implements bivariategridinterpolator
int r = arrays . binarysearch ( val , c ) ;
throws dimensionmismatchexception , nullargumentexception , nodataexception , nonmonotonicsequenceexception
public bicubicsplineinterpolatingfunction ( double [ ] x , double [ ] y ,
}
if ( x = = null | | y = = null | | f = = null | | f [ 0 ] = = null ) {
{
private int searchindex ( double c , double [ ] val , int offset , int count ) {
if ( xval = = null | | yval = = null | | fval = = null | | fval [ 0 ] = = null )
if ( x < xval [ 0 ] | | x > xval [ xval . length - 1 ] | | y < yval [ 0 ] | |
throws dimensionmismatchexception , nullargumentexception ,
for ( int zindex = 0 ; zindex < count ; zindex + + ) {
xarray [ index ] = xval [ i + index ] ;
interparray [ zindex ] = spline . value ( x ) ;
return new bicubicsplineinterpolatingfunction ( xval , yval , fval ) ;
if ( r < 0 ) {
final int i = searchindex ( x , xval , offset , count ) ;
nodataexception , nonmonotonicsequenceexception {
final int minimumlength = akimasplineinterpolator . minimum number points ;
for ( index = 0 ; index < count ; index + + ) {
final int j = searchindex ( y , yval , offset , count ) ;
public bicubicsplineinterpolator ( )
final int count = offset + 3 ;
throw new dimensionmismatchexception ( ylen , f [ 0 ] . length ) ;
spline = interpolator . interpolate ( yarray , interparray ) ;
double returnvalue = spline . value ( y ) ;
if ( xlen < minimumlength | | ylen < minimumlength | |
public bicubicsplineinterpolatingfunction interpolate ( final double [ ] xval , final double [ ] yval ,
throw new nullargumentexception ( ) ;
if ( ( r + count ) > = val . length ) {
if ( r = = - 1 | | r = = - val . length - 1 ) {
import org . apache . commons . math3 . exception . insufficientdataexception ;
r - = offset ;
r = val . length - count ;
if ( xval . length = = 0 | | yval . length = = 0 | | fval . length = = 0 )
double xarray [ ] = new double [ count ] ;
int index ;
f . length < minimumlength | | f [ 0 ] . length < minimumlength ) {
double yarray [ ] = new double [ count ] ;
private final double [ ] [ ] fval ;
return r ;
throw new insufficientdataexception ( ) ;
akimasplineinterpolator interpolator = new akimasplineinterpolator ( ) ;
final int offset = 2 ;
double interparray [ ] = new double [ count ] ;
yarray [ index ] = yval [ j + index ] ;
import org . apache . commons . math3 . analysis . polynomials . polynomialsplinefunction ;
r = 0 ;
return returnvalue ;
final insidefinder < s > finder = new insidefinder < s > ( this ) ;
public subhyperplane < s > outsidetouching ( ) {
}
private final region < s > region ;
if ( ! ( plusfound & & minusfound ) ) {
switch ( sub . side ( hyperplane ) ) {
private boolean plusfound ;
} else {
final boolean inside = ( boolean ) node . getattribute ( ) ;
minusfound = false ;
public boolean touchinside ( ) {
case both :
private subhyperplane < s > outsidetouching ;
recursesides ( node . getminus ( ) , split . getminus ( ) ) ;
recursesides ( node . getplus ( ) , sub ) ;
if ( ( node . getminus ( ) . getcut ( ) ! = null ) | | ( ( boolean ) node . getminus ( ) . getattribute ( ) ) ) {
throw new mathinternalerror ( ) ;
plusfound = true ;
return insidetouching ! = null & & !insidetouching . isempty ( ) ;
public insidefinder ( final region < s > region ) {
private subhyperplane < s > insidetouching ;
package org . apache . commons . math3 . geometry . partitioning ;
if ( ( node . getplus ( ) . getcut ( ) ! = null ) | | ( ( boolean ) node . getplus ( ) . getattribute ( ) ) ) {
return outsidetouching ;
public boolean plusfound ( ) {
if ( ( boolean ) node . getattribute ( ) ) {
final hyperplane < s > hyperplane = node . getcut ( ) . gethyperplane ( ) ;
final subhyperplane . splitsubhyperplane < s > split = sub . split ( hyperplane ) ;
if ( node . getcut ( ) = = null ) {
return plusfound ;
finder . recursesides ( tree , hyperplane . wholehyperplane ( ) ) ;
private boolean minusfound ;
plusfound = false ;
class insidefinder < s extends space > {
insidetouching = sub ;
characterize ( node . getplus ( ) , sub ) ;
return insidetouching ;
break ;
import org . apache . commons . math3 . geometry . space ;
characterize ( node . getminus ( ) , sub ) ;
outsidetouching = null ;
public void recursesides ( final bsptree < s > node , final subhyperplane < s > sub ) {
public characterization ( final bsptree < s > node , final subhyperplane < s > sub ) {
public subhyperplane < s > insidetouching ( ) {
characterize ( node . getminus ( ) , split . getminus ( ) ) ;
outsidetouching = outsidetouching . reunite ( sub ) ;
return minusfound ;
( finder . minusfound ( ) ? side . both : side . plus ) :
if ( inside ) {
public boolean minusfound ( ) {
if ( !region . isempty ( node . getplus ( ) ) ) {
recursesides ( node . getplus ( ) , split . getplus ( ) ) ;
recursesides ( node . getminus ( ) , sub ) ;
private void addinsidetouching ( final subhyperplane < s > sub ) {
characterize ( node . getplus ( ) , split . getplus ( ) ) ;
minusfound = true ;
if ( !region . isempty ( node . getminus ( ) ) ) {
( finder . minusfound ( ) ? side . minus : side . hyper ) ;
class characterization < s extends space > {
private void characterize ( final bsptree < s > node , final subhyperplane < s > sub ) {
this . region = region ;
case plus :
if ( node . getcut ( ) . gethyperplane ( ) . sameorientationas ( sub . gethyperplane ( ) ) ) {
insidetouching = insidetouching . reunite ( sub ) ;
return outsidetouching ! = null & & !outsidetouching . isempty ( ) ;
default :
case minus :
return finder . plusfound ( ) ?
public boolean touchoutside ( ) {
addinsidetouching ( sub ) ;
characterize ( node , sub ) ;
private void addoutsidetouching ( final subhyperplane < s > sub ) {
insidetouching = null ;
return ;
if ( outsidetouching = = null ) {
if ( node . getcut ( ) . side ( sub . gethyperplane ( ) ) = = side . plus ) {
if ( insidetouching = = null ) {
outsidetouching = sub ;
addoutsidetouching ( sub ) ;
import org . apache . commons . math3 . exception . mathinternalerror ;
}
if ( kernel instanceof constantrealdistribution ) {
final realdistribution kernel = k ( x ) ;
return pbminus + pb ;
}
if ( kernel instanceof constantrealdistribution ) {
final realdistribution kernel = k ( x ) ;
return pbminus + pb ;
@ override
}
final double d1low = d - d1high ;
if ( double . isnan ( result ) ) {
return result ;
final double result = resulthigh + resultlow ;
d2phigh = double . longbitstodouble ( double . doubletorawlongbits ( tmphigh ) & ( ( - 1l ) < < 27 ) ) ;
final double d1high = double . longbitstodouble ( double . doubletorawlongbits ( d ) & ( ( - 1l ) < < 27 ) ) ;
final double rhh = double . longbitstodouble ( double . doubletorawlongbits ( resulthigh ) & ( ( - 1l ) < < 27 ) ) ;
return double . nan ;
} else {
final double cd2ph = double . longbitstodouble ( double . doubletorawlongbits ( d2phigh ) & ( ( - 1l ) < < 27 ) ) ;
if ( double . isnan ( d ) ) {
return ( d < 0 & & ( e & 0x1 ) = = 1 ) ? double . negative infinity : double . positive infinity ;
} else if ( x = = 0 ) {
} else if ( x < 0 ) {
} else if ( x = = double . positive infinity ) {
return y ;
} else if ( x = = double . negative infinity ) {
} else if ( y = = double . positive infinity ) {
} else if ( y = = double . negative infinity ) {
} else if ( y ! = y ) {
} else if ( x ! = x ) {
final split mulbasic = new split ( full * b . full ) ;
} else if ( xrawexp = = 2047 ) {
private split pow ( final long e ) {
low = x - high ;
}
final double mulerror = low * b . low - ( ( ( mulbasic . full - high * b . high ) - low * b . high ) - high * b . low ) ;
split result = new split ( 1 ) ;
if ( x = = 0 ) {
z = z * lnb + 1 . 0 ;
return new split ( d ) . reciprocal ( ) . pow ( - e ) . full ;
final double result = exp ( lna , z , null ) ;
for ( long p = e ; p ! = 0 ; p > > = 1 ) {
} else {
return lores ;
final double yb = y - ya ;
this . low = low ;
return pow ( d , ( long ) e ) ;
} else if ( full < 0 & & ( e & 0x1 ) = = 1 ) {
final long xbits = double . doubletorawlongbits ( x ) ;
final long integralmask = ( - 1l ) < < ( 1075 - yrawexp ) ;
( xrawexp = = 2047 & & xrawmantissa ! = 0 ) ) {
return double . isnan ( error ) ? splitinv : new split ( splitinv . high , splitinv . low - error / full ) ;
if ( ( p & 0x1 ) ! = 0 ) {
final long yrawmantissa = ybits & mask double mantissa ;
lnb = - ( lna - aa - ab ) ;
private final double full ;
public static final split negative infinity = new split ( double . negative infinity , 0 ) ;
final double lns [ ] = new double [ 2 ] ;
private final double low ;
final long l = yfullmantissa < < ( yrawexp - 1075 ) ;
if ( xrawmantissa = = 0 ) {
} else if ( xrawexp = = 1023 & & xrawmantissa = = 0 ) {
final long xrawmantissa = xbits & mask double mantissa ;
private final double high ;
private static final long mask double mantissa = 0x000fffffffffffffl ;
if ( yrawexp = = 2047 ) {
if ( double . isinfinite ( lores ) ) {
if ( ( yfullmantissa & integralmask ) = = yfullmantissa ) {
final double aa = lna * ya ;
if ( fastmath . abs ( full ) < 1 ) {
if ( yrawexp > 1085 ) {
final split product = multiply ( splitinv ) ;
final double error = ( product . high - 1 ) + product . low ;
final int xrawexp = ( int ) ( ( xbits & mask double exponent ) > > 52 ) ;
return split . nan ;
z = z * lnb + ( 1 . 0 / 6 . 0 ) ;
public static final split nan = new split ( double . nan , 0 ) ;
if ( double . isnan ( full ) ) {
z = z * lnb + ( 1 . 0 / 24 . 0 ) ;
d2p = d2p . multiply ( d2p ) ;
final double ya = ( y + tmp ) - tmp ;
private static class split {
public split multiply ( final split b ) {
return ( y < 0 ) ? + 0 . 0 : double . positive infinity ;
lna = aa + ab ;
if ( yrawexp > = 1023 ) {
public static double pow ( double d , long e ) {
result = result . multiply ( d2p ) ;
return y < 0 ? double . positive infinity : + 0 . 0 ;
} else if ( x < 0 ) {
double z = 1 . 0 / 120 . 0 ;
return double . positive infinity ;
final double tmp2 = ( lna + tmp1 ) - tmp1 ;
if ( y = = 0 ) {
if ( yrawexp < 1075 ) {
} else if ( e > 0 ) {
public split ( final double high , final double low ) {
if ( double . isnan ( result . full ) ) {
high = double . longbitstodouble ( double . doubletorawlongbits ( x ) & ( ( - 1l ) < < 27 ) ) ;
final double ab = lna * yb + lnb * ya + lnb * yb ;
public split reciprocal ( ) {
lnb + = lna - tmp2 ;
this . full = full ;
private static final long mask double exponent = 0x7ff0000000000000l ;
public static final split positive infinity = new split ( double . positive infinity , 0 ) ;
lna = tmp2 ;
if ( ( y > 0 ) ^ ( xrawexp < 1023 ) ) {
private static final long implicit high bit = 0x0010000000000000l ;
z = z * lnb + 0 . 5 ;
public split ( final double x ) {
return + 0 . 0 ;
final double approximateinv = 1 . 0 / full ;
split d2p = new split ( full , high , low ) ;
return result ;
final double lores = log ( x , lns ) ;
final long ybits = double . doubletorawlongbits ( y ) ;
public split ( final double full , final double high , final double low ) {
final double tmp = y * hex 40000000 ;
final double tmp1 = lna * hex 40000000 ;
return new split ( fastmath . copysign ( 0 . 0 , full ) , 0 . 0 ) ;
this . high = high ;
return new split ( d ) . pow ( e ) . full ;
return new split ( mulbasic . high , mulbasic . low + mulerror ) ;
final long yfullmantissa = implicit high bit | yrawmantissa ;
return split . positive infinity ;
return split . negative infinity ;
full = x ;
this ( high + low , high , low ) ;
if ( ( yrawexp = = 2047 & & yrawmantissa ! = 0 ) | |
final long l = yfullmantissa > > ( 1075 - yrawexp ) ;
return fastmath . pow ( x , ( y < 0 ) ? - l : l ) ;
z * = lnb ;
double lna = lns [ 0 ] ;
final split splitinv = new split ( approximateinv ) ;
final int yrawexp = ( int ) ( ( ybits & mask double exponent ) > > 52 ) ;
return 1 . 0 ;
return double . nan ;
double lnb = lns [ 1 ] ;
final int n = x . length ;
return ( double ) tail / ( double ) combinatoricsutils . binomialcoefficient ( n + m , n ) ;
return false ;
final int order = precision . compareto ( curd , d , tol ) ;
system . arraycopy ( y , 0 , universe , n , m ) ;
final double [ ] mset = new double [ m ] ;
}
final int [ ] nseti = combinationsiterator . next ( ) ;
import java . util . hashset ;
for ( int i = 0 ; i < x . length ; i + + ) {
final int m = y . length ;
} else {
return true ;
nset [ j + + ] = universe [ i ] ;
final double [ ] universe = new double [ n + m ] ;
final double tol = 1e - 12 ;
system . arraycopy ( x , 0 , universe , 0 , n ) ;
public double exactp ( double [ ] x , double [ ] y , boolean strict ) {
if ( !values . add ( x [ i ] ) ) {
private boolean hasties ( double [ ] x , double [ ] y ) {
int j = 0 ;
int k = 0 ;
while ( combinationsiterator . hasnext ( ) ) {
if ( j < n & & nseti [ j ] = = i ) {
long tail = 0 ;
final double d = kolmogorovsmirnovstatistic ( x , y ) ;
tail + + ;
final double [ ] nset = new double [ n ] ;
if ( hasties ( x , y ) ) {
iterator < int [ ] > combinationsiterator = combinatoricsutils . combinationsiterator ( n + m , n ) ;
final double curd = kolmogorovsmirnovstatistic ( nset , mset ) ;
return exactp ( x , y , strict ) ;
mset [ k + + ] = universe [ i ] ;
if ( !values . add ( y [ i ] ) ) {
for ( int i = 0 ; i < y . length ; i + + ) {
hashset < double > values = new hashset < double > ( ) ;
for ( int i = 0 ; i < n + m ; i + + ) {
if ( order > 0 | | ( order = = 0 & & !strict ) ) {
long nm = n * ( long ) m ;
if ( strict & & lowerbound = = upperbound ) {
for ( int j = 0 ; j < b . length ; + + j ) {
final long curd = integralkolmogorovsmirnovstatistic ( nset , mset ) ;
if ( curd > = d ) {
return integralkolmogorovsmirnovstatistic ( x , y ) / ( ( double ) ( x . length * ( long ) y . length ) ) ;
long supd = 0l ;
if ( curd < = - d ) {
return integralexactp ( integralkolmogorovsmirnovstatistic ( x , y ) + ( ( strict ) ? 1l : 0l ) , x . length , y . length ) ;
else if ( - curd > supd ) {
return upperbound + 1l ;
supd = - curd ;
curd + = mm ;
}
if ( b [ j ] ) {
final long d = integralkolmogorovsmirnovstatistic ( x , y ) ;
long curd = 0l ;
curd - = nn ;
curd - = n ;
long lowerbound = ( long ) fastmath . floor ( ( d + tol ) * nm ) ;
private static long calculateintegrald ( double d , int n , int m , boolean strict ) {
final double tol = 1e - 12 ;
long upperbound = ( long ) fastmath . ceil ( ( d - tol ) * nm ) ;
curd + = m ;
return integralexactp ( calculateintegrald ( d , n , m , strict ) , n , m ) ;
else {
private double integralexactp ( long d , int n , int m ) {
tail + + ;
if ( curd > d | | ( curd = = d & & !strict ) ) {
return integralmontecarlop ( integralkolmogorovsmirnovstatistic ( x , y ) + ( ( strict ) ? 1l : 0l ) , x . length , y . length , monte carlo iterations ) ;
private double integralmontecarlop ( final long d , final int n , final int m , final int iterations ) {
private long integralkolmogorovsmirnovstatistic ( double [ ] x , double [ ] y ) {
break ;
return integralmontecarlop ( calculateintegrald ( d , n , m , strict ) , n , m , iterations ) ;
return upperbound ;
final int n = x . length ;
long nm = n * ( long ) m ;
if ( strict & & lowerbound = = upperbound ) {
for ( int j = 0 ; j < b . length ; + + j ) {
return integralkolmogorovsmirnovstatistic ( x , y ) / ( ( double ) ( x . length * ( long ) y . length ) ) ;
return ( double ) tail / ( double ) combinatoricsutils . binomialcoefficient ( n + m , n ) ;
final long curd = integralkolmogorovsmirnovstatistic ( nset , mset ) ;
if ( curd > = d ) {
if ( curd < = - d ) {
return integralexactp ( integralkolmogorovsmirnovstatistic ( x , y ) + ( ( strict ) ? 1l : 0l ) , x . length , y . length ) ;
else if ( - curd > supd ) {
return upperbound + 1l ;
long supd = 0l ;
supd = - curd ;
curd + = mm ;
system . arraycopy ( y , 0 , universe , n , m ) ;
final double [ ] mset = new double [ m ] ;
}
final int [ ] nseti = combinationsiterator . next ( ) ;
if ( b [ j ] ) {
final long d = integralkolmogorovsmirnovstatistic ( x , y ) ;
long curd = 0l ;
curd - = nn ;
final int m = y . length ;
curd - = n ;
} else {
long lowerbound = ( long ) fastmath . floor ( ( d + tol ) * nm ) ;
private static long calculateintegrald ( double d , int n , int m , boolean strict ) {
nset [ j + + ] = universe [ i ] ;
final double [ ] universe = new double [ n + m ] ;
final double tol = 1e - 12 ;
long upperbound = ( long ) fastmath . ceil ( ( d - tol ) * nm ) ;
curd + = m ;
system . arraycopy ( x , 0 , universe , 0 , n ) ;
public double exactp ( double [ ] x , double [ ] y , boolean strict ) {
int j = 0 ;
return integralexactp ( calculateintegrald ( d , n , m , strict ) , n , m ) ;
int k = 0 ;
while ( combinationsiterator . hasnext ( ) ) {
else {
if ( j < n & & nseti [ j ] = = i ) {
long tail = 0 ;
private double integralexactp ( long d , int n , int m ) {
tail + + ;
final double [ ] nset = new double [ n ] ;
if ( curd > d | | ( curd = = d & & !strict ) ) {
iterator < int [ ] > combinationsiterator = combinatoricsutils . combinationsiterator ( n + m , n ) ;
return integralmontecarlop ( integralkolmogorovsmirnovstatistic ( x , y ) + ( ( strict ) ? 1l : 0l ) , x . length , y . length , monte carlo iterations ) ;
mset [ k + + ] = universe [ i ] ;
private double integralmontecarlop ( final long d , final int n , final int m , final int iterations ) {
private long integralkolmogorovsmirnovstatistic ( double [ ] x , double [ ] y ) {
break ;
return integralmontecarlop ( calculateintegrald ( d , n , m , strict ) , n , m , iterations ) ;
return upperbound ;
for ( int i = 0 ; i < n + m ; i + + ) {
return integralmontecarlop ( integralkolmogorovsmirnovstatistic ( x , y ) + ( strict ? 1l : 0l ) , x . length , y . length , monte carlo iterations ) ;
return integralexactp ( integralkolmogorovsmirnovstatistic ( x , y ) + ( strict ? 1l : 0l ) , x . length , y . length ) ;
return integralmontecarlop ( integralkolmogorovsmirnovstatistic ( x , y ) + ( strict ? 1l : 0l ) , x . length , y . length , monte carlo iterations ) ;
return integralexactp ( integralkolmogorovsmirnovstatistic ( x , y ) + ( strict ? 1l : 0l ) , x . length , y . length ) ;
translationtransform ( final vector3d translation ) {
facetscontributionvisitor ( ) {
rotationtransform ( final vector3d center , final rotation rotation ) {
if ( lengthproduct < large sample product & & hasties ( x , y ) ) {
final hashset < double > values = new hashset < double > ( ) ;
if ( ties ) {
ya = matharrays . copyof ( y ) ;
import org . apache . commons . math4 . random . jdkrandomgenerator ;
ties = hasties ( x , y ) ;
double mindelta = 1 ;
double prev = values [ 0 ] ;
private static void jitter ( double [ ] data , realdistribution dist ) {
} while ( ties & & ct < 1000 ) ;
}
ya = y ;
for ( int i = 1 ; i < values . length ; i + + ) {
if ( delta < mindelta ) {
double [ ] xa = null ;
if ( values . length = = x . length + y . length ) {
return integralexactp ( integralkolmogorovsmirnovstatistic ( xa , ya ) + ( strict ? 1l : 0l ) , x . length , y . length ) ;
jitter ( y , dist ) ;
ct + + ;
} else {
final realdistribution dist =
jitter ( x , dist ) ;
new uniformrealdistribution ( new jdkrandomgenerator ( 100 ) , - mindelta , mindelta ) ;
mindelta = delta ;
delta = prev - values [ i ] ;
import org . apache . commons . math4 . distribution . uniformrealdistribution ;
int ct = 0 ;
return ;
return integralmontecarlop ( integralkolmogorovsmirnovstatistic ( xa , ya ) + ( strict ? 1l : 0l ) , x . length , y . length , monte carlo iterations ) ;
xa = x ;
private static void fixties ( double [ ] x , double [ ] y ) {
for ( int i = 0 ; i < data . length ; i + + ) {
prev = values [ i ] ;
private static boolean hasties ( double [ ] x , double [ ] y ) {
fixties ( xa , ya ) ;
final double [ ] values = matharrays . unique ( matharrays . concatenate ( x , y ) ) ;
do {
throw new mathinternalerror ( ) ;
xa = matharrays . copyof ( x ) ;
double [ ] ya = null ;
double delta = 1 ;
data [ i ] = data [ i ] + dist . sample ( ) ;
import org . apache . commons . math4 . exception . mathinternalerror ;
mindelta / = 2 ;
boolean ties = true ;
system . out . println ( "d = " + d ) ;
rng = randomsource . create ( randomsource . well 19937 c ) ;
jitter ( y , sampler ) ;
cury = abstractrealdistribution . sample ( ylength , sampler ) ;
import org . apache . commons . math4 . rng . uniformrandomprovider ;
private static void jitter ( double [ ] data , realdistribution . sampler sampler ) {
new uniformrealdistribution ( - mindelta , mindelta ) . createsampler ( randomsource . create ( randomsource . jdk , 100 ) ) ;
private static void fillbooleanarrayrandomlywithfixednumbertruevalues ( final boolean [ ] b , final int numberoftruevalues , final uniformrandomprovider rng ) {
@ deprecated
private final uniformrandomprovider rng ;
curx = abstractrealdistribution . sample ( xlength , sampler ) ;
system . out . println ( ) ;
public kolmogorovsmirnovtest ( randomsource source ,
jitter ( x , sampler ) ;
final double d = sampler . sample ( ) ;
rng = randomsource . create ( source , seed ) ;
import org . apache . commons . math4 . rng . randomsource ;
import org . apache . commons . math4 . distribution . abstractrealdistribution ;
long seed ) {
final realdistribution . sampler sampler =
final realdistribution . sampler sampler = new enumeratedrealdistribution ( combined ) . createsampler ( rng ) ;
data [ i ] + = d ;
final double d = delta * ( 2 * rng . nextdouble ( ) - 1 ) ;
private static void jitter ( final double [ ] data , final uniformrandomprovider rng , final double delta ) {
jitter ( y , rng , mindelta ) ;
mindelta * = 2 ;
uniformrandomprovider rng = randomsource . create ( randomsource . jdk , 100 ) ;
jitter ( x , rng , mindelta ) ;
throw new mathinternalerror ( ) ;
mindelta * = 2 ;
final uniformrandomprovider rng = randomsource . create ( randomsource . two cmres , 654321 ) ;
final cartesian3d v = vertices . get ( facet [ i ] ) ;
connectablesegment ( final cartesian2d start , final cartesian2d end , final line line ,
final cartesian2d [ ] two2points = new cartesian2d [ facet . length ] ;
import org . apache . commons . math4 . geometry . euclidean . oned . cartesian1d ;
final cartesian3d point ,
final cartesian3d hit3d = plane . intersection ( line ) ;
private static int [ ] [ ] successors ( final list < cartesian3d > vertices , final list < int [ ] > facets ,
public cartesian2d getlocation ( ) {
final cartesian3d facetb = plane . tospace ( polygon . getbarycenter ( ) ) ;
return new cartesian3d ( 1 . 0 , ( cartesian3d ) point , 1 . 0 , translation ) ;
private bsptree < euclidean2d > selectclosest ( final cartesian2d point , final iterable < bsptree < euclidean2d > > candidates ) {
final cartesian2d . . . vertices ) {
private static list < subhyperplane < euclidean3d > > buildboundary ( final list < cartesian3d > vertices ,
final plane pzmax = new plane ( new cartesian3d ( 0 , 0 , zmax ) , cartesian3d . plus k , tolerance ) ;
final cartesian2d tp00 = tplane . tosubspace ( apply ( p00 ) ) ;
final cartesian2d end = segment . getend ( ) ;
for ( cartesian2d [ ] loop : v ) {
for ( final cartesian2d point : loop ) {
double x = segment . getline ( ) . tosubspace ( segment . getend ( ) ) . getx ( ) ;
final cartesian3d p00 = oplane . getorigin ( ) ;
final double distance = cartesian2d . distance ( end , candidatenext . getstart ( ) ) ;
final cartesian3d vi = vertices . get ( i ) ;
setbarycenter ( ( point < euclidean2d > ) new cartesian2d ( sumx / ( 3 * sum ) , sumy / ( 3 * sum ) ) ) ;
final cartesian2d tp10 = tplane . tosubspace ( apply ( p10 ) ) ;
public polygonsset ( final double hyperplanethickness , final cartesian2d . . . vertices ) {
final plane pzmin = new plane ( new cartesian3d ( 0 , 0 , zmin ) , cartesian3d . minus k , tolerance ) ;
vertex ( final cartesian2d location ) {
vertices = new cartesian2d [ 0 ] [ ] ;
import org . apache . commons . math4 . geometry . euclidean . twod . cartesian2d ;
rotationtransform ( final cartesian3d center , final rotation rotation ) {
private final cartesian3d center ;
null : line . tospace ( new cartesian1d ( i . getsup ( ) ) ) ;
array [ j + + ] = segment . getline ( ) . tospace ( new cartesian1d ( x ) ) ;
public polyhedronsset ( final list < cartesian3d > vertices , final list < int [ ] > facets ,
final cartesian2d minmax = new cartesian2d ( xmin , ymax ) ;
final cartesian2d [ ] array = new cartesian2d [ loop . size ( ) + 2 ] ;
vertices [ i + + ] = new cartesian2d [ ] {
final cartesian2d tp01 = tplane . tosubspace ( apply ( p01 ) ) ;
final cartesian3d end = vertices . get ( successors [ v ] [ k ] ) ;
private subhyperplane < euclidean3d > boundaryfacet ( final cartesian3d point ,
final plane pxmin = new plane ( new cartesian3d ( xmin , 0 , 0 ) , cartesian3d . minus i , tolerance ) ;
final cartesian3d delta = ( ( cartesian3d ) point ) . subtract ( center ) ;
final cartesian3d start = vertices . get ( v ) ;
final cartesian2d maxmin = new cartesian2d ( xmax , ymin ) ;
private static int [ ] [ ] findreferences ( final list < cartesian3d > vertices , final list < int [ ] > facets ) {
final cartesian3d p10 = oplane . tospace ( new cartesian2d ( 1 . 0 , 0 . 0 ) ) ;
final cartesian2d point2d = ( ( plane ) node . getcut ( ) . gethyperplane ( ) ) . tosubspace ( point ) ;
private final cartesian2d location ;
null : line . tospace ( new cartesian1d ( i . getinf ( ) ) ) ;
private final cartesian3d translation ;
line . tospace ( new cartesian1d ( - float . max value ) ) ,
double x = segment . getline ( ) . tosubspace ( segment . getstart ( ) ) . getx ( ) ;
final plane pxmax = new plane ( new cartesian3d ( xmax , 0 , 0 ) , cartesian3d . plus i , tolerance ) ;
setbarycenter ( ( point < euclidean2d > ) cartesian2d . nan ) ;
public polyhedronsset rotate ( final cartesian3d center , final rotation rotation ) {
final cartesian2d shift = tplane . tosubspace ( apply ( oplane . getorigin ( ) ) ) ;
final cartesian2d endv = double . isinfinite ( i . getsup ( ) ) ?
setbarycenter ( ( point < euclidean2d > ) new cartesian2d ( 0 , 0 ) ) ;
final plane pymin = new plane ( new cartesian3d ( 0 , ymin , 0 ) , cartesian3d . minus j , tolerance ) ;
public subhyperplane < euclidean3d > firstintersection ( final cartesian3d point , final line line ) {
vertices = new cartesian2d [ loops . size ( ) ] [ ] ;
final cartesian2d [ ] [ ] v = getvertices ( ) ;
final cartesian2d startv = double . isinfinite ( i . getinf ( ) ) ?
return new cartesian3d ( 1 . 0 , center , 1 . 0 , rotation . applyto ( delta ) ) ;
setbarycenter ( ( point < euclidean3d > ) cartesian3d . nan ) ;
final cartesian3d p01 = oplane . tospace ( new cartesian2d ( 0 . 0 , 1 . 0 ) ) ;
final cartesian2d minmin = new cartesian2d ( xmin , ymin ) ;
final cartesian2d maxmax = new cartesian2d ( xmax , ymax ) ;
public polyhedronsset translate ( final cartesian3d translation ) {
translationtransform ( final cartesian3d translation ) {
final plane pymax = new plane ( new cartesian3d ( 0 , ymax , 0 ) , cartesian3d . plus j , tolerance ) ;
public cartesian3d apply ( final point < euclidean3d > point ) {
final cartesian3d end = vertices . get ( vb ) ;
setbarycenter ( ( point < euclidean3d > ) new cartesian3d ( 1 . 0 / ( 4 * getsize ( ) ) , ( cartesian3d ) getbarycenter ( ) ) ) ;
setbarycenter ( ( point < euclidean3d > ) new cartesian3d ( 0 , 0 , 0 ) ) ;
public cartesian2d [ ] [ ] getvertices ( ) {
private cartesian2d [ ] [ ] vertices ;
line . tospace ( new cartesian1d ( + float . max value ) )
final cartesian3d start = vertices . get ( va ) ;
if ( cartesian3d . distance ( vi , vertices . get ( j ) ) < = tolerance ) {
setbarycenter ( ( point < euclidean3d > ) new cartesian3d ( 1 . 0 , ( cartesian3d ) getbarycenter ( ) , scaled , facetb ) ) ;
final cartesian2d [ ] array = new cartesian2d [ loop . size ( ) ] ;
return eventit . next ( ) ;
typetoclassmap . put ( line . substring ( 0 , line . indexof ( ' ' ) ) , annotationconfiguration . attribute type ) ;
return new attributeannotation ( values [ id offset ] . getcoveredtext ( line ) . tostring ( ) ,
}
default :
case "relations" :
if ( values . length = = 4 ) {
public static final string attribute type = "attribute" ;
values [ type offset ] . getcoveredtext ( line ) . tostring ( ) ,
bratannotation parse ( span [ ] values , charsequence line ) throws ioexception {
throw new invalidformatexception ( "line must have 3 or 4 fields" ) ;
private static final int value offset = 3 ;
static class attributeannotationparser extends bratannotationparser {
case "attributes" :
switch ( sectiontype ) {
else {
parsers . put ( annotationconfiguration . attribute type , new attributeannotationparser ( ) ) ;
value = values [ value offset ] . getcoveredtext ( line ) . tostring ( ) ;
string value = null ;
@ override
case "entities" :
values [ attached to offset ] . getcoveredtext ( line ) . tostring ( ) , value ) ;
" type class , no parser registered : " + tokens [ bratannotationparser . type offset ]
. getcoveredtext ( line ) . tostring ( ) ) ;
private static final int attached to offset = 2 ;
break ;
if ( values . length = = 3 | | values . length = = 4 ) {
}
mnamefinder . clearadaptivedata ( ) ;
int [ ] tmpoutcomes = new int [ numoutcomes ] ;
pred labels = new string [ pmap . size ( ) ] ;
}
object [ ] data = model . getdatastructures ( ) ;
if ( cp . compareto ( predicate ) = = 0 ) {
} else {
tmpoutcomes [ numparams ] = outcomepattern [ pi ] ;
import opennlp . tools . ml . model . abstractmodel ;
import java . io . ioexception ;
this . numoutcomes = model . getnumoutcomes ( ) ;
writeutf ( s . name ) ;
activeparams [ pi ] = tmpparams [ pi ] ;
import opennlp . tools . ml . model . comparablepredicate ;
import opennlp . tools . ml . model . context ;
writeint ( compressed . size ( ) ) ;
writeint ( sorted . length ) ;
numparams + + ;
comparablepredicate [ ] tmppreds = new comparablepredicate [ params . length ] ;
pmap . toarray ( pred labels ) ;
public abstract class naivebayesmodelwriter extends abstractmodelwriter {
writeint ( outcome labels . length ) ;
comparablepredicate cp = sorted [ 0 ] ;
cp = predicate ;
int [ ] activeoutcomes = new int [ numparams ] ;
double [ ] predparams = params [ pid ] . getparameters ( ) ;
for ( string label : outcome labels ) {
system . err . println ( outcomepatterns . size ( ) + " outcome patterns" ) ;
system . arraycopy ( tmppreds , 0 , sortpreds , 0 , numpreds ) ;
for ( comparablepredicate predicate : sorted ) {
tmpparams [ numparams ] = predparams [ pi ] ;
if ( predparams [ pi ] ! = 0d ) {
outcomepatterns . add ( newgroup ) ;
import opennlp . tools . ml . model . indexhashtable ;
import java . util . arraylist ;
writeutf ( a . size ( ) + a . get ( 0 ) . tostring ( ) ) ;
params = ( context [ ] ) data [ 0 ] ;
protected comparablepredicate [ ] sortvalues ( ) {
for ( list < comparablepredicate > a : compressed ) {
activeoutcomes [ pi ] = tmpoutcomes [ pi ] ;
protected context [ ] params ;
if ( numparams ! = 0 ) {
for ( int pid = 0 ; pid < params . length ; pid + + ) {
import java . util . list ;
system . err . println ( "compressed " + params . length + " parameters to " + numpreds ) ;
return outcomepatterns ;
for ( int pi = 0 ; pi < predparams . length ; pi + + ) {
protected string [ ] outcome labels ;
arrays . sort ( sortpreds ) ;
writedouble ( sorted [ i ] . params [ j ] ) ;
for ( comparablepredicate s : sorted ) {
writeutf ( "naivebayes" ) ;
list < comparablepredicate > newgroup = new arraylist < comparablepredicate > ( ) ;
public void persist ( ) throws ioexception {
indexhashtable < string > pmap = ( indexhashtable < string > ) data [ 1 ] ;
for ( int pi = 0 ; pi < numparams ; pi + + ) {
int numoutcomes ;
for ( int j = 0 ; j < sorted [ i ] . params . length ; j + + )
list < list < comparablepredicate > > outcomepatterns = new arraylist < list < comparablepredicate > > ( ) ;
import opennlp . tools . ml . model . abstractmodelwriter ;
newgroup = new arraylist < comparablepredicate > ( ) ;
return sortpreds ;
numpreds + + ;
sortpreds = new comparablepredicate [ numpreds ] ;
double [ ] activeparams = new double [ numparams ] ;
writeutf ( label ) ;
list < list < comparablepredicate > > compressed = computeoutcomepatterns ( sorted ) ;
int [ ] outcomepattern = params [ pid ] . getoutcomes ( ) ;
int numpreds = 0 ;
comparablepredicate [ ] sortpreds ;
close ( ) ;
outcome labels = ( string [ ] ) data [ 2 ] ;
import java . util . arrays ;
package opennlp . tools . ml . naivebayes ;
protected list < list < comparablepredicate > > computeoutcomepatterns ( comparablepredicate [ ] sorted ) {
newgroup . add ( predicate ) ;
int numparams = 0 ;
for ( int i = 0 ; i < sorted . length ; i + + )
comparablepredicate [ ] sorted = sortvalues ( ) ;
public naivebayesmodelwriter ( abstractmodel model ) {
tmppreds [ numpreds ] = new comparablepredicate ( pred labels [ pid ] , activeoutcomes , activeparams ) ;
protected string [ ] pred labels ;
double [ ] tmpparams = new double [ numoutcomes ] ;
string text = sb . substring ( 0 , sb . length ( ) ) ;
string tokenizermodelname = cmdlineutil . getparameter ( " - tk" , args ) ;
tokenizer = new tokenizerme ( tokenizermodel ) ;
public static parse [ ] parseline ( string line , parser parser , tokenizer tokenizer , int numparses ) {
return "usage : " + cli . cmd + " " + getname ( ) + " [ - bs n - ap n - k n - tk tok model ] model < sentences \ n"
import opennlp . tools . tokenize . tokenizer ;
import opennlp . tools . tokenize . tokenizermodel ;
}
public static parse [ ] parseline ( string line , parser parser , int numparses ) {
if ( tokenizermodelname ! = null ) {
import opennlp . tools . tokenize . whitespacetokenizer ;
+ " - tk tok model : use the specified tokenizer model to tokenize the sentences . defaults to a whitespacetokenizer . " ;
list < string > tokens = arrays . aslist ( tokenizer . tokenize ( line ) ) ;
import opennlp . tools . parser . parser ;
import java . util . arrays ;
parse [ ] parses = parseline ( line , parser , tokenizer , numparses ) ;
for ( string tok : tokens ) {
import opennlp . tools . cmdline . tokenizer . tokenizermodelloader ;
tokenizer tokenizer = whitespacetokenizer . instance ;
parser parser = parserfactory . create ( model , beamsize , advancepercentage ) ;
tokenizermodel tokenizermodel = new tokenizermodelloader ( ) . load ( new file ( tokenizermodelname ) ) ;
+ " - k n : show the top n parses . this will also display their log - probablities . \ n"
return parseline ( line , parser , whitespacetokenizer . instance , numparses ) ;
import opennlp . tools . tokenize . tokenizerme ;
! = version . getminor ( ) ) {
if ( version . currentversion ( ) . getmajor ( ) = = version . getmajor ( ) & & ( version . currentversion ( ) . getminor ( ) - 2 )
private static final string token class prefix = "wc" ;
else if ( pattern . isallcapitalletter ( ) ) {
else if ( pattern . containsperiod ( ) ) {
feat = "ic" ;
feat = "other" ;
else if ( pattern . containsslash ( ) ) {
else if ( pattern . containshyphen ( ) ) {
else if ( capperiod . matcher ( token ) . find ( ) ) {
feat = "dp" ;
}
else if ( pattern . isallcapitalletter ( ) & & token . length ( ) = = 1 ) {
feat = "lc" ;
else if ( pattern . digits ( ) = = 4 ) {
feat = "4d" ;
else if ( pattern . isinitialcapitalletter ( ) ) {
private static final pattern capperiod = pattern . compile ( " ^ [ a - z ] \ \ . $" ) ;
stringpattern pattern = stringpattern . recognize ( token ) ;
return ( feat ) ;
feat = "num" ;
feat = "2d" ;
else {
feat = "dd" ;
else if ( pattern . containscomma ( ) ) {
feat = "cp" ;
string feat ;
feat = "sc" ;
private static final string token and class prefix = "w & c" ;
if ( pattern . containsletters ( ) ) {
feat = "ds" ;
feat = "dc" ;
else if ( pattern . containsdigit ( ) ) {
if ( pattern . isalllowercaseletter ( ) ) {
import java . util . regex . pattern ;
feat = "ac" ;
else if ( pattern . digits ( ) = = 2 ) {
feat = "an" ;
}
if ( !objects . isnull ( this . defaulttype ) ) {
import java . util . objects ;
string outcomes [ ] = codec . encode ( names , sample . getsentence ( ) . length ) ;
public namefindereventstream ( objectstream < namesample > datastream , string type , namecontextgenerator contextgenerator , sequencecodec < string > codec ) {
list < event > events = new arraylist < > ( outcomes . length ) ;
if ( objects . isnull ( n . gettype ( ) ) ) {
for ( int i = 0 ; i < names . length ; i + + ) {
overridedefaulttype ( names ) ;
names [ i ] = new span ( n . getstart ( ) , n . getend ( ) , this . defaulttype ,
span n = names [ i ] ;
n . getprob ( ) ) ;
private void overridedefaulttype ( span [ ] names ) {
this . defaulttype = type ;
private final string defaulttype ;
span [ ] names = sample . getnames ( ) ;
return train ( indexer ) ;
return model ;
import java . util . hashmap ;
}
public abstract class abstracteventtrainer extends abstracttrainer implements eventtrainer {
string dataindexername = parameters . getstringparam ( data indexer param ,
indexer = new onepassdataindexer ( ) ;
parameters . addtoreport ( abstracttrainer . trainer type param , eventtrainer . event value ) ;
indexer = new twopassdataindexer ( ) ;
indexer . init ( indexparams , parameters . getreportmap ( ) ) ;
indexparams . put ( abstractdataindexer . cutoff param , integer . tostring ( getcutoff ( ) ) ) ;
map < string , string > indexparams = new hashmap < string , string > ( ) ;
public final maxentmodel train ( dataindexer indexer ) throws ioexception {
maxentmodel model = dotrain ( indexer ) ;
import java . util . map ;
import opennlp . tools . ml . model . abstractdataindexer ;
if ( !isvalid ( ) ) {
indexer . index ( events ) ;
string dataindexer = parameters . getstringparam ( data indexer param ,
indexparams . put ( abstractdataindexer . sort param , boolean . tostring ( issortandmerge ( ) ) ) ;
throw new illegalargumentexception ( "trainparams are not valid!" ) ;
protected map < string , string > reportmap = new hashmap < > ( ) ;
dataindexer indexer = dataindexerfactory . getdataindexer ( trainingparameters , reportmap ) ;
if ( trainingparameters . getintparameter ( cutoff param , - 1 ) = = - 1 ) {
import opennlp . tools . ml . model . dataindexerfactory ;
trainingparameters . put ( abstractdataindexer . sort param , boolean . tostring ( issortandmerge ( ) ) ) ;
public static final string data indexer one pass real value = "onepassrealvalue" ;
addtoreport ( abstracttrainer . trainer type param , eventtrainer . event value ) ;
trainingparameters . put ( cutoff param , "5" ) ;
return new spanannotation ( id , type , new span ( parseint ( values [ begin offset ]
. getcoveredtext ( line ) . tostring ( ) ) , endoffset , type ) , coveredtext ) ;
private static final string smoothing param = "smoothing" ;
public gismodel trainmodel ( objectstream < event > eventstream ) throws ioexception {
public gistrainer ( ) {
return model ;
public boolean issortandmerge ( ) {
abstractmodel model ;
}
return trainmodel ( eventstream , 100 , 0 ) ;
public gismodel trainmodel ( int iterations , dataindexer di , int threads ) {
return true ;
import opennlp . tools . ml . model . abstractmodel ;
protected void display ( string s ) {
indexingparameters . put ( gistrainer . iterations param , integer . tostring ( iterations ) ) ;
public class gistrainer extends abstracteventtrainer {
import opennlp . tools . ml . abstracteventtrainer ;
int iterations = getiterations ( ) ;
public maxentmodel dotrain ( dataindexer indexer ) throws ioexception {
this . setsmoothing ( smoothing ) ;
boolean smoothing = trainingparameters . getbooleanparameter ( smoothing param , smoothing default ) ;
private static final double smoothing observation = 0 . 1 ;
private static final boolean smoothing default = false ;
@ override
public static final string maxent value = "maxent" ;
return trainmodel ( iterations , di , new uniformprior ( ) , threads ) ;
import opennlp . tools . ml . model . maxentmodel ;
indexingparameters . put ( gistrainer . cutoff param , integer . tostring ( cutoff ) ) ;
int threads = trainingparameters . getintparameter ( trainingparameters . threads param , 1 ) ;
model = trainmodel ( iterations , indexer , threads ) ;
this . dictmap . put ( arrays . aslist ( elems [ 0 ] , elems [ 1 ] ) , arrays . aslist ( elems [ 2 ] ) ) ;
return alllemmas ;
while ( ( line = breader . readline ( ) ) ! = null ) {
public dictionarylemmatizer ( final inputstream dictionary ) throws ioexception {
lemmas . add ( this . lemmatize ( tokens [ i ] , postags [ i ] ) ) ;
for ( int i = 0 ; i < tokens . size ( ) ; i + + ) {
final bufferedreader breader = new bufferedreader (
alllemmas . add ( this . getalllemmas ( tokens . get ( i ) , postags . get ( i ) ) ) ;
}
list < string > lemmaslist = new arraylist < > ( ) ;
new inputstreamreader ( dictionary ) ) ;
private final map < list < string > , list < string > > dictmap ;
list < list < string > > alllemmas = new arraylist < > ( ) ;
} else {
final list < string > keys = this . getdictkeys ( word , postag ) ;
return lemmaslist ;
lemmaslist . add ( "o" ) ;
private list < string > getalllemmas ( final string word , final string postag ) {
lemma = keyvalues . get ( 0 ) ;
if ( !keyvalues . isempty ( ) ) {
private string lemmatize ( final string word , final string postag ) {
final string [ ] elems = line . split ( "" ) ;
lemmaslist . addall ( keyvalues ) ;
public list < list < string > > lemmatize ( final list < string > tokens , final list < string > postags ) {
public map < list < string > , list < string > > getdictmap ( ) {
final list < string > keyvalues = this . dictmap . get ( keys ) ;
bytearrayoutputstream bytes = new bytearrayoutputstream ( ) ;
try ( inputstream in = tokennamefinderfactory . class . getresourceasstream (
this . resources = resources ;
map < string , object > resources , tagdictionary posdictionary )
featuregeneratorbytes = artifactprovider . getartifact (
}
private byte [ ] featuregeneratorbytes ;
protected map < string , object > getresources ( ) {
int len ;
if ( featuregeneratorbytes = = null ) {
private static byte [ ] loaddefaultfeaturegeneratorbytes ( ) {
} catch ( invalidformatexception e ) {
adaptivefeaturegenerator generator ;
while ( ( len = in . read ( buf ) ) > 0 ) {
thefactory = extensionloader . instantiateextension (
if ( artifactprovider ! = null ) {
throw new illegalstateexception ( "failed reading from pos - default - features . xml file on classpath!" ) ;
if ( subclassname = = null ) {
try {
thefactory . init ( featuregeneratorbytes , resources , posdictionary ) ;
public static class posdictionaryserializer implements artifactserializer < posdictionary > {
return thefactory ;
thefactory = new postaggerfactory ( null , posdictionary ) ;
import opennlp . tools . util . featuregen . adaptivefeaturegenerator ;
return collections . emptymap ( ) ;
postaggerfactory . class , subclassname ) ;
public postaggerfactory ( byte [ ] featuregeneratorbytes , final map < string , object > resources ,
import opennlp . tools . util . version ;
@ deprecated
import opennlp . tools . util . featuregen . aggregatedfeaturegenerator ;
throws invalidformatexception {
import java . io . bytearrayoutputstream ;
} ) ;
public postaggerfactory ( dictionary ngramdictionary , tagdictionary posdictionary ) {
return featuregeneratorbytes ;
public adaptivefeaturegenerator createfeaturegenerators ( ) {
throw new illegalstateexception ( ) ;
catch ( ioexception e ) {
if ( resources ! = null ) {
else {
+ " . the initialization throw an exception . " ;
this . posdictionary = posdictionary ;
byte [ ] buf = new byte [ 1024 ] ;
string msg = "could not instantiate the " + subclassname
this . featuregeneratorbytes = loaddefaultfeaturegeneratorbytes ( ) ;
if ( version . currentversion ( ) . getminor ( ) > = 8 ) {
return generator ;
throw new invalidformatexception ( msg , e ) ;
posdictionaryserializer . register ( serializers ) ;
protected void init ( byte [ ] featuregeneratorbytes , final map < string , object > resources ,
throw new illegalstateexception ( "classpath must contain pos - default - features . xml file!" ) ;
generator = generatorfactory . create ( descriptorin , key - > {
import java . io . bytearrayinputstream ;
return getposcontextgenerator ( 0 ) ;
if ( this . featuregeneratorbytes = = null ) {
tagdictionary posdictionary ) {
private map < string , object > resources ;
} catch ( ioexception e ) {
featuregeneratorbytes = loaddefaultfeaturegeneratorbytes ( ) ;
posmodel . generator descriptor entry name ) ;
} catch ( exception e ) {
if ( in = = null ) {
import opennlp . tools . namefind . tokennamefinderfactory ;
return resources ;
return new configurableposcontextgenerator ( cachesize , createfeaturegenerators ( ) ) ;
protected byte [ ] getfeaturegenerator ( ) {
if ( version . currentversion ( ) . getminor ( ) < 8 ) {
if ( featuregeneratorbytes = = null & & artifactprovider ! = null ) {
return artifactprovider . getartifact ( key ) ;
postaggerfactory thefactory ;
throw new illegalstateexception ( "reading from mem cannot result in an i / o error" , e ) ;
this . featuregeneratorbytes = featuregeneratorbytes ;
public static postaggerfactory create ( string subclassname , byte [ ] featuregeneratorbytes ,
inputstream descriptorin = new bytearrayinputstream ( featuregeneratorbytes ) ;
import opennlp . tools . util . featuregen . generatorfactory ;
return resources . get ( key ) ;
return bytes . tobytearray ( ) ;
bytes . write ( buf , 0 , len ) ;
" / opennlp / tools / postag / pos - default - features . xml" ) ) {
bytearrayoutputstream bytes = new bytearrayoutputstream ( ) ;
throw new terminatetoolexception ( - 1 , "io error while loading resources" , e ) ;
if ( posmodel . getmanifestproperty ( beamsearch . beam size parameter ) = = null ) {
import opennlp . tools . cmdline . namefind . tokennamefindertrainertool ;
new file ( resourcespath . tofile ( ) , "en - pos - perceptron . bin" ) . topath ( ) ,
null , posmodel . getfactory ( ) ) ;
package opennlp . tools . util . model ;
params . put ( "threads" , "4" ) ;
posmodel = new posmodel ( posmodel . getlanguage ( ) , posmodel . getposmodel ( ) , 10 ,
cv . evaluate ( filteredsamples , 5 ) ;
return posmodel ;
import opennlp . tools . util . version ;
filteredsamples = samples ;
paths . get ( this . getclass ( ) . getresource ( "ner - en pos - features . xml" ) . touri ( ) ) . tofile ( ) ) ;
}
byte [ ] featuregen = bytes . tobytearray ( ) ;
resources = tokennamefindertrainertool . loadresources ( resourcespath . tofile ( ) ,
throws ioexception {
@ test
import java . io . outputstream ;
catch ( ioexception | urisyntaxexception e ) {
tokennamefindercrossvalidator cv = new tokennamefindercrossvalidator ( "en" , null ,
import java . io . bytearrayoutputstream ;
import opennlp . tools . postag . posmodel ;
import java . nio . file . paths ;
artifact . serialize ( out ) ;
import opennlp . tools . ml . beamsearch ;
import java . io . ioexception ;
trainingparameters params = modelutil . createdefaulttrainingparameters ( ) ;
int len ;
files . copy ( new file ( evalutil . getopennlpdatadir ( ) , "models - sf / en - pos - perceptron . bin" ) . topath ( ) ,
catch ( ioexception e ) {
if ( version . getmajor ( ) = = 1 & & version . getminor ( ) = = 5 ) {
throw new illegalstateexception ( "failed reading from ner - default - features . xml file on classpath!" ) ;
public posmodel create ( inputstream in ) throws ioexception {
byte [ ] buf = new byte [ 1024 ] ;
public void serialize ( posmodel artifact , outputstream out )
import opennlp . tools . cmdline . terminatetoolexception ;
import java . nio . file . files ;
try ( objectstream < namesample > samples = createnamesamplestream ( ) ) {
import java . net . urisyntaxexception ;
while ( ( len = in . read ( buf ) ) > 0 ) {
public class posmodelserializer implements artifactserializer < posmodel > {
path resourcespath = files . createtempdirectory ( "opennlp resources" ) ;
import java . io . inputstream ;
import java . nio . file . path ;
import java . util . map ;
import java . nio . file . standardcopyoption ;
posmodel posmodel = new posmodel ( new uncloseableinputstream ( in ) ) ;
bytes . write ( buf , 0 , len ) ;
standardcopyoption . replace existing ) ;
try ( inputstream in = this . getclass ( ) . getresourceasstream (
params , featuregen , resources ) ;
public void evalalltypeswithposnamefinder ( ) throws ioexception {
"ner - en pos - features . xml" ) ) {
version version = posmodel . getversion ( ) ;
map < string , object > resources ;
assert . assertequals ( 0 . 8044097625338349d , cv . getfmeasure ( ) . getfmeasure ( ) , 0 . 001d ) ;
objectstream < namesample > filteredsamples ;
try {
return tokensample . parse ( text . tostring ( ) , tokensample . default separator chars ) ;
int tokenindex = text . indexof ( token , searchindex ) ;
string token = wordline . getform ( ) ;
continue ;
throw new ioexception ( string . format ( "failed to match token [ % s ] in sentence [ % s ] with text [ % s ] " ,
import opennlp . tools . util . stringutil ;
tokensample . default separator chars ) ;
}
if ( charaftertokenindex < text . length ( ) ) {
int charaftertokenindex = tokenindex + token . length ( ) ;
package opennlp . tools . formats . conllu ;
if ( sentence . gettextcomment ( ) ! = null ) {
if ( sentence ! = null ) {
if ( !stringutil . iswhitespace ( text . charat ( charaftertokenindex ) ) ) {
public conllutokensamplestream ( objectstream < conllusentence > samples ) {
import java . io . ioexception ;
throw new ioexception ( "sentence is missing raw text sample!" ) ;
public tokensample read ( ) throws ioexception {
conllusentence sentence = samples . read ( ) ;
int searchindex = 0 ;
for ( conlluwordline wordline : sentence . getwordlines ( ) ) {
import opennlp . tools . tokenize . tokensample ;
else {
text . insert ( charaftertokenindex ,
stringbuilder text = new stringbuilder ( sentence . gettextcomment ( ) ) ;
if ( wordline . getid ( ) . contains ( " . " ) ) {
searchindex + = tokensample . default separator chars . length ( ) ;
public class conllutokensamplestream extends filterobjectstream < conllusentence , tokensample > {
import opennlp . tools . util . filterobjectstream ;
super ( samples ) ;
@ override
return null ;
searchindex + = token . length ( ) ;
import opennlp . tools . util . objectstream ;
token , sentence . getsentenceidcomment ( ) , text ) ) ;
if ( tokenindex = = - 1 ) {
tokenindexmap . put ( - ( sentence . getstart ( ) + tokens [ i ] . getstart ( ) ) , i ) ;
conflictingname . getstart ( ) < sentence . getstart ( ) ) {
entityidset . remove ( ann . getid ( ) ) ;
list < span > names = new arraylist < > ( ) ;
private sentencedetector sentdetector ;
public bratdocumentparser ( sentencedetector sentencedetector , tokenizer tokenizer ) {
for ( int i = 0 ; i < tokens . length ; i + + ) {
private tokenizer tokenizer ;
for ( span sentence : sentences ) {
+ entityspan . getcoveredtext ( sample . gettext ( ) ) + " ) " + " in document "
if ( sentences . size ( ) > 0 & & conflictingname ! = null & &
list < span > sentences = new arraylist < > ( ) ;
import opennlp . tools . namefind . namesample ;
import opennlp . tools . sentdetect . sentencedetector ;
import opennlp . tools . tokenize . tokenizer ;
sample . getid ( ) ) ;
entityidset . add ( ann . getid ( ) ) ;
entityspan = entityspan . trim ( sample . gettext ( ) ) ;
import java . util . list ;
for ( span sentence : sentdetector . sentposdetect ( sample . gettext ( ) ) ) {
+ sample . getid ( ) + " , it is not matching tokenization!" ) ;
if ( namebeginindex ! = null & & nameendindex ! = null ) {
}
map < integer , integer > tokenindexmap = new hashmap < > ( ) ;
sentences . add ( sentence ) ;
samples . add ( new namesample ( sample . getid ( ) , span . spanstostrings ( tokens , sentencetext ) ,
import java . util . hashmap ;
import java . util . hashset ;
import opennlp . tools . util . span ;
package opennlp . tools . formats . brat ;
sample . getid ( ) + " , is not matching sentence segmentation!" ) ;
span conflictingname = coveredindexes . get ( sentence . getstart ( ) ) ;
set < string > entityidset = new hashset < > ( ) ;
sentences . add ( new span ( lastsentence . getstart ( ) , sentence . getend ( ) ) ) ;
import java . util . set ;
this . sentdetector = sentencedetector ;
sample . gettext ( ) ) . tostring ( ) ;
system . err . println ( "dropped entity " + entity . getid ( ) + " ( "
spanannotation entity = ( spanannotation ) ann ;
names . add ( new span ( namebeginindex , nameendindex , entity . gettype ( ) ) ) ;
string sentencetext = sentence . getcoveredtext (
integer nameendindex = tokenindexmap . get ( entityspan . getend ( ) ) ;
span lastsentence = sentences . remove ( sentences . size ( ) - 1 ) ;
else {
public class bratdocumentparser {
span [ ] tokens = tokenizer . tokenizepos ( sentencetext ) ;
system . err . println ( "dropped entity " + id + " in document " +
tokenindexmap . put ( sentence . getstart ( ) + tokens [ i ] . getend ( ) , i + 1 ) ;
map < integer , span > coveredindexes = new hashmap < > ( ) ;
list < namesample > samples = new arraylist < > ( sentences . size ( ) ) ;
return samples ;
integer namebeginindex = tokenindexmap . get ( - entityspan . getstart ( ) ) ;
names . toarray ( new span [ names . size ( ) ] ) , null , samples . size ( ) = = 0 ) ) ;
import java . util . map ;
import java . util . arraylist ;
span span = ( ( spanannotation ) ann ) . getspan ( ) ;
for ( int i = span . getstart ( ) ; i < span . getend ( ) ; i + + ) {
coveredindexes . put ( i , span ) ;
if ( sentence . contains ( entityspan ) ) {
span entityspan = entity . getspan ( ) ;
if ( ann instanceof spanannotation ) {
system . out . println ( "correcting sentence segmentation in document " +
public list < namesample > parse ( bratdocument sample ) {
this . tokenizer = tokenizer ;
for ( bratannotation ann : sample . getannotations ( ) ) {
for ( string id : entityidset ) {
lineindex + + ;
if ( tabindex ! = - 1 ) {
sentences . add ( line ) ;
import java . util . random ;
string line ;
samplestring . append ( line . substring ( textstart ) + " " ) ;
import java . util . list ;
int tabindex = line . indexof ( '' ) ;
}
import java . util . stream . intstream ;
import java . util . hashset ;
new markablefileinputstreamfactory ( sentencesfile ) , standardcharsets . utf 8 ) ) {
stringbuilder samplestring = new stringbuilder ( ) ;
import java . util . set ;
collections . shuffle ( sentences , random ) ;
if ( selectedlines . contains ( lineindex ) ) {
try ( objectstream < string > linestream = new plaintextbylinestream (
int textstart = line . indexof ( '' ) + 1 ;
while ( ( line = linestream . read ( ) ) ! = null ) {
int count = 0 ;
set < integer > selectedlines = new hashset < > (
private final random random ;
import java . util . collections ;
if ( samplestring . length ( ) > 0 ) {
list < integer > indexes = intstream . range ( 0 , totallinecount )
import java . nio . file . files ;
this . lang = lang ;
list < string > sentences = new arraylist < > ( ) ;
. boxed ( ) . collect ( collectors . tolist ( ) ) ;
int lineindex = 0 ;
lineiterator = sentences . iterator ( ) ;
private iterator < string > lineiterator ;
indexes . sublist ( 0 , sentencespersample * numberofsamples ) ) ;
count + + ;
import java . util . arraylist ;
int totallinecount = ( int ) files . lines ( sentencesfile . topath ( ) ) . count ( ) ;
while ( count < sentencespersample & & lineiterator . hasnext ( ) ) {
collections . shuffle ( indexes , random ) ;
return new languagesample ( new language ( lang ) , samplestring ) ;
random = new random ( 23 ) ;
string line = lineiterator . next ( ) ;
if ( currll - prevll < llthreshold ) {
log likelihood threshold default ) ;
llthreshold = trainingparameters . getdoubleparameter ( log likelihood threshold param ,
public static final string log likelihood threshold param = "llthreshold" ;
public static final double log likelihood threshold default = 0 . 0001 ;
private double llthreshold = 0 . 0001 ;
public atom ( string s ) {
public boolean equals ( object o ) {
return delimited . substring ( 0 , delimited . length ( ) - 1 ) ;
if ( delimited . length ( ) = = 0 ) {
package org . apache . parquet . glob ;
private final fieldprojectionfilter fieldprojectionfilter ;
return children ;
import java . util . list ;
return "atom ( " + s + " ) " ;
}
private static boolean iskeyfieldofmap ( thriftfield currentfield , thriftfield previousfield ) {
private final list < globnode > children ;
for ( thriftfield field : fields ) {
if ( currenttype ! = null ) {
public string tostring ( ) {
public void push ( thriftfield f ) {
import org . apache . parquet . preconditions ;
return todelimitedstring ( " . " ) ;
public arraylist < thriftfield > getfields ( ) {
return visitor . visit ( this ) ;
stringbuilder delimited = new stringbuilder ( ) ;
public globnodesequence ( list < globnode > children ) {
public string get ( ) {
t visit ( globnodesequence seq ) ;
static class atom implements globnode {
currentrepetition = getrepetition ( field ) ;
return fields ;
this . s = s ;
private final arraylist < thriftfield > fields = new arraylist < thriftfield > ( ) ;
return "globnodesequence" + children ;
public < r > r accept ( visitor < r > visitor ) {
static class globnodesequence implements globnode {
this . children = children ;
if ( this = = o ) return true ;
return s . hashcode ( ) ;
return s ;
public oneof ( list < globnode > children ) {
private final string s ;
private type currenttype ;
public string todelimitedstring ( string delim ) {
private string currentname = "parquetschema" ;
private final fieldspath currentfieldpath = new fieldspath ( ) ;
delimited . append ( currentfield . getname ( ) ) . append ( delim ) ;
static interface visitor < t > {
return previoustype instanceof thrifttype . maptype & & ( ( thrifttype . maptype ) previoustype ) . getkey ( ) = = currentfield ;
return getclass ( ) = = o . getclass ( ) & & children . equals ( ( ( oneof ) o ) . children ) ;
if ( !fieldprojectionfilter . keep ( currentfieldpath ) ) {
interface globnode {
public int hashcode ( ) {
return getclass ( ) = = o . getclass ( ) & & s . equals ( ( ( atom ) o ) . s ) ;
this . fields . add ( f ) ;
@ override
t visit ( oneof oneof ) ;
this . fieldprojectionfilter = preconditions . checknotnull ( fieldprojectionfilter , "fieldprojectionfilter" ) ;
delimited . append ( "value" ) ;
} else if ( fieldspath . isvaluefieldofmap ( currentfield , previousfield ) ) {
delimited . append ( delim ) ;
delimited . append ( "key" ) ;
public list < globnode > getchildren ( ) {
private type . repetition currentrepetition = type . repetition . repeated ;
t visit ( atom atom ) ;
return "oneof" + children ;
return children . hashcode ( ) ;
private static boolean isvaluefieldofmap ( thriftfield currentfield , thriftfield previousfield ) {
return previoustype instanceof thrifttype . maptype & & ( ( thrifttype . maptype ) previoustype ) . getvalue ( ) = = currentfield ;
if ( fieldspath . iskeyfieldofmap ( currentfield , previousfield ) ) {
static class oneof implements globnode {
< r > r accept ( visitor < r > visitor ) ;
messagetype convertedmessagetype = visitor . getconvertedmessagetype ( ) ;
import it . unimi . dsi . fastutil . chars . chararraylist ;
case long :
} else if ( schema . gettype ( ) . equals ( schema . type . map ) ) {
return false ;
private final class < ? > mapclass ;
return new avroconverters . fieldintegerconverter ( parent ) ;
this . schema = mapschema ;
avrounionconverter . this . membervalue = = null ,
this ( null , parquetschema , avroschema , basemodel ) ;
if ( mapclass = = null | | mapclass . isassignablefrom ( hashmap . class ) ) {
return elementconverter ;
return currentrecord ;
} else if ( elementclass = = int . class ) {
converter = newconverter ( elementschema , repeatedtype , model , new parentvaluecontainer ( ) {
return new avroconverters . fieldstringconverter ( parent ) ;
setter . addlong ( value ) ;
for ( schema . field f : avroschema . getfields ( ) ) {
protected void set ( string name , int avroindex , object value ) {
container . add ( null ) ;
container . clear ( ) ;
set ( f . name ( ) , f . pos ( ) , defaultvalue ) ;
preconditions . checkargument ( arrayclass . isarray ( ) ,
schema avroschema , genericdata model ) {
avrorecordconverter . this . set ( avrofield . name ( ) , finalavroindex , value ) ;
public avrorecordconverter ( parentvaluecontainer parent ,
import java . util . map ;
schema elementschema = this . avroschema . getelementtype ( ) ;
final public void addbinary ( binary value ) {
for ( type parquetfield : parquetschema . getfields ( ) ) {
avrofieldindexes . put ( field . name ( ) , avrofieldindex + + ) ;
container . add ( element ) ;
import org . apache . parquet . io . api . binary ;
for ( map . entry < schema . field , object > entry : recorddefaults . entryset ( ) ) {
schema . field avrofield = avroschema . getfield ( parquetfieldname ) ;
if ( iselementtype ( repeatedtype , elementschema ) ) {
if ( elementclass = = boolean . class ) {
} else if ( schema . gettype ( ) . equals ( schema . type . int ) ) {
return model . deepcopy ( schema , value ) ;
valueconverter = newconverter ( nonnullvalueschema , valuetype , model , new parentvaluecontainer ( ) {
} ) ;
return true ;
import it . unimi . dsi . fastutil . booleans . booleanarraylist ;
return new avroconverters . fieldbytebufferconverter ( parent ) ;
"union is resolving to more than one type" ) ;
type repeatedtype = type . gettype ( 0 ) ;
private class < ? > containerclass ;
private v value ;
} catch ( nosuchmethodexception e ) {
nonnullelementschema , elementtype , model , new parentvaluecontainer ( ) {
final list < object > list = new arraylist < object > ( ) ;
type valuetype = keyvaluetype . gettype ( 1 ) ;
public avrorecordconverter ( messagetype parquetschema , schema avroschema ,
final bytearraylist list = new bytearraylist ( ) ;
membervalue = null ;
if ( !isset ) {
static final class avrocollectionconverter extends groupconverter {
private final schema avroschema ;
private final genericdata model ;
} else if ( schema . gettype ( ) . equals ( schema . type . record ) ) {
case int :
return new avroconverters . fieldfloatconverter ( parent ) ;
fillindefaults ( ) ;
} else if ( elementclass = = byte . class ) {
this . converters = new converter [ parquetschema . getfieldcount ( ) ] ;
import org . apache . parquet . io . invalidrecordexception ;
private final converter valueconverter ;
grouptype parquetgroup = parquetschema . asgrouptype ( ) ;
import it . unimi . dsi . fastutil . bytes . bytearraylist ;
this . currentrecord = ( t ) model . newrecord ( null , avroschema ) ;
this . keyvalueconverter = new mapkeyvalueconverter (
setter . addfloat ( value ) ;
return null ;
converter = new primitiveelementconverter (
class < ? > containerclass ) {
this . parent = parent ;
if ( parent ! = null ) {
if ( containerclass = = null ) {
if ( f . aliases ( ) . contains ( parquetfieldname ) ) {
} else if ( schema . gettype ( ) . equals ( schema . type . enum ) ) {
import org . apache . avro . specific . specificdata ;
default :
return new avroconverters . fieldbyteconverter ( parent ) ;
return f ;
if ( fieldindex = = 0 ) {
setter . add ( value ) ;
import org . apache . parquet . io . api . converter ;
map . put ( key , value ) ;
isset = false ;
import org . apache . avro . schema ;
static final class avroarrayconverter extends groupconverter {
setter . addshort ( value ) ;
private collection < ? > container ;
for ( string fieldname : avrofieldindexes . keyset ( ) ) {
public mapconverter ( parentvaluecontainer parent , grouptype maptype ,
parent . add ( container ) ;
final int finalavroindex = avrofieldindexes . remove ( avrofield . name ( ) ) ;
private final converter converter ;
} else if ( schema . gettype ( ) . equals ( schema . type . float ) ) {
private boolean isset ;
return new avroconverters . fieldbooleanconverter ( parent ) ;
this . avroschema = avroschema ;
public void addlong ( long value ) {
schema nonnullvalueschema = avroschemaconverter . getnonnull ( mapschema . getvaluetype ( ) ) ;
return new avrounionconverter ( parent , type , schema , model ) ;
schema . field field = avroschema . getfield ( fieldname ) ;
} else {
setter . adddouble ( value ) ;
private final converter keyvalueconverter ;
return new arraylist < object > ( ) ;
this . elementconverter = newconverter (
return new avroconverters . fieldbytearrayconverter ( parent ) ;
parent . add ( ( ( arraylist ) container ) . toarray ( ) ) ;
import org . apache . parquet . schema . grouptype ;
repeatedkeyvaluetype , mapschema , model ) ;
if ( field . schema ( ) . gettype ( ) = = schema . type . null ) {
memberconverters [ parquetindex ] = newconverter ( memberschema , membertype , model , new parentvaluecontainer ( ) {
this . memberconverters = new converter [ parquetgroup . getfieldcount ( ) ] ;
} else if ( schema . gettype ( ) . equals ( schema . type . union ) ) {
try {
schema . field f = entry . getkey ( ) ;
return converter ;
@ suppresswarnings ( "unchecked" )
final shortarraylist list = new shortarraylist ( ) ;
static final class avrounionconverter extends avroconverters . avrogroupconverter {
import java . util . hashmap ;
private static boolean iselementtype ( type repeatedtype , schema elementschema ) {
map < string , integer > avrofieldindexes = new hashmap < string , integer > ( ) ;
parent . add ( currentrecord ) ;
schema nonnullelementschema = avroschemaconverter . getnonnull ( elementschema ) ;
if ( repeatedtype . isprimitive ( ) | |
parquetindex + + ;
schema elementschema , genericdata model ,
converters [ parquetfieldindex + + ] = newconverter (
} else if ( elementclass = = float . class ) {
import java . util . arraylist ;
throw new invalidrecordexception ( string . format (
value = null ;
return keyvalueconverter ;
} else if ( schema . gettype ( ) . equals ( schema . type . string ) ) {
return valueconverter ;
import org . apache . parquet . io . api . primitiveconverter ;
"cannot convert list of optional elements to primitive array" ) ;
final chararraylist list = new chararraylist ( ) ;
case double :
private collection < object > container ;
import static org . apache . parquet . schema . type . repetition . required ;
} else if ( elementclass = = double . class ) {
private static converter newconverter ( schema schema , type type ,
repeatedtype . asgrouptype ( ) . getfieldcount ( ) > 1 ) {
object defaultvalue = deepcopy ( f . schema ( ) , entry . getvalue ( ) ) ;
public void addint ( int value ) {
} else if ( fieldindex = = 1 ) {
for ( int index = 0 ; index < avroschema . gettypes ( ) . size ( ) ; index + + ) {
private map < string , v > map ;
} else if ( datumclass = = char . class | | datumclass = = character . class ) {
@ override
import java . util . collection ;
if ( datumclass = = null ) {
public void addbyte ( byte value ) {
} else if ( elementclass = = char . class ) {
import it . unimi . dsi . fastutil . ints . intarraylist ;
return memberconverters [ fieldindex ] ;
} else if ( schema . gettype ( ) . equals ( schema . type . bytes ) ) {
this . map = newmap ( ) ;
parent . add ( ( ( booleanarraylist ) container ) . tobooleanarray ( ) ) ;
private class < ? > elementclass ;
public void start ( ) {
continue ;
} else if ( schema . gettype ( ) . equals ( schema . type . double ) ) {
elementschema . getfields ( ) . size ( ) = = 1 & &
repeatedtype . asgrouptype ( ) . getfieldname ( 0 ) ) ) {
final parentvaluecontainer setter ) {
schema mapschema , genericdata model ) {
import org . apache . parquet . preconditions ;
t getcurrentrecord ( ) {
public mapkeyvalueconverter ( grouptype keyvaluetype , schema mapschema ,
final booleanarraylist list = new booleanarraylist ( ) ;
import org . apache . parquet . schema . type ;
private final converter elementconverter ;
return ( class < t > ) getclassmethod . invoke ( schema ) ;
if ( !memberschema . gettype ( ) . equals ( schema . type . null ) ) {
parquetfieldname ) ) ;
model . setfield ( currentrecord , name , avroindex , value ) ;
super ( parent ) ;
return ( ( specificdata ) model ) . getclass ( schema ) ;
private schema . field getavrofield ( string parquetfieldname ) {
recorddefaults . put ( field , this . model . getdefaultvalue ( field ) ) ;
private object deepcopy ( schema schema , object value ) {
import it . unimi . dsi . fastutil . longs . longarraylist ;
container = newcontainer ( ) ;
}
public converter getconverter ( int fieldindex ) {
public void addshort ( short value ) {
import org . apache . avro . generic . genericdata ;
this . elementconverter = newconverter ( nonnullelementschema , elementtype , model , new parentvaluecontainer ( ) {
private final converter keyconverter ;
parent . add ( ( ( shortarraylist ) container ) . toshortarray ( ) ) ;
converter = new elementconverter ( repeatedtype . asgrouptype ( ) , elementschema , model ) ;
"cannot convert non - array : " + arrayclass . getname ( ) ) ;
this . container = list ;
return new avroconverters . fieldshortconverter ( parent ) ;
} else if ( elementclass = = long . class ) {
import java . lang . reflect . invocationtargetexception ;
final class primitiveelementconverter extends groupconverter {
int avrofieldindex = 0 ;
private map < string , v > newmap ( ) {
this . containerclass = containerclass ;
} else if ( schema . gettype ( ) . equals ( schema . type . array ) ) {
setter . addchar ( value ) ;
private object element ;
} else if ( elementschema ! = null & &
class < ? > datumclass = getdatumclass ( schema , model ) ;
key = null ;
this . elementclass = arrayclass . getcomponenttype ( ) ;
elementschema . getfields ( ) . get ( 0 ) . name ( ) . equals (
repeatedtype . asgrouptype ( ) , elementschema , model , setter ) ;
import org . apache . avro . reflect . reflectdata ;
final class mapkeyvalueconverter extends groupconverter {
return ( collection < object > ) reflectdata . newinstance ( containerclass , avroschema ) ;
schema avroschema , genericdata model ,
public void add ( object value ) {
fieldindex = = 0 , "illegal field index : " + fieldindex ) ;
} catch ( illegalaccessexception e ) {
for ( schema . field field : avroschema . getfields ( ) ) {
public avroarrayconverter ( parentvaluecontainer parent , grouptype type ,
throw new unsupportedoperationexception ( string . format (
return ( map < string , v > ) reflectdata . newinstance ( mapclass , schema ) ;
import it . unimi . dsi . fastutil . floats . floatarraylist ;
converter = newconverter ( elementschema , repeatedtype , model , setter ) ;
return converters [ fieldindex ] ;
import java . util . list ;
parent . add ( ( ( doublearraylist ) container ) . todoublearray ( ) ) ;
} else if ( datumclass = = short . class | | datumclass = = short . class ) {
return new avrorecordconverter ( parent , type . asgrouptype ( ) , schema , model ) ;
return new avrocollectionconverter (
setter . addboolean ( value ) ;
if ( schema . gettype ( ) . equals ( schema . type . boolean ) ) {
keyconverter = new primitiveconverter ( ) {
return keyconverter ;
} else if ( schema . gettype ( ) . equals ( schema . type . fixed ) ) {
schema nonnullschema = avroschemaconverter . getnonnull ( avrofield . schema ( ) ) ;
getclassmethod = modelclass . getmethod ( "getclass" , schema . class ) ;
switch ( schema . gettype ( ) ) {
return new avroconverters . fielddoubleconverter ( parent ) ;
elementconverter . this . element = value ;
private static < t > class < t > getdatumclass ( schema schema , genericdata model ) {
isset = true ;
private final schema schema ;
if ( model instanceof specificdata ) {
throw new illegalargumentexception ( "only the key ( 0 ) and value ( 1 ) fields expected : " + fieldindex ) ;
} else if ( datumclass . isarray ( ) & & datumclass . getcomponenttype ( ) = = byte . class ) {
genericdata model , parentvaluecontainer parent ) {
import java . lang . reflect . method ;
package org . apache . parquet . avro ;
parent . add ( ( ( longarraylist ) container ) . tolongarray ( ) ) ;
list . add ( value ) ;
final class elementconverter extends groupconverter {
private final map < schema . field , object > recorddefaults = new hashmap < schema . field , object > ( ) ;
public elementconverter ( grouptype repeatedtype , schema elementschema , genericdata model ) {
return new hashmap < string , v > ( ) ;
parent . add ( ( ( bytearraylist ) container ) . tobytearray ( ) ) ;
return new genericdata . array < object > ( 0 , avroschema ) ;
public avrounionconverter ( parentvaluecontainer parent , type parquetschema ,
final doublearraylist list = new doublearraylist ( ) ;
type membertype = parquetgroup . gettype ( parquetindex ) ;
membervalue = value ;
schema memberschema = avroschema . gettypes ( ) . get ( index ) ;
private void fillindefaults ( ) {
return new mapconverter ( parent , type . asgrouptype ( ) , schema , model ) ;
key = value . tostringusingutf8 ( ) ;
final schema . field avrofield = getavrofield ( parquetfield . getname ( ) ) ;
if ( avrofield ! = null ) {
parent . add ( map ) ;
return new avroconverters . fieldcharconverter ( parent ) ;
grouptype repeatedkeyvaluetype = maptype . gettype ( 0 ) . asgrouptype ( ) ;
parentvaluecontainer setter = createsetterandcontainer ( ) ;
this . model = ( model = = null ? reflectdata . get ( ) : model ) ;
private object membervalue = null ;
!elementclass . isprimitive ( ) | | elementtype . isrepetition ( required ) ,
private parentvaluecontainer createsetterandcontainer ( ) {
private string key ;
import org . apache . parquet . schema . messagetype ;
public primitiveelementconverter ( grouptype repeatedtype ,
parent . add ( ( ( floatarraylist ) container ) . tofloatarray ( ) ) ;
"parquet / avro schema mismatch : avro field ' % s' not found" ,
private collection < object > newcontainer ( ) {
public void addfloat ( float value ) {
return new avroconverters . fieldfixedconverter ( parent , schema , model ) ;
private final parentvaluecontainer parent ;
import org . apache . parquet . io . api . groupconverter ;
parent . add ( ( ( intarraylist ) container ) . tointarray ( ) ) ;
preconditions . checkargument (
private final converter [ ] converters ;
element = null ;
this . mapclass = getdatumclass ( mapschema , model ) ;
container . add ( value ) ;
public void addboolean ( boolean value ) {
} else if ( containerclass . isassignablefrom ( arraylist . class ) ) {
} catch ( invocationtargetexception e ) {
return avrofield ;
int parquetindex = 0 ;
final floatarraylist list = new floatarraylist ( ) ;
case boolean :
class < ? extends genericdata > modelclass = model . getclass ( ) ;
static final class mapconverter < v > extends groupconverter {
setter . addbyte ( value ) ;
method getclassmethod ;
genericdata basemodel ) {
} else if ( datumclass = = byte . class | | datumclass = = byte . class ) {
} else if ( elementclass = = short . class ) {
return value ;
parent , type . asgrouptype ( ) , schema , model , datumclass ) ;
setter . addint ( value ) ;
"cannot convert avro type : % s to parquet type : % s" , schema , type ) ) ;
private final converter [ ] memberconverters ;
import it . unimi . dsi . fastutil . doubles . doublearraylist ;
public void end ( ) {
protected t currentrecord ;
public avrocollectionconverter ( parentvaluecontainer parent , grouptype type ,
} else if ( schema . gettype ( ) . equals ( schema . type . long ) ) {
mapkeyvalueconverter . this . value = ( v ) value ;
public void adddouble ( double value ) {
final longarraylist list = new longarraylist ( ) ;
public void addchar ( char value ) {
return new avroarrayconverter (
return new parentvaluecontainer ( ) {
class avrorecordconverter < t > extends avroconverters . avrogroupconverter {
genericdata model ) {
type elementtype = repeatedtype . gettype ( 0 ) ;
if ( field . defaultvalue ( ) = = null | | this . model . getdefaultvalue ( field ) = = null ) {
case float :
import it . unimi . dsi . fastutil . shorts . shortarraylist ;
elementschema . gettype ( ) = = schema . type . record & &
nonnullschema , parquetfield , this . model , new parentvaluecontainer ( ) {
int parquetfieldindex = 0 ;
return new avroconverters . fieldenumconverter ( parent , schema , model ) ;
final intarraylist list = new intarraylist ( ) ;
grouptype parquetschema , schema avroschema ,
return new avroconverters . fieldlongconverter ( parent ) ;
} else if ( model . getclass ( ) = = genericdata . class ) {
} ;
parent . add ( ( ( chararraylist ) container ) . tochararray ( ) ) ;
if ( datumclass ! = null & & datumclass . isarray ( ) ) {
parent . add ( membervalue ) ;
class < ? > arrayclass ) {
public static memorymanager getmemorymanager ( ) {
thriftfield keyfield = maptype . getkey ( ) ;
return visitprimitivetype ( int64 , state ) ;
list < type > convertedchildren = new arraylist < type > ( ) ;
if ( childstate . repetition = = required ) {
state childstate = new state ( state . path . push ( child ) , getrepetition ( child ) , child . getname ( ) ) ;
return false ;
public static messagetype convert ( structtype struct , fieldprojectionfilter filter ) {
import org . apache . parquet . thrift . struct . thrifttype . bytetype ;
public convertedfield visit ( listtype listtype , state state ) {
private final boolean doprojection ;
import static org . apache . parquet . schema . types . primitive ;
}
boolean hassentinelunioncolumns = false ;
import static org . apache . parquet . schema . primitivetype . primitivetypename . int32 ;
return new keep ( state . path , new grouptype ( state . repetition , state . name , convertedchildren ) ) ;
this . doprojection = doprojection ;
throw new shouldneverhappenexception ( "encountered unknown structoruniontype" ) ;
public convertedfield visit ( booltype booltype , state state ) {
import org . apache . parquet . thrift . struct . thrifttype . i32type ;
} else {
type mapfield = maptype (
if ( isset & & doprojection ) {
sentinelvalue . askeep ( ) . gettype ( ) ) ;
public convertedfield visit ( structtype structtype , state state ) {
import static org . apache . parquet . schema . type . repetition . optional ;
if ( hasnonsentinelunioncolumns ) {
valuefield . gettype ( ) . accept ( new thriftschemaconvertvisitor ( new keeponlyfirstprimitivefilter ( ) , true ) , valuestate ) ;
public messagetype convert ( structtype struct ) {
private convertedfield visitprimitivetype ( primitivetypename type , originaltype orig , state state ) {
return new sentinelunion ( state . path , new grouptype ( state . repetition , state . name , convertedchildren ) ) ;
final boolean isunion = isunion ( structtype . getstructoruniontype ( ) ) ;
return visitprimitivetype ( int32 , state ) ;
public convertedfield visit ( i64type i64type , state state ) {
primitivebuilder < primitivetype > b = primitive ( type , state . repetition ) ;
this . fieldprojectionfilter = checknotnull ( fieldprojectionfilter , "fieldprojectionfilter" ) ;
this . name = name ;
convertedfield fullconv = listlike
import org . apache . parquet . thrift . convertedfield . keep ;
import org . apache . parquet . thrift . struct . thrifttype . stringtype ;
return messagetype ;
import org . apache . parquet . thrift . struct . thrifttype . listtype ;
import static org . apache . parquet . schema . type . repetition . repeated ;
return visitlistlike ( listtype . getvalues ( ) , state , false ) ;
if ( fieldprojectionfilter . keep ( state . path ) ) {
return new drop ( state . path ) ;
throw new thriftprojectionexception ( "cannot select only a subset of the fields in a map key , " +
import org . apache . parquet . thrift . struct . thrifttype . settype ;
public final string name ;
if ( isunion & & !converted . iskeep ( ) ) {
public state ( fieldspath path , repetition repetition , string name ) {
import org . apache . parquet . thrift . struct . thrifttype . doubletype ;
import org . apache . parquet . thrift . struct . thrifttype . i64type ;
. accept ( new thriftschemaconvertvisitor ( fieldprojectionfilter . all columns , false ) , childstate ) ;
if ( !fullconvkey . askeep ( ) . gettype ( ) . equals ( convertedkey . askeep ( ) . gettype ( ) ) ) {
private convertedfield visitlistlike ( thriftfield listlike , state state , boolean isset ) {
b = b . as ( orig ) ;
import org . apache . parquet . thrift . struct . thrifttype . maptype ;
public final fieldspath path ;
convertedfield fullconvkey = keyfield
convertedfield convertedkey = keyfield . gettype ( ) . accept ( this , keystate ) ;
private thriftschemaconvertvisitor ( fieldprojectionfilter fieldprojectionfilter , boolean doprojection ) {
return true ;
throw new thriftprojectionexception (
import org . apache . parquet . thrift . struct . thrifttype . structtype ;
thriftfield valuefield = maptype . getvalue ( ) ;
public convertedfield visit ( doubletype doubletype , state state ) {
this . repetition = repetition ;
new thriftschemaconvertvisitor ( new keeponlyfirstprimitivefilter ( ) , true ) , childstate ) ;
hassentinelunioncolumns = true ;
convertedfield converted = listlike . gettype ( ) . accept ( this , childstate ) ;
hasnonsentinelunioncolumns = true ;
if ( !converted . askeep ( ) . gettype ( ) . equals ( fullconv . askeep ( ) . gettype ( ) ) ) {
throw new shouldneverhappenexception ( "unrecognized type : " + s ) ;
public final type . repetition repetition ;
state . name ,
private static boolean isunion ( structoruniontype s ) {
return visitprimitivetype ( binary , utf8 , state ) ;
"for path " + state . path ) ;
convertedkey . askeep ( ) . gettype ( ) ,
if ( doprojection ) {
import org . apache . parquet . thrift . convertedfield . drop ;
"cannot select only the values of a map , you must keep the keys as well : " + state . path ) ;
import org . apache . parquet . shouldneverhappenexception ;
import static org . apache . parquet . schema . primitivetype . primitivetypename . boolean ;
import static org . apache . parquet . schema . conversionpatterns . listtype ;
import org . apache . parquet . thrift . convertedfield . sentinelunion ;
import static org . apache . parquet . schema . originaltype . enum ;
return new keep ( state . path , b . named ( state . name ) ) ;
return visitlistlike ( settype . getvalues ( ) , state , true ) ;
throw new thriftprojectionexception ( "no columns have been selected" ) ;
case union :
if ( !converted . iskeep ( ) ) {
import org . apache . parquet . thrift . struct . thrifttype . i16type ;
state keystate = new state ( state . path . push ( keyfield ) , required , "key" ) ;
import org . apache . parquet . schema . type . repetition ;
case struct :
public convertedfield visit ( maptype maptype , state state ) {
if ( orig ! = null ) {
public convertedfield visit ( bytetype bytetype , state state ) {
public static final class state {
state valuestate = new state ( state . path . push ( valuefield ) , optional , "value" ) ;
import static org . apache . parquet . schema . type . repetition . required ;
. accept ( new thriftschemaconvertvisitor ( fieldprojectionfilter . all columns , false ) , keystate ) ;
if ( !convertedkey . iskeep ( ) ) {
import static org . apache . parquet . schema . primitivetype . primitivetypename . int64 ;
public convertedfield visit ( i16type i16type , state state ) {
convertedchildren . add ( converted . askeep ( ) . gettype ( ) . withid ( child . getfieldid ( ) ) ) ;
private type . repetition getrepetition ( thriftfield thriftfield ) {
public convertedfield visit ( stringtype stringtype , state state ) {
state . repetition ,
for ( field field : fields ) {
} else if ( converted . iskeep ( ) ) {
if ( converted . iskeep ( ) ) {
state childstate = new state ( state . path , repeated , state . name + " tuple" ) ;
import static org . apache . parquet . schema . conversionpatterns . maptype ;
import static org . apache . parquet . schema . primitivetype . primitivetypename . double ;
return new keep ( state . path , mapfield ) ;
import static org . apache . parquet . schema . originaltype . utf8 ;
import org . apache . parquet . thrift . struct . thrifttype . structtype . structoruniontype ;
@ override
import static org . apache . parquet . preconditions . checknotnull ;
if ( converted . issentinelunion ( ) ) {
if ( !hasnonsentinelunioncolumns & & hassentinelunioncolumns ) {
case unknown :
return visitprimitivetype ( boolean , state ) ;
convertedfield converted = child . gettype ( ) . accept ( this , childstate ) ;
. gettype ( )
return visitprimitivetype ( binary , enum , state ) ;
messagetype messagetype = thriftschemaconvertvisitor . convert ( struct , fieldprojectionfilter ) ;
throw new thriftprojectionexception ( "cannot select only a subset of the fields in a set , " +
return visitprimitivetype ( double , state ) ;
import org . apache . parquet . thrift . struct . thrifttype . booltype ;
public convertedfield visit ( settype settype , state state ) {
default :
private convertedfield visitprimitivetype ( primitivetypename type , state state ) {
return new keep ( state . path , listtype ( state . repetition , state . name , converted . askeep ( ) . gettype ( ) ) ) ;
import org . apache . parquet . thrift . struct . thrifttype . enumtype ;
convertedchildren . add ( converted . assentinelunion ( ) . gettype ( ) . withid ( child . getfieldid ( ) ) ) ;
return visitprimitivetype ( type , null , state ) ;
for ( thriftfield child : structtype . getchildren ( ) ) {
convertedfield convertedvalue = valuefield . gettype ( ) . accept ( this , valuestate ) ;
convertedfield sentinelvalue =
return new messagetype ( state . name , converted . askeep ( ) . gettype ( ) . asgrouptype ( ) . getfields ( ) ) ;
if ( convertedvalue . iskeep ( ) ) {
convertedchildren . add ( firstprimitive . askeep ( ) . gettype ( ) . withid ( child . getfieldid ( ) ) ) ;
state state = new state ( new fieldspath ( ) , repeated , "parquetschema" ) ;
this . path = path ;
boolean hasnonsentinelunioncolumns = false ;
convertedfield converted = struct . accept ( new thriftschemaconvertvisitor ( filter , true ) , state ) ;
public convertedfield visit ( i32type i32type , state state ) {
public convertedfield visit ( enumtype enumtype , state state ) {
switch ( s ) {
public class thriftschemaconvertvisitor implements thrifttype . typevisitor < convertedfield , thriftschemaconvertvisitor . state > {
import static org . apache . parquet . schema . primitivetype . primitivetypename . binary ;
convertedvalue . askeep ( ) . gettype ( ) ) ;
convertedfield firstprimitive = child . gettype ( ) . accept (
extends builder < this , p > {
. repetition ( type . repetition . repeated ) ;
public elementbuilder < p , this > requiredelement ( primitivetypename type ) {
public groupvaluebuilder < p , this > optionalgroupvalue ( ) {
super ( parent , type ) ;
public listelementbuilder < p , this > listelement ( type . repetition repetition ) {
return listvalue ( type . repetition . optional ) ;
. repeatedgroup ( ) . addfields ( elementtype ) . named ( "list" )
public groupbuilder < this > repeatedgroup ( ) {
public this value ( type type ) {
return buildgroup ( repetition ) . as ( originaltype . map )
public keybuilder < p , this > key ( primitivetypename type ) {
return this ;
private type elementtype = null ;
return new listvaluebuilder < mp , m > ( mapbuilder ) . repetition ( repetition ) ;
public basemapbuilder ( p parent ) {
"list is already the logical type and can't be changed" ) ;
return new mapbuilder < this > ( self ( ) )
protected listvaluebuilder < mp , m > self ( ) {
private baseprimitivebuilder ( p parent , primitivetypename type ) {
super ( ( ( baselistbuilder < lp , l > ) listbuilder ) . parent ) ;
}
return map ( type . repetition . required ) ;
setvaluetype ( type ) ;
" [ bug ] parent and return type are null : must override named" ) ;
public listbuilder ( class < p > returntype ) {
protected groupkeybuilder < mp , m > self ( ) {
return map ( type . repetition . optional ) ;
return buildgroup ( repetition ) . as ( originaltype . list )
return mapbuilder . named ( name ) ;
return element ( type , type . repetition . required ) ;
public listvaluebuilder < p , this > listvalue ( type . repetition repetition ) {
if ( keytype = = null ) {
private baseprimitivebuilder ( class < p > returntype , primitivetypename type ) {
return new primitivebuilder < this > ( self ( ) , type )
return new valuebuilder < p , this > ( self ( ) , type ) . repetition ( repetition ) ;
protected final this repetition ( type . repetition repetition ) {
mapbuilder . setvaluetype ( type ) ;
} else {
return groupelement ( type . repetition . required ) ;
public listbuilder ( p parent ) {
public primitivebuilder < this > required (
public groupbuilder < this > requiredgroup ( ) {
public static listbuilder < grouptype > requiredlist ( ) {
return new listelementbuilder < p , this > ( self ( ) ) . repetition ( repetition ) ;
private primitivebuilder ( class < p > returntype , primitivetypename type ) {
import java . util . collections ;
public this addfields ( type . . . types ) {
public groupelementbuilder < p , this > groupelement ( type . repetition repetition ) {
this . elementtype = elementtype ;
super ( mapbuilder . parent , type ) ;
public listvaluebuilder ( m mapbuilder ) {
return new mapelementbuilder < p , this > ( self ( ) ) . repetition ( repetition ) ;
protected type build ( string name ) {
this . mapbuilder = mapbuilder ;
return value ( type , type . repetition . required ) ;
protected listelementbuilder < lp , l > self ( ) {
protected mapbuilder < p > self ( ) {
. repeatedgroup ( ) . addfields ( keytype , valuetype ) . named ( "map" )
this . listbuilder = listbuilder ;
public elementbuilder < p , this > optionalelement ( primitivetypename type ) {
preconditions . checknotnull ( elementtype , "list element type" ) ;
return listelement ( type . repetition . required ) ;
public groupvaluebuilder < p , this > groupvalue ( type . repetition repetition ) {
public abstract static class
return list ( type . repetition . optional ) ;
return value ( type , type . repetition . optional ) ;
public listbuilder < this > requiredlist ( ) {
extends baseprimitivebuilder < lp , elementbuilder < lp , l > > {
public mp named ( string name ) {
protected groupbuilder < p > self ( ) {
public abstract static class baselistbuilder < p , this extends baselistbuilder < p , this > >
public groupelementbuilder ( l listbuilder ) {
private final m mapbuilder ;
return groupvalue ( type . repetition . required ) ;
return new primitivebuilder < primitivetype > ( primitivetype . class , type )
public mapbuilder ( p parent ) {
public static class mapvaluebuilder < mp , m extends basemapbuilder < mp , m > >
public m value ( type type ) {
protected groupelementbuilder < lp , l > self ( ) {
return new groupvaluebuilder < mp , m > ( mapbuilder ) . repetition ( repetition ) ;
return new groupelementbuilder < p , this > ( self ( ) ) . repetition ( repetition ) ;
extends baselistbuilder < mp , listvaluebuilder < mp , m > > {
. repeatedgroup ( ) . addfields ( keytype ) . named ( "map" )
protected elementbuilder < lp , l > self ( ) {
keytype = string key ;
public mapelementbuilder ( l listbuilder ) {
private basegroupbuilder ( p parent ) {
return mapvalue ( type . repetition . required ) ;
public listvaluebuilder < mp , m > optionallistvalue ( ) {
public groupvaluebuilder < mp , m > groupvalue ( type . repetition repetition ) {
extends basegroupbuilder < mp , groupkeybuilder < mp , m > > {
private basegroupbuilder ( class < p > returntype ) {
protected valuebuilder < mp , m > self ( ) {
return new groupkeybuilder < p , this > ( self ( ) ) ;
return new elementbuilder < p , this > ( self ( ) , type ) . repetition ( repetition ) ;
private final baselistbuilder < lp , l > listbuilder ;
return listelement ( type . repetition . optional ) ;
public static primitivebuilder < primitivetype > primitive ( primitivetypename type ,
public groupvaluebuilder < p , this > requiredgroupvalue ( ) {
mapbuilder . setkeytype ( build ( "key" ) ) ;
extends basemapbuilder < mp , mapvaluebuilder < mp , m > > {
private basemapbuilder ( class < p > returntype ) {
"map is already a logical type and can't be changed . " ) ;
extends basegroupbuilder < mp , groupvaluebuilder < mp , m > > {
"only one key type can be built with a valuebuilder" ) ;
public this addfield ( type type ) {
public listvaluebuilder < p , this > optionallistvalue ( ) {
public groupkeybuilder < p , this > groupkey ( ) {
public void setelementtype ( type elementtype ) {
super ( returntype ) ;
super ( mapbuilder . parent ) ;
"only one element can be built with a listbuilder" ) ;
public static listbuilder < grouptype > optionallist ( ) {
preconditions . checkstate ( this . valuetype = = null ,
public baseprimitivebuilder < p , this > length ( int length ) {
private final l listbuilder ;
super ( returntype , type ) ;
return new mapbuilder < grouptype > ( grouptype . class ) . repetition ( repetition ) ;
extends basemapbuilder < lp , mapelementbuilder < lp , l > > {
public listelementbuilder ( l listbuilder ) {
setkeytype ( type ) ;
public groupvaluebuilder < mp , m > requiredgroupvalue ( ) {
public keybuilder ( m mapbuilder , primitivetypename type ) {
return listbuilder . named ( name ) ;
baseprimitivebuilder < p , this extends baseprimitivebuilder < p , this > >
public valuebuilder < mp , m > value ( primitivetypename type ,
return listvalue ( type . repetition . required ) ;
. repetition ( repetition ) ;
protected mapelementbuilder < lp , l > self ( ) {
return groupelement ( type . repetition . optional ) ;
setelementtype ( type ) ;
public this as ( originaltype type ) {
this . keytype = keytype ;
public baseprimitivebuilder < p , this > scale ( int scale ) {
return list ( type . repetition . required ) ;
public mapelementbuilder < p , this > mapelement ( type . repetition repetition ) {
public static primitivebuilder < primitivetype > required ( primitivetypename type ) {
repetition ( type . repetition . required ) ;
public mapvaluebuilder < mp , m > optionalmapvalue ( ) {
public groupkeybuilder ( m mapbuilder ) {
public groupelementbuilder < p , this > optionalgroupelement ( ) {
public static class valuebuilder < mp , m extends basemapbuilder < mp , m > >
super ( ( ( baselistbuilder < lp , l > ) listbuilder ) . parent , type ) ;
preconditions . checkstate ( originaltype = = null ,
return new listvaluebuilder < p , this > ( self ( ) ) . repetition ( repetition ) ;
. required ( primitivetypename . binary ) . as ( originaltype . utf8 ) . named ( "key" ) ;
return new groupbuilder < this > ( self ( ) )
public abstract static class basemapbuilder < p , this extends basemapbuilder < p , this > >
public valuebuilder ( m mapbuilder , primitivetypename type ) {
return this . mapbuilder ;
public mapbuilder < this > requiredmap ( ) {
baseprimitivebuilder < mp , keybuilder < mp , m > > {
public listelementbuilder < p , this > requiredlistelement ( ) {
public static class keybuilder < mp , m extends basemapbuilder < mp , m > > extends
public static class groupbuilder < p > extends basegroupbuilder < p , groupbuilder < p > > {
public groupbuilder < this > group ( type . repetition repetition ) {
private mapbuilder ( class < p > returntype ) {
public static primitivebuilder < primitivetype > optional ( primitivetypename type ) {
public static class listvaluebuilder < mp , m extends basemapbuilder < mp , m > >
protected keybuilder < mp , m > self ( ) {
return new keybuilder < p , this > ( self ( ) , type ) ;
return new mapvaluebuilder < p , this > ( self ( ) ) . repetition ( repetition ) ;
public static class mapelementbuilder < lp , l extends baselistbuilder < lp , l > >
return groupvalue ( type . repetition . optional ) ;
basegroupbuilder . class . cast ( parent ) . addfield ( type ) ;
protected mapvaluebuilder < mp , m > self ( ) {
if ( valuetype ! = null ) {
public static primitivebuilder < primitivetype > repeated ( primitivetypename type ) {
private p parent ;
public static class mapbuilder < p > extends basemapbuilder < p , mapbuilder < p > > {
public static listbuilder < grouptype > list ( type . repetition repetition ) {
return new groupvaluebuilder < p , this > ( self ( ) ) . repetition ( repetition ) ;
public abstract static class builder < this extends builder , p > {
public valuebuilder < mp , m > optionalvalue ( primitivetypename type ) {
private primitivebuilder ( p parent , primitivetypename type ) {
public groupelementbuilder < p , this > requiredgroupelement ( ) {
public this key ( type type ) {
throw new illegalstateexception (
public abstract static class basegroupbuilder < p , this extends basegroupbuilder < p , this > >
public static class primitivebuilder < p > extends baseprimitivebuilder < p , primitivebuilder < p > > {
@ override
public mapelementbuilder < p , this > optionalmapelement ( ) {
private static final type string key = types
. repetition ( type . repetition . required ) ;
public listvaluebuilder < p , this > requiredlistvalue ( ) {
return new mapbuilder < this > ( self ( ) ) . repetition ( repetition ) ;
public mapbuilder < this > map (
public mapvaluebuilder < mp , m > requiredmapvalue ( ) {
. repetition ( type . repetition . optional ) ;
protected groupvaluebuilder < mp , m > self ( ) {
return new mapvaluebuilder < mp , m > ( mapbuilder ) . repetition ( repetition ) ;
this . parent = parent ;
public primitivebuilder < this > primitive (
public mapbuilder < this > optionalmap ( ) {
public valuebuilder < p , this > value ( primitivetypename type ,
public listbuilder < this > optionallist ( ) {
private type keytype = null ;
public mapvaluebuilder < mp , m > mapvalue ( type . repetition repetition ) {
extends baselistbuilder < lp , listelementbuilder < lp , l > > {
public static class groupkeybuilder < mp , m extends basemapbuilder < mp , m > >
public baselistbuilder ( p parent ) {
public mapvaluebuilder < p , this > optionalmapvalue ( ) {
return new listbuilder < this > ( self ( ) ) . repetition ( repetition ) ;
public primitivebuilder < this > repeated (
protected void setvaluetype ( type valuetype ) {
private groupbuilder ( class < p > returntype ) {
preconditions . checkstate ( this . elementtype = = null ,
public baselistbuilder ( class < p > returntype ) {
public groupvaluebuilder < mp , m > optionalgroupvalue ( ) {
public mapelementbuilder < p , this > requiredmapelement ( ) {
public mapvaluebuilder < p , this > mapvalue ( type . repetition repetition ) {
protected listbuilder < p > self ( ) {
public listelementbuilder < p , this > optionallistelement ( ) {
public static class listbuilder < p > extends baselistbuilder < p , listbuilder < p > > {
public static class listelementbuilder < lp , l extends baselistbuilder < lp , l > >
public listbuilder < this > list ( type . repetition repetition ) {
return element ( type , type . repetition . optional ) ;
public mapvaluebuilder < p , this > requiredmapvalue ( ) {
return new listbuilder < grouptype > ( grouptype . class ) . repetition ( repetition ) ;
public static class elementbuilder < lp , l extends baselistbuilder < lp , l > >
public static mapbuilder < grouptype > map ( type . repetition repetition ) {
return mapvalue ( type . repetition . optional ) ;
"only one key type can be built with a mapbuilder" ) ;
} else if ( returnclass ! = null ) {
return mapelement ( type . repetition . optional ) ;
extends baseprimitivebuilder < mp , valuebuilder < mp , m > > {
public static mapbuilder < grouptype > requiredmap ( ) {
public lp named ( string name ) {
return mapelement ( type . repetition . required ) ;
. named ( name ) ;
preconditions . checkstate ( this . keytype = = null ,
public elementbuilder ( l listbuilder , primitivetypename type ) {
public static class groupvaluebuilder < mp , m extends basemapbuilder < mp , m > >
protected abstract this self ( ) ;
public groupvaluebuilder ( m mapbuilder ) {
protected primitivebuilder < p > self ( ) {
public valuebuilder < p , this > requiredvalue ( primitivetypename type ) {
public mapvaluebuilder ( m mapbuilder ) {
this . valuetype = valuetype ;
public static mapbuilder < grouptype > optionalmap ( ) {
private type valuetype = null ;
extends basegroupbuilder < lp , groupelementbuilder < lp , l > > {
public valuebuilder < mp , m > requiredvalue ( primitivetypename type ) {
public primitivebuilder < this > optional (
if ( basegroupbuilder . class . isassignablefrom ( parent . getclass ( ) ) ) {
super ( parent ) ;
public listvaluebuilder < mp , m > listvalue ( type . repetition repetition ) {
public this id ( int id ) {
public baselistbuilder < p , this > element ( type type ) {
type . repetition repetition ) {
private groupbuilder ( p parent ) {
public groupbuilder < this > optionalgroup ( ) {
public valuebuilder < p , this > optionalvalue ( primitivetypename type ) {
public static class groupelementbuilder < lp , l extends baselistbuilder < lp , l > >
return self ( ) ;
collections . addall ( fields , types ) ;
public listvaluebuilder < mp , m > requiredlistvalue ( ) {
listbuilder . setelementtype ( build ( "element" ) ) ;
mapbuilder . setvaluetype ( build ( "value" ) ) ;
protected void setkeytype ( type keytype ) {
public baseprimitivebuilder < p , this > precision ( int precision ) {
public elementbuilder < p , this > element ( primitivetypename type ,
return new valuebuilder < mp , m > ( mapbuilder , type ) . repetition ( repetition ) ;
preconditions . checkargument ( elementclass = = long . class ,
case long :
for ( boolean element : array ) {
recordconsumer . adddouble ( element ) ;
recordconsumer . addboolean ( element ) ;
for ( object elt : array ) {
for ( int element : array ) {
}
"null list element for " + schema . getname ( ) ) ;
throw new illegalargumentexception (
private class twolevellistwriter extends listwriter {
static final boolean write old list structure default = true ;
for ( char element : array ) {
} else if ( elementclass = = int . class ) {
} else {
recordconsumer . startfield ( map repeated name , 0 ) ;
protected void writebytearray ( byte [ ] array ) {
recordconsumer . endfield ( list element name , 0 ) ;
"cannot write unless collection or array : " + arrayclass . getname ( ) ) ;
protected abstract void endarray ( ) ;
protected void startarray ( ) {
protected void writelongarray ( long [ ] array ) {
throw new runtimeexception (
preconditions . checkargument ( elementclass = = float . class ,
preconditions . checkargument ( arrayclass . isarray ( ) ,
for ( object element : array ) {
switch ( avroschema . getelementtype ( ) . gettype ( ) ) {
recordconsumer . startfield ( old list repeated name , 0 ) ;
preconditions . checkargument ( elementclass = = double . class ,
protected void writeobjectarray ( grouptype type , schema schema ,
recordconsumer . startfield ( map value name , 1 ) ;
for ( float element : array ) {
preconditions . checkargument ( elementclass = = boolean . class ,
if ( !elementclass . isprimitive ( ) ) {
boolean writeoldliststructure = configuration . getboolean (
} else if ( !elementtype . isrepetition ( type . repetition . optional ) ) {
recordconsumer . startgroup ( ) ;
if ( elementclass = = byte . class ) {
public void writelist ( grouptype schema , schema avroschema , object value ) {
private abstract class listwriter {
writedoublearray ( ( double [ ] ) value ) ;
recordconsumer . endfield ( map key name , 0 ) ;
writechararray ( ( char [ ] ) value ) ;
recordconsumer . addinteger ( element ) ;
listwriter . writelist ( type . asgrouptype ( ) , nonnullavroschema , value ) ;
public void writecollection ( grouptype schema , schema avroschema ,
protected void writeshortarray ( short [ ] array ) {
"cannot write as double array : " + arrayclass . getname ( ) ) ;
if ( writeoldliststructure ) {
writeshortarray ( ( short [ ] ) value ) ;
writebooleanarray ( ( boolean [ ] ) value ) ;
for ( short element : array ) {
protected abstract void writecollection (
recordconsumer . startfield ( list repeated name , 0 ) ;
if ( collection . size ( ) > 0 ) {
protected void writeintarray ( int [ ] array ) {
if ( element ! = null ) {
for ( object element : collection ) {
protected void writecollection ( grouptype type , schema schema , collection < ? > collection ) {
if ( array . size ( ) > 0 ) {
writefloatarray ( ( float [ ] ) value ) ;
recordconsumer . addlong ( element ) ;
protected abstract void writeobjectarray (
throw new illegalargumentexception ( "cannot write " +
writeintarray ( ( int [ ] ) value ) ;
writejavaarray ( schema , avroschema , arrayclass , value ) ;
break ;
private listwriter listwriter ;
private static final string list repeated name = "list" ;
case boolean :
public void writejavaarray ( grouptype schema , schema avroschema ,
grouptype repeatedtype = type . gettype ( 0 ) . asgrouptype ( ) ;
writebytearray ( ( byte [ ] ) value ) ;
startarray ( ) ;
private class threelevellistwriter extends listwriter {
protected void writebooleanarray ( boolean [ ] array ) {
"cannot write as float array : " + arrayclass . getname ( ) ) ;
case int :
case double :
writecollection ( schema , avroschema , ( collection ) value ) ;
} else if ( elementclass = = short . class ) {
recordconsumer . endfield ( map value name , 1 ) ;
endarray ( ) ;
for ( byte element : array ) {
public static final string write old list structure =
this . listwriter = new twolevellistwriter ( ) ;
class < ? > arrayclass , object value ) {
class < ? > arrayclass = value . getclass ( ) ;
"cannot write as an int array : " + arrayclass . getname ( ) ) ;
"cannot write as boolean array : " + arrayclass . getname ( ) ) ;
grouptype type , schema schema , collection < ? > collection ) ;
writeobjectarray ( schema , avroschema , ( object [ ] ) value ) ;
recordconsumer . endfield ( list repeated name , 0 ) ;
"parquet . avro . write - old - list - structure" ;
protected void writefloatarray ( float [ ] array ) {
recordconsumer . endfield ( map repeated name , 0 ) ;
recordconsumer . addfloat ( element ) ;
@ override
recordconsumer . endgroup ( ) ;
protected void writechararray ( char [ ] array ) {
private static final string old list repeated name = "array" ;
private static final string map repeated name = "key value" ;
} else if ( elementclass = = char . class ) {
writevalue ( elementtype , schema . getelementtype ( ) , element ) ;
protected abstract void startarray ( ) ;
recordconsumer . startfield ( list element name , 0 ) ;
recordconsumer . endfield ( old list repeated name , 0 ) ;
writevalue ( schema . gettype ( 0 ) , avroschema . getelementtype ( ) , elt ) ;
avroschema . getelementtype ( ) + " array : " + arrayclass . getname ( ) ) ;
write old list structure , write old list structure default ) ;
if ( value instanceof collection ) {
writevalue ( type . gettype ( 0 ) , schema . getelementtype ( ) , element ) ;
"cannot write as long array : " + arrayclass . getname ( ) ) ;
protected void endarray ( ) {
type elementtype = repeatedtype . gettype ( 0 ) ;
recordconsumer . startfield ( map key name , 0 ) ;
default :
this . listwriter = new threelevellistwriter ( ) ;
private static final string map value name = "value" ;
case float :
protected void writedoublearray ( double [ ] array ) {
object [ ] array ) {
grouptype type , schema schema , object [ ] array ) ;
static final string list element name = "element" ;
return ;
for ( double element : array ) {
private static final string map key name = "key" ;
writelongarray ( ( long [ ] ) value ) ;
collection < ? > array ) {
class < ? > elementclass = arrayclass . getcomponenttype ( ) ;
for ( long element : array ) {
if ( array . length > 0 ) {
import java . io . file ;
public void run ( ) throws ioexception {
add ( props . getproperty ( "fullversion" ) ) ;
add ( " \ " ; \ n \ n" ) ;
" / * * \ n" +
public static void main ( string [ ] args ) throws ioexception {
writer . close ( ) ;
add ( " system . out . println ( full version ) ; \ n" ) ;
file srcfile = new file ( args [ 0 ] + " / org / apache / parquet / version . java" ) ;
throw new ioexception ( " / parquet - version . properties not found" ) ;
props . load ( in ) ;
if ( !parent . exists ( ) ) {
file parent = srcfile . getparentfile ( ) ;
add ( "public class version { \ n" ) ;
add ( "package org . apache . parquet ; \ n" +
add ( " } \ n" ) ;
}
" * do not manually edit! \ n" +
this . writer = new filewriter ( file ) ;
" \ n" +
properties props = new properties ( ) ;
add ( " public static final string version number = \ "" ) ;
private final filewriter writer ;
add ( " public static final string full version = \ "" ) ;
import java . io . ioexception ;
" * this class is auto - generated by { @ link org . apache . parquet . version . versiongenerator } \ n" +
add ( " \ " ; \ n" ) ;
add ( " public static void main ( string [ ] args ) { \ n" ) ;
private void add ( string s ) throws ioexception {
srcfile = srcfile . getabsolutefile ( ) ;
add ( props . getproperty ( "versionnumber" ) ) ;
import java . io . filewriter ;
writer . write ( s ) ;
import java . util . properties ;
package org . apache . parquet . version ;
inputstream in = versiongenerator . class . getresourceasstream ( " / parquet - version . properties" ) ;
public versiongenerator ( file file ) throws ioexception {
import java . io . inputstream ;
public class versiongenerator {
if ( !parent . mkdirs ( ) ) {
throw new ioexception ( "couldn't mkdirs for " + parent ) ;
" * / \ n" ) ;
if ( in = = null ) {
new versiongenerator ( srcfile ) . run ( ) ;
public self enabledictionaryencoding ( ) {
dictionarypagesize , enabledictionary , validating , writerversion , conf ,
this . maxpaddingsize = maxpaddingsize ;
max padding size default ) ;
private final path file ;
private parquetfilewriter . mode mode ;
}
private configuration conf = new configuration ( ) ;
this . enablevalidation = enablevalidation ;
public self withcompressioncodec ( compressioncodecname codecname ) {
filewriter . start ( ) ;
this . writerversion = version ;
private int pagesize = default page size ;
schema ,
parquetfilewriter filewriter = new parquetfilewriter (
boolean enabledictionary ,
public self withmaxpaddingsize ( int maxpaddingsize ) {
validating ,
enabledictionary ,
public self withpagesize ( int pagesize ) {
this . rowgroupsize = rowgroupsize ;
pagesize ,
this . enablevalidation = true ;
parquetfilewriter . mode mode ,
int pagesize ,
parquetwriter (
codecfactory codecfactory = new codecfactory ( conf ) ;
int blocksize ,
protected abstract self self ( ) ;
public self withconf ( configuration conf ) {
@ deprecated
protected builder ( path file ) {
blocksize ,
messagetype schema = writecontext . getschema ( ) ;
path file ,
this . codecname = codecname ;
private writerversion writerversion = default writer version ;
this . enabledictionary = true ;
private int maxpaddingsize = max padding size default ;
this . dictionarypagesize = dictionarypagesize ;
this . mode = mode ;
conf , schema , file , mode , blocksize , maxpaddingsize ) ;
int dictionarypagesize ,
rowgroupsize , pagesize , dictionarypagesize , enabledictionary ,
int maxpaddingsize ) throws ioexception {
this . writer = new internalparquetrecordwriter < t > (
writerversion writerversion ,
private compressioncodecname codecname = default compression codec name ;
public self withdictionaryencoding ( boolean enabledictionary ) {
enablevalidation , writerversion , conf , maxpaddingsize ) ;
writerversion ) ;
private boolean enablevalidation = default is validating enabled ;
codecfactory . bytescompressor compressor = codecfactory . getcompressor ( compressioncodecname , 0 ) ;
private boolean enabledictionary = default is dictionary enabled ;
this . file = file ;
public parquetwriter < t > build ( ) throws ioexception {
private int dictionarypagesize = default page size ;
public self withrowgroupsize ( int rowgroupsize ) {
writesupport . writecontext writecontext = writesupport . init ( conf ) ;
this . enabledictionary = enabledictionary ;
return new parquetwriter < t > ( file , mode , getwritesupport ( conf ) , codecname ,
boolean validating ,
this ( file , mode , writesupport , compressioncodecname , blocksize , pagesize ,
compressor ,
public self withwriterversion ( writerversion version ) {
this . conf = conf ;
public self withwritemode ( parquetfilewriter . mode mode ) {
public abstract static class builder < t , self extends builder < t , self > > {
writesupport < t > writesupport ,
public self enablevalidation ( ) {
public self withvalidation ( boolean enablevalidation ) {
filewriter ,
this . pagesize = pagesize ;
writesupport ,
compressioncodecname compressioncodecname ,
private int rowgroupsize = default block size ;
public self withdictionarypagesize ( int dictionarypagesize ) {
configuration conf ,
return self ( ) ;
protected abstract writesupport < t > getwritesupport ( configuration conf ) ;
dictionarypagesize ,
writecontext . getextrametadata ( ) ,
warnparseerror ( createdby , e ) ;
preconditions . checkargument ( major > = 0 , "major must be > = 0" ) ;
public boolean equals ( object o ) {
string semver = matcher . group ( 3 ) ;
string appbuildhash = matcher . group ( 4 ) ;
return false ;
super ( message , cause ) ;
final int minor ;
public final int major ;
final int patch ;
}
preconditions . checkargument ( patch > = 0 , "patch must be > = 0" ) ;
log . warn ( "ignoring statistics because created by could not be parsed ( see parquet - 251 ) : " + createdby , e ) ;
throw new semanticversionparseexception (
import java . util . regex . matcher ;
import org . apache . parquet . schema . primitivetype . primitivetypename ;
cmp = integer . compare ( major , o . major ) ;
import org . apache . parquet . semanticversion . semanticversionparseexception ;
public semanticversionparseexception ( string message ) {
this . application = application ;
throw new semanticversionparseexception ( e ) ;
log . info ( "ignoring statistics because this file was created prior to "
if ( strings . isnullorempty ( createdby ) ) {
public static final string format = " ( . + ) version ( ( . * ) ) ? \ \ ( build ? ( . * ) \ \ ) " ;
string . format ( "major ( % d ) , minor ( % d ) , and patch ( % d ) must all be > = 0" , major , minor , patch ) ) ;
this . appbuildhash = strings . isnullorempty ( appbuildhash ) ? null : appbuildhash ;
this . semver = strings . isnullorempty ( semver ) ? null : semver ;
public int compareto ( semanticversion o ) {
parsedversion version = versionparser . parse ( createdby ) ;
throw new versionparseexception ( "could not parse created by : " + createdby + " using format : " + format ) ;
final int major ;
try {
throw new semanticversionparseexception ( "" + version + " does not match format " + format ) ;
matcher matcher = pattern . matcher ( version ) ;
minor = integer . valueof ( matcher . group ( 2 ) ) ;
if ( semver . compareto ( parquet 251 fixed version ) < 0 ) {
private static final string format = " ^ ( \ \ d + ) \ \ . ( \ \ d + ) \ \ . ( \ \ d + ) ( ( . * ) ( \ \ d + ) ) ? ( \ \ - ( . * ) ) ? $" ;
if ( columntype ! = primitivetypename . binary & & columntype ! = primitivetypename . fixed len byte array ) {
} catch ( runtimeexception e ) {
if ( strings . isnullorempty ( version . semver ) ) {
+ parquet 251 fixed version
checkargument ( !strings . isnullorempty ( application ) , "application cannot be null or empty" ) ;
if ( !"parquet - mr" . equals ( version . application ) ) {
"application = " + application +
return true ;
public static final pattern pattern = pattern . compile ( format ) ;
if ( appbuildhash ! = null ? !appbuildhash . equals ( version . appbuildhash ) : version . appbuildhash ! = null )
private static void warnparseerror ( string createdby , throwable e ) {
public static class semanticversionparseexception extends exception {
if ( cmp ! = 0 ) {
public final int patch ;
import org . apache . parquet . versionparser . versionparseexception ;
return new semanticversion ( major , minor , patch ) ;
return integer . compare ( patch , o . patch ) ;
string application = matcher . group ( 1 ) ;
public static semanticversion parse ( string version ) throws semanticversionparseexception {
if ( major < 0 | | minor < 0 | | patch < 0 ) {
if ( !matcher . matches ( ) ) {
public semanticversionparseexception ( string message , throwable cause ) {
public final class semanticversion implements comparable < semanticversion > {
public parsedversion ( string application , string semver , string appbuildhash ) {
super ( ) ;
if ( semver ! = null ? !semver . equals ( version . semver ) : version . semver ! = null ) return false ;
private static final semanticversion parquet 251 fixed version = new semanticversion ( 1 , 8 , 0 ) ;
if ( o = = null | | getclass ( ) ! = o . getclass ( ) ) return false ;
private static final pattern pattern = pattern . compile ( format ) ;
cmp = integer . compare ( minor , o . minor ) ;
int cmp ;
" , appbuildhash = " + appbuildhash +
public final int minor ;
public string tostring ( ) {
log . warn ( "ignoring statistics because created by did not contain a semver ( see parquet - 251 ) : " + createdby ) ;
int result = application ! = null ? application . hashcode ( ) : 0 ;
result = 31 * result + minor ;
throw new versionparseexception ( "application cannot be null or empty" ) ;
return "parsedversion ( " +
+ " , see parquet - 251" ) ;
patch = integer . valueof ( matcher . group ( 3 ) ) ;
semanticversion that = ( semanticversion ) o ;
preconditions . checkargument ( minor > = 0 , "minor must be > = 0" ) ;
} catch ( versionparseexception e ) {
@ override
package org . apache . parquet ;
import static org . apache . parquet . preconditions . checkargument ;
int result = major ;
public static parsedversion parse ( string createdby ) throws versionparseexception {
" , semver = " + semver +
result = 31 * result + ( appbuildhash ! = null ? appbuildhash . hashcode ( ) : 0 ) ;
public static class versionparseexception extends exception {
' ) ' ;
return major + " . " + minor + " . " + patch ;
return new parsedversion ( application , semver , appbuildhash ) ;
import org . apache . parquet . versionparser . parsedversion ;
public static boolean shouldignorestatistics ( string createdby , primitivetypename columntype ) {
return result ;
} catch ( numberformatexception e ) {
super ( message ) ;
public semanticversion ( int major , int minor , int patch ) {
parsedversion version = ( parsedversion ) o ;
log . info ( "ignoring statistics because created by is null or empty! see parquet - 251 and parquet - 297" ) ;
public static class parsedversion {
major = integer . valueof ( matcher . group ( 1 ) ) ;
this . minor = minor ;
return compareto ( that ) = = 0 ;
public versionparseexception ( string message ) {
public class versionparser {
private static final log log = log . getlog ( corruptstatistics . class ) ;
if ( this = = o ) return true ;
} catch ( semanticversionparseexception e ) {
this . major = major ;
if ( application ! = null ? !application . equals ( version . application ) : version . application ! = null ) return false ;
this . patch = patch ;
public semanticversionparseexception ( throwable cause ) {
public int hashcode ( ) {
public class corruptstatistics {
public final string appbuildhash ;
public semanticversionparseexception ( ) {
result = 31 * result + ( semver ! = null ? semver . hashcode ( ) : 0 ) ;
semanticversion semver = semanticversion . parse ( version . semver ) ;
super ( cause ) ;
matcher matcher = pattern . matcher ( createdby ) ;
result = 31 * result + patch ;
if ( strings . isnullorempty ( application ) ) {
import java . util . regex . pattern ;
public final string semver ;
return cmp ;
public final string application ;
return new avroreadsupport < t > ( model ) ;
this . enablecompatibility = false ;
public builder < t > disablecompatibility ( ) {
public builder < t > withcompatibility ( boolean enablecompatibility ) {
return this ;
private builder ( path path ) {
import org . apache . avro . specific . specificdata ;
if ( isreflect ) {
conf . setboolean ( avroreadsupport . avro compatibility , false ) ;
}
private boolean enablecompatibility = true ;
isreflect = true ;
if ( model . getclass ( ) ! = genericdata . class & &
import org . apache . avro . generic . genericdata ;
super ( path ) ;
conf . setboolean ( avroreadsupport . avro compatibility , enablecompatibility ) ;
public static class builder < t > extends parquetreader . builder {
} else {
return new builder < t > ( file ) ;
private boolean isreflect = true ;
protected readsupport < t > getreadsupport ( ) {
public builder < t > withdatamodel ( genericdata model ) {
this . model = model ;
import org . apache . parquet . hadoop . api . readsupport ;
@ override
this . enablecompatibility = enablecompatibility ;
private genericdata model = null ;
model . getclass ( ) ! = specificdata . class ) {
preconditions . checkargument ( major > = 0 , "major must be > = 0" ) ;
cmp = integer . compare ( patch , o . patch ) ;
import org . apache . parquet . column . encoding ;
if ( conf . getboolean ( parquetinputformat . split files , true ) ) {
public final boolean prerelease ;
this . minor = minor ;
}
preconditions . checkargument ( patch > = 0 , "patch must be > = 0" ) ;
import org . apache . parquet . corruptdeltabytearrays ;
import static org . apache . parquet . hadoop . parquetinputformat . split files ;
prerelease | = ( matcher . group ( g ) ! = null ) ;
return new semanticversion ( major , minor , patch , prerelease ) ;
set < encoding > encodings = new hashset < encoding > ( ) ;
public semanticversion ( int major , int minor , int patch , boolean isprerelease ) {
this . major = major ;
"parquet - 246 : to read safely , set " + split files + " to false" ) ;
this . patch = patch ;
for ( encoding encoding : encodings ) {
private void checkdeltabytearrayproblem ( filemetadata meta , configuration conf , blockmetadata block ) {
throw new parquetdecodingexception ( "cannot read data due to " +
if ( cmp ! = 0 ) {
preconditions . checkargument ( minor > = 0 , "minor must be > = 0" ) ;
import org . apache . parquet . hadoop . metadata . columnchunkmetadata ;
if ( corruptdeltabytearrays . requiressequentialreads ( meta . getcreatedby ( ) , encoding ) ) {
import org . apache . parquet . io . parquetdecodingexception ;
this . prerelease = isprerelease ;
boolean prerelease = false ;
encodings . addall ( column . getencodings ( ) ) ;
this . prerelease = false ;
import org . apache . parquet . hadoop . metadata . filemetadata ;
for ( columnchunkmetadata column : block . getcolumns ( ) ) {
checkdeltabytearrayproblem ( footer . getfilemetadata ( ) , configuration , filteredblocks . get ( 0 ) ) ;
return cmp ;
return boolean . compare ( o . prerelease , prerelease ) ;
for ( int g = 4 ; g < = matcher . groupcount ( ) ; g + = 1 ) {
}
cmp = compareintegers ( patch , o . patch ) ;
return ( x < y ) ? - 1 : ( ( x = = y ) ? 0 : 1 ) ;
cmp = compareintegers ( major , o . major ) ;
int comparebooleans ( boolean x , boolean y ) {
return comparebooleans ( o . prerelease , prerelease ) ;
int compareintegers ( int x , int y ) {
cmp = compareintegers ( minor , o . minor ) ;
return ( x = = y ) ? 0 : ( x ? 1 : - 1 ) ;
import org . apache . parquet . format . converter . parquetmetadataconverter ;
parquetmetadata metadata = parquetfilereader . readfooter ( conf , inpath , no filter ) ;
import static org . apache . parquet . format . converter . parquetmetadataconverter . no filter ;
}
string modelname = writesupport . getname ( ) ;
finalmetadata . put ( parquetwriter . object model name prop , modelname ) ;
if ( modelname ! = null ) {
}
string modelname = writesupport . getname ( ) ;
finalmetadata . put ( parquetwriter . object model name prop , modelname ) ;
if ( modelname ! = null ) {
if ( info ) log . info ( "parquet block size to " + blocksize ) ;
bytescompressor compressor ,
. estimaterowcountforpagesizecheck ( getestimatepagesizecheck ( conf ) )
. withpagesize ( pagesize )
if ( info ) log . info ( "min row count for page size check is : " + props . getminrowcountforpagesizecheck ( ) ) ;
return configuration . getint ( min row count for page size check ,
writesupport < t > writesupport ,
. build ( ) ;
memorymanager ) ;
. withdictionarypagesize ( getdictionarypagesize ( conf ) )
codecfactory . getcompressor ( codec , props . getpagesizethreshold ( ) ) ,
dictionary page size , parquetproperties . default dictionary page size ) ;
if ( info ) log . info ( "writer version is : " + props . getwriterversion ( ) ) ;
parquetproperties props ,
. withpagesize ( getpagesize ( conf ) )
parquetproperties . default estimate row count for page size check ) ;
}
. withminrowcountforpagesizecheck ( getminrowcountforpagesizecheck ( conf ) )
@ deprecated
. withwriterversion ( writerversion )
blocksize ,
return configuration . getboolean (
messagetype schema ,
parquetfilewriter w ,
import org . apache . parquet . column . parquetproperties ;
if ( info ) log . info ( "parquet page size to " + props . getpagesizethreshold ( ) ) ;
if ( info ) log . info ( "min row count for page size check is : " + props . getmaxrowcountforpagesizecheck ( ) ) ;
public static final string min row count for page size check = "parquet . page . size . row . check . min" ;
public static int getminrowcountforpagesizecheck ( configuration configuration ) {
if ( info ) log . info ( "dictionary is " + ( props . isenabledictionary ( ) ? "on" : "off" ) ) ;
return configuration . getint ( max row count for page size check ,
parquetrecordwriter (
if ( info ) log . info ( "page size checking is : " + ( props . estimatenextsizecheck ( ) ? "estimated" : "constant" ) ) ;
public static final string max row count for page size check = "parquet . page . size . row . check . max" ;
this . memorymanager = null ;
if ( info ) log . info ( "parquet dictionary page size to " + props . getdictionarypagesizethreshold ( ) ) ;
public static final string estimate page size check = "parquet . page . size . check . estimate" ;
this ( w , writesupport , schema , extrametadata , blocksize , compressor ,
codecfactory codecfactory = new codecfactory ( conf ) ;
. withdictionaryencoding ( enabledictionary )
return configuration . getboolean ( estimate page size check ,
writer version , parquetproperties . default writer version . tostring ( ) ) ;
. build ( ) ,
return configuration . getint (
int maxpaddingsize = getmaxpaddingsize ( conf ) ;
extrametadata , blocksize , compressor , validating , props ) ;
parquetproperties . default minimum record count for check ) ;
string writerversion = configuration . get (
. withwriterversion ( getwriterversion ( conf ) )
enable dictionary , parquetproperties . default is dictionary enabled ) ;
public static int getmaxrowcountforpagesizecheck ( configuration configuration ) {
props ,
boolean validating ,
map < string , string > extrametadata ,
. withdictionarypagesize ( dictionarypagesize )
public static boolean getestimatepagesizecheck ( configuration configuration ) {
. withmaxrowcountforpagesizecheck ( getmaxrowcountforpagesizecheck ( conf ) )
return configuration . getint ( page size , parquetproperties . default page size ) ;
memorymanager memorymanager ) {
validating , parquetproperties . builder ( )
parquetproperties props = parquetproperties . builder ( )
import org . apache . commons . math3 . analysis . function . add ;
. withdictionaryencoding ( getenabledictionary ( conf ) )
long blocksize ,
. estimaterowcountforpagesizecheck ( getestimatepagesizecheck ( conf ) )
rowgroupsize , enablevalidation , conf , maxpaddingsize ,
return configuration . getint ( min row count for page size check ,
encodingpropsbuilder . withdictionarypagesize ( dictionarypagesize ) ;
dictionary page size , parquetproperties . default dictionary page size ) ;
parquetproperties . default writer version ;
}
codecfactory codecfactory = new codecfactory ( conf , encodingprops . getpagesizethreshold ( ) ) ;
import org . apache . parquet . column . parquetproperties ;
if ( info ) log . info ( "parquet page size to " + props . getpagesizethreshold ( ) ) ;
public static final string min row count for page size check = "parquet . page . size . row . check . min" ;
public static int getminrowcountforpagesizecheck ( configuration configuration ) {
this . memorymanager = null ;
parquetproperties encodingprops ) throws ioexception {
. withdictionaryencoding ( enabledictionary )
encodingprops ) ;
int maxpaddingsize = getmaxpaddingsize ( conf ) ;
extrametadata , blocksize , compressor , validating , props ) ;
. withwriterversion ( getwriterversion ( conf ) )
enable dictionary , parquetproperties . default is dictionary enabled ) ;
map < string , string > extrametadata ,
int maxpaddingsize ,
memorymanager memorymanager ) {
. withdictionarypagesize ( getdictionarypagesize ( conf ) )
parquetproperties . default is dictionary enabled ;
@ deprecated
blocksize ,
return configuration . getboolean (
parquetrecordwriter (
return configuration . getint ( max row count for page size check ,
if ( info ) log . info ( "page size checking is : " + ( props . estimatenextsizecheck ( ) ? "estimated" : "constant" ) ) ;
if ( info ) log . info ( "parquet dictionary page size to " + props . getdictionarypagesizethreshold ( ) ) ;
private parquetproperties . builder encodingpropsbuilder =
return configuration . getboolean ( estimate page size check ,
public static int getmaxrowcountforpagesizecheck ( configuration configuration ) {
encodingpropsbuilder . withwriterversion ( version ) ;
codecfactory codecfactory = new codecfactory ( conf , props . getpagesizethreshold ( ) ) ;
public static final int default page size =
parquetproperties props = parquetproperties . builder ( )
. withdictionaryencoding ( getenabledictionary ( conf ) )
if ( info ) log . info ( "parquet block size to " + blocksize ) ;
validating , conf , max padding size default ,
if ( info ) log . info ( "min row count for page size check is : " + props . getminrowcountforpagesizecheck ( ) ) ;
memorymanager ) ;
. withpagesize ( getpagesize ( conf ) )
parquetproperties . default estimate row count for page size check ) ;
. withwriterversion ( writerversion )
parquetproperties . default page size ;
messagetype schema ,
encodingpropsbuilder . build ( ) ) ;
if ( info ) log . info ( "min row count for page size check is : " + props . getmaxrowcountforpagesizecheck ( ) ) ;
if ( info ) log . info ( "dictionary is " + ( props . isenabledictionary ( ) ? "on" : "off" ) ) ;
parquetproperties . builder ( )
public static final string max row count for page size check = "parquet . page . size . row . check . max" ;
. build ( ) ,
return configuration . getint (
string writerversion = configuration . get (
boolean validating ,
encodingpropsbuilder . withdictionaryencoding ( enabledictionary ) ;
encodingpropsbuilder . withdictionaryencoding ( true ) ;
public static boolean getestimatepagesizecheck ( configuration configuration ) {
. withmaxrowcountforpagesizecheck ( getmaxrowcountforpagesizecheck ( conf ) )
validating , parquetproperties . builder ( )
long blocksize ,
bytescompressor compressor ,
. withpagesize ( pagesize )
writesupport < t > writesupport ,
. build ( ) ;
if ( info ) log . info ( "writer version is : " + props . getwriterversion ( ) ) ;
parquetproperties props ,
. withminrowcountforpagesizecheck ( getminrowcountforpagesizecheck ( conf ) )
encodingpropsbuilder . withpagesize ( pagesize ) ;
parquetfilewriter w ,
this ( file , mode , writesupport , compressioncodecname , blocksize ,
parquetproperties . builder ( ) ;
this ( w , writesupport , schema , extrametadata , blocksize , compressor ,
public static final string estimate page size check = "parquet . page . size . check . estimate" ;
writer version , parquetproperties . default writer version . tostring ( ) ) ;
public static final boolean default is dictionary enabled =
parquetproperties . default minimum record count for check ) ;
props ,
. withdictionarypagesize ( dictionarypagesize )
return configuration . getint ( page size , parquetproperties . default page size ) ;
. build ( ) ) ;
private blockmetadata currentblock ;
if ( debug ) log . debug (
new hashmap < string , columnchunkmetadata > ( ) ;
chunk . gettotalsize ( ) ,
appendrowgroup ( file , block , dropcolumns ) ;
public void appendfile ( configuration conf , path file ) throws ioexception {
}
throw new illegalargumentexception (
private primitivetypename currentchunktype ;
private long currentrecordcount ;
bytescopied + = bytesread ;
chunk . getstatistics ( ) ,
} else {
private compressioncodecname currentchunkcodec ;
strings . join ( columnstocopy . keyset ( ) , " , " ) ) ) ;
to . write ( buffer , 0 , bytesread ) ;
return new byte [ 8192 ] ;
for ( blockmetadata block : rowgroups ) {
from . seek ( start ) ;
columnchunkmetadata chunk = columnstocopy . remove ( path ) ;
currentblock . settotalbytesize ( blockcompressedsize ) ;
"unexpected end of input file at " + start + bytescopied ) ;
for ( columndescriptor descriptor : schema . getcolumns ( ) ) {
private columnpath currentchunkpath ;
for ( columnchunkmetadata chunk : rowgroup . getcolumns ( ) ) {
list < blockmetadata > rowgroups ,
long start , long length ) throws ioexception {
chunk . gettotaluncompressedsize ( ) ) ) ;
long start = - 1 ;
new arraylist < columnchunkmetadata > ( ) ;
chunk . getcodec ( ) ,
boolean dropcolumns ) throws ioexception {
private statistics currentstatistics ;
chunk . getpath ( ) ,
import org . apache . hadoop . fs . fsdatainputstream ;
long newchunkstart = out . getpos ( ) + length ;
if ( !dropcolumns & & !columnstocopy . isempty ( ) ) {
public void appendrowgroup ( fsdatainputstream from , blockmetadata rowgroup ,
long bytescopied = 0 ;
chunk . getvaluecount ( ) ,
if ( bytesread < 0 ) {
if ( ( i + 1 ) = = columnsinorder . size ( ) | |
length = 0 ;
startblock ( rowgroup . getrowcount ( ) ) ;
byte [ ] buffer = copy buffer . get ( ) ;
chunk . gettype ( ) ,
int bytesread = from . read ( buffer , 0 ,
endblock ( ) ;
long blockcompressedsize = 0 ;
map < string , columnchunkmetadata > columnstocopy =
new threadlocal < byte [ ] > ( ) {
length + = chunk . gettotalsize ( ) ;
throw new illegalargumentexception ( string . format (
private static final threadlocal < byte [ ] > copy buffer =
"missing column ' % s' , cannot copy row group : % s" , path , rowgroup ) ) ;
long length = 0 ;
chunk . getencodings ( ) ,
long bytesleft = length - bytescopied ;
private set < encoding > currentencodings ;
currentblock . addcolumn ( columnchunkmetadata . get (
@ override
string path = columnpath . get ( descriptor . getpath ( ) ) . todotstring ( ) ;
parquetfilereader . open ( conf , file ) . appendto ( this ) ;
private long currentchunkvaluecount ;
while ( bytescopied < length ) {
private long currentchunkfirstdatapage ;
if ( start < 0 ) {
newchunkstart ,
"copying " + length + " bytes at " + start + " to " + to . getpos ( ) ) ;
import org . apache . parquet . strings ;
( buffer . length < bytesleft ? buffer . length : ( int ) bytesleft ) ) ;
if ( chunk ! = null ) {
columnstocopy . put ( chunk . getpath ( ) . todotstring ( ) , chunk ) ;
import java . io . outputstream ;
copy ( from , out , start , length ) ;
blockcompressedsize + = chunk . gettotalsize ( ) ;
columnsinorder . get ( i + 1 ) . getstartingpos ( ) ! = ( start + length ) ) {
protected byte [ ] initialvalue ( ) {
"columns cannot be copied ( missing from target schema ) : % s" ,
private static void copy ( fsdatainputstream from , fsdataoutputstream to ,
list < columnchunkmetadata > columnsinorder =
start = chunk . getstartingpos ( ) ;
public void appendrowgroups ( fsdatainputstream file ,
start = - 1 ;
} ;
columnchunkmetadata chunk = columnsinorder . get ( i ) ;
private long currentchunkdictionarypageoffset ;
for ( int i = 0 ; i < columnsinorder . size ( ) ; i + = 1 ) {
columnsinorder . add ( chunk ) ;
private blockmetadata currentblock ;
if ( debug ) log . debug (
new hashmap < string , columnchunkmetadata > ( ) ;
chunk . gettotalsize ( ) ,
appendrowgroup ( file , block , dropcolumns ) ;
public void appendfile ( configuration conf , path file ) throws ioexception {
}
throw new illegalargumentexception (
private primitivetypename currentchunktype ;
private long currentrecordcount ;
bytescopied + = bytesread ;
chunk . getstatistics ( ) ,
} else {
private compressioncodecname currentchunkcodec ;
strings . join ( columnstocopy . keyset ( ) , " , " ) ) ) ;
to . write ( buffer , 0 , bytesread ) ;
return new byte [ 8192 ] ;
for ( blockmetadata block : rowgroups ) {
from . seek ( start ) ;
columnchunkmetadata chunk = columnstocopy . remove ( path ) ;
currentblock . settotalbytesize ( blockcompressedsize ) ;
"unexpected end of input file at " + start + bytescopied ) ;
for ( columndescriptor descriptor : schema . getcolumns ( ) ) {
private columnpath currentchunkpath ;
for ( columnchunkmetadata chunk : rowgroup . getcolumns ( ) ) {
list < blockmetadata > rowgroups ,
long start , long length ) throws ioexception {
chunk . gettotaluncompressedsize ( ) ) ) ;
long start = - 1 ;
new arraylist < columnchunkmetadata > ( ) ;
chunk . getcodec ( ) ,
boolean dropcolumns ) throws ioexception {
private statistics currentstatistics ;
chunk . getpath ( ) ,
import org . apache . hadoop . fs . fsdatainputstream ;
long newchunkstart = out . getpos ( ) + length ;
if ( !dropcolumns & & !columnstocopy . isempty ( ) ) {
public void appendrowgroup ( fsdatainputstream from , blockmetadata rowgroup ,
long bytescopied = 0 ;
chunk . getvaluecount ( ) ,
if ( bytesread < 0 ) {
if ( ( i + 1 ) = = columnsinorder . size ( ) | |
length = 0 ;
startblock ( rowgroup . getrowcount ( ) ) ;
byte [ ] buffer = copy buffer . get ( ) ;
chunk . gettype ( ) ,
int bytesread = from . read ( buffer , 0 ,
endblock ( ) ;
long blockcompressedsize = 0 ;
map < string , columnchunkmetadata > columnstocopy =
new threadlocal < byte [ ] > ( ) {
length + = chunk . gettotalsize ( ) ;
throw new illegalargumentexception ( string . format (
private static final threadlocal < byte [ ] > copy buffer =
"missing column ' % s' , cannot copy row group : % s" , path , rowgroup ) ) ;
long length = 0 ;
chunk . getencodings ( ) ,
long bytesleft = length - bytescopied ;
private set < encoding > currentencodings ;
currentblock . addcolumn ( columnchunkmetadata . get (
@ override
string path = columnpath . get ( descriptor . getpath ( ) ) . todotstring ( ) ;
parquetfilereader . open ( conf , file ) . appendto ( this ) ;
private long currentchunkvaluecount ;
while ( bytescopied < length ) {
private long currentchunkfirstdatapage ;
if ( start < 0 ) {
newchunkstart ,
"copying " + length + " bytes at " + start + " to " + to . getpos ( ) ) ;
import org . apache . parquet . strings ;
( buffer . length < bytesleft ? buffer . length : ( int ) bytesleft ) ) ;
if ( chunk ! = null ) {
columnstocopy . put ( chunk . getpath ( ) . todotstring ( ) , chunk ) ;
import java . io . outputstream ;
copy ( from , out , start , length ) ;
blockcompressedsize + = chunk . gettotalsize ( ) ;
columnsinorder . get ( i + 1 ) . getstartingpos ( ) ! = ( start + length ) ) {
protected byte [ ] initialvalue ( ) {
"columns cannot be copied ( missing from target schema ) : % s" ,
private static void copy ( fsdatainputstream from , fsdataoutputstream to ,
list < columnchunkmetadata > columnsinorder =
start = chunk . getstartingpos ( ) ;
public void appendrowgroups ( fsdatainputstream file ,
start = - 1 ;
} ;
columnchunkmetadata chunk = columnsinorder . get ( i ) ;
private long currentchunkdictionarypageoffset ;
for ( int i = 0 ; i < columnsinorder . size ( ) ; i + = 1 ) {
columnsinorder . add ( chunk ) ;
}
import java . util . hashset ;
fieldnames . add ( field . name ( ) ) ;
set < string > fieldnames = new hashset < string > ( ) ;
elementschema . gettype ( ) = = schema . type . record ) {
for ( schema . field field : elementschema . getfields ( ) ) {
return fieldnames . contains ( repeatedtype . asgrouptype ( ) . getfieldname ( 0 ) ) ;
import java . util . set ;
public filemetadata getfilemetadata ( ) {
int footerlength = readintlittleendian ( f ) ;
public parquetmetadata getfooter ( ) {
if ( footerindex < magic . length | | footerindex > = footerlengthindex ) {
return false ;
bytesdecompressor decompressor = codecfactory . getdecompressor ( meta . getcodec ( ) ) ;
private final map < string , columnchunkmetadata > columns ;
return footer ;
long footerlengthindex = l - footer length size - magic . length ;
this . filestatus = fs . getfilestatus ( file ) ;
this . filestatus = fs . getfilestatus ( filepath ) ;
private static dictionarypage readcompresseddictionary (
public static parquetfilereader open ( configuration conf , path file , parquetmetadata footer ) throws ioexception {
if ( currentblock = = blocks . size ( ) ) {
}
string dotpath = strings . join ( descriptor . getpath ( ) , " . " ) ;
bin , uncompressedpagesize , dictheader . getnum values ( ) ,
byte [ ] dictpagebytes = new byte [ compressedpagesize ] ;
this . columns = new hashmap < string , columnchunkmetadata > ( ) ;
return converter . readparquetmetadata ( f , filter ) ;
return cache . get ( dotpath ) ;
return new parquetfilereader ( conf , file , footer ) ;
columnchunkmetadata column = columns . get ( dotpath ) ;
currentrowgroup . addcolumn ( chunk . descriptor . col , chunk . readallpages ( ) ) ;
set < long > set = new hashset < long > ( ) ;
import org . apache . parquet . filter2 . compat . filtercompat ;
if ( nextdictionaryreader ! = null ) {
compressedpage . getencoding ( ) ) ;
int uncompressedpagesize = pageheader . getuncompressed page size ( ) ;
import java . io . ioexception ;
if ( cache . containskey ( dotpath ) ) {
if ( conf . getboolean ( "parquet . filter . statistics . enabled" , true ) ) {
static final class rangemetadatafilter extends metadatafilter implements offsetmetadatafilter {
private final parquetfilereader reader ;
this . rowgroup = rowgroup ;
this . currentrowgroup = new columnchunkpagereadstore ( block . getrowcount ( ) ) ;
this . offsets = offsets ;
public static metadatafilter offsets ( long . . . offsets ) {
< t , e extends throwable > t accept ( metadatafiltervisitor < t , e > visitor ) throws e {
return new dictionarypage (
this ( configuration , file , no filter ) ;
import org . apache . parquet . io . parquetdecodingexception ;
for ( blockmetadata block : blocks ) {
import java . util . map ;
if ( !cache . containskey ( dotpath ) ) {
levels . add ( dictionary ) ;
this . footer = footer ;
int footer length size = 4 ;
this . nextdictionaryreader = getdictionaryreader ( blocks . get ( currentblock ) ) ;
static final class offsetlistmetadatafilter extends metadatafilter implements offsetmetadatafilter {
try {
return getfooter ( ) . getfilemetadata ( ) ;
dictionarypageheader dictheader = pageheader . getdictionary page header ( ) ;
return offsets . contains ( offset ) ;
pageheader pageheader , fsdatainputstream fin ) throws ioexception {
converter . getencoding ( dictheader . getencoding ( ) ) ) ;
boolean contains ( long offset ) ;
getpath ( ) + " offset " + descriptor . metadata . getfirstdatapageoffset ( ) +
import java . util . hashmap ;
@ deprecated
dictionarypagereader ( parquetfilereader reader , blockmetadata block ) {
public static parquetfilereader open ( configuration conf , path file , metadatafilter filter ) throws ioexception {
levels . add ( statistics ) ;
throw new parquetdecodingexception ( "unable to read file footer" , e ) ;
import org . apache . parquet . column . page . dictionarypage ;
f . close ( ) ;
import java . util . set ;
private dictionarypagereader nextdictionaryreader = null ;
columns . put ( column . getpath ( ) . todotstring ( ) , column ) ;
return true ;
private columnchunkpagereadstore rowgroup = null ;
for ( columndescriptor col : footer . getfilemetadata ( ) . getschema ( ) . getcolumns ( ) ) {
import static org . apache . parquet . filter2 . compat . rowgroupfilter . filterlevel . statistics ;
total + = block . getrowcount ( ) ;
return total ;
if ( !meta . getencodings ( ) . contains ( encoding . plain dictionary ) & &
in . close ( ) ;
"cannot load dictionary , unknown column : " + dotpath ) ;
private final filemetadata filemetadata ;
advancetonextblock ( ) ;
fsdatainputstream in = filesystem . open ( file . getpath ( ) ) ;
long total = 0 ;
private final filestatus filestatus ;
this . blocks = footer . getblocks ( ) ;
public parquetfilereader ( configuration conf , path file , parquetmetadata footer ) throws ioexception {
import org . apache . parquet . column . columndescriptor ;
public long getrecordcount ( ) {
if ( log . debug ) {
int compressedpagesize = pageheader . getcompressed page size ( ) ;
long l = file . getlen ( ) ;
f . seek ( meta . getstartingpos ( ) ) ;
return new dictionarypagereader ( this , block ) ;
private boolean advancetonextblock ( ) {
if ( conf . getboolean ( "parquet . filter . dictionary . enabled" , false ) ) {
throw new parquetdecodingexception (
compressedpage . getdictionarysize ( ) ,
import org . apache . parquet . column . encoding ;
if ( filemetadata ! = null ) {
t visit ( offsetmetadatafilter filter ) throws e ;
for ( long offset : offsets ) {
this . blocks = rowgroupfilter . filterrowgroups ( levels , filter , blocks , this ) ;
private parquetmetadata footer ;
return blocks ;
private final configuration conf ;
log . debug ( "file length " + l ) ;
if ( l < magic . length + footer length size + magic . length ) {
dictionarypage readdictionary ( columnchunkmetadata meta ) throws ioexception {
package org . apache . parquet . hadoop ;
import org . apache . parquet . hadoop . metadata . blockmetadata ;
return new parquetfilereader ( conf , file , filter ) ;
return new offsetlistmetadatafilter ( set ) ;
public boolean contains ( long offset ) {
import org . apache . parquet . column . page . dictionarypagereadstore ;
private static final parquetmetadata readfooter ( filestatus file , fsdatainputstream f , metadatafilter filter ) throws ioexception {
import org . apache . parquet . filter2 . compat . rowgroupfilter ;
f . seek ( footerindex ) ;
public static parquetfilereader open ( configuration conf , path file ) throws ioexception {
this . footer = readfooter ( filestatus , f , filter ) ;
if ( !arrays . equals ( magic , magic ) ) {
private final set < long > offsets ;
import static org . apache . parquet . column . encoding . plain dictionary ;
list < rowgroupfilter . filterlevel > levels = new arraylist < rowgroupfilter . filterlevel > ( ) ;
private columnchunkpagereadstore currentrowgroup = null ;
return advancetonextblock ( ) ;
static filemetadata filterfilemetadata ( filemetadata metadata , offsetmetadatafilter filter ) {
private list < blockmetadata > blocks ;
import static org . apache . parquet . filter2 . compat . rowgroupfilter . filterlevel . dictionary ;
getfilemetadata ( ) . getcreatedby ( ) ,
bytesinput bin = bytesinput . from ( dictpagebytes ) ;
public dictionarypagereadstore getnextdictionaryreader ( ) {
long footerindex = footerlengthindex - footerlength ;
if ( rowgroup ! = null ) {
private final map < string , dictionarypage > cache = new hashmap < string , dictionarypage > ( ) ;
set . add ( offset ) ;
public path getpath ( ) {
dictionarypage compressedpage = readcompresseddictionary ( pageheader , f ) ;
} catch ( ioexception e ) {
return nextdictionaryreader ;
return filestatus . getpath ( ) ;
@ override
this . f = fs . open ( file ) ;
public parquetfilereader ( configuration conf , path file , metadatafilter filter ) throws ioexception {
return null ;
return ( encodings . contains ( plain dictionary ) | | encodings . contains ( rle dictionary ) ) ;
paths . put ( columnpath . get ( col . getpath ( ) ) , col ) ;
!meta . getencodings ( ) . contains ( encoding . rle dictionary ) ) {
codecfactory . release ( ) ;
fin . readfully ( dictpagebytes ) ;
synchronized ( cache ) {
byte [ ] magic = new byte [ magic . length ] ;
this . conf = conf ;
return readfooter ( file , in , filter ) ;
if ( column = = null ) {
if ( codecfactory ! = null ) {
private boolean hasdictionarypage ( columnchunkmetadata column ) {
public boolean skipnextrowgroup ( ) {
if ( nextdictionaryreader = = null & & currentblock < blocks . size ( ) ) {
throw new runtimeexception ( "corrupted file : the footer index is not within the file" ) ;
import org . apache . parquet . strings ;
if ( f ! = null ) {
void setrowgroup ( columnchunkpagereadstore rowgroup ) {
return filemetadata ;
nextdictionaryreader . setrowgroup ( currentrowgroup ) ;
throw new runtimeexception ( file . getpath ( ) + " is not a parquet file . expected magic number at tail " + arrays . tostring ( magic ) + " but found " + arrays . tostring ( magic ) ) ;
if ( f . getpos ( ) ! = meta . getstartingpos ( ) ) {
cache . put ( dotpath , dict ) ;
f . seek ( footerlengthindex ) ;
this . conf = configuration ;
void filterrowgroups ( filtercompat . filter filter ) throws ioexception {
public dictionarypage readdictionarypage ( columndescriptor descriptor ) {
interface offsetmetadatafilter {
private parquetfilereader ( configuration configuration , path file ) throws ioexception {
return visitor . visit ( this ) ;
import static org . apache . parquet . column . encoding . rle dictionary ;
public list < blockmetadata > getrowgroups ( ) {
this . filemetadata = footer . getfilemetadata ( ) ;
log . debug ( "read footer length : " + footerlength + " , footer index : " + footerindex ) ;
this . nextdictionaryreader = null ;
this . codecfactory = new codecfactory ( conf ) ;
dictionarypage dict = hasdictionarypage ( column ) ? reader . readdictionary ( column ) : null ;
if ( footer = = null ) {
return new parquetfilereader ( conf , file ) ;
this . footer = readfooter ( filestatus , f , skip row groups ) ;
this . reader = reader ;
f . readfully ( magic ) ;
"failed to read dictionary" , e ) ;
decompressor . decompress ( compressedpage . getbytes ( ) , compressedpage . getuncompressedsize ( ) ) ,
filesystem fs = file . getfilesystem ( conf ) ;
throw new runtimeexception ( file . getpath ( ) + " is not a parquet file ( too small ) " ) ;
return currentrowgroup ;
import org . apache . parquet . hadoop . metadata . columnchunkmetadata ;
if ( !pageheader . issetdictionary page header ( ) ) {
return rowgroup . readdictionarypage ( descriptor ) ;
public offsetlistmetadatafilter ( set < long > offsets ) {
log . debug ( "reading footer index at " + footerlengthindex ) ;
class dictionarypagereader implements dictionarypagereadstore {
for ( columnchunkmetadata column : block . getcolumns ( ) ) {
set < encoding > encodings = column . getencodings ( ) ;
public dictionarypagereader getdictionaryreader ( blockmetadata block ) {
pageheader pageheader = util . readpageheader ( f ) ;
public filemetadata visit ( offsetmetadatafilter filter ) throws ioexception {
public filemetadata getfilemetadata ( ) {
int footerlength = readintlittleendian ( f ) ;
public parquetmetadata getfooter ( ) {
if ( footerindex < magic . length | | footerindex > = footerlengthindex ) {
return false ;
bytesdecompressor decompressor = codecfactory . getdecompressor ( meta . getcodec ( ) ) ;
private final map < string , columnchunkmetadata > columns ;
return footer ;
long footerlengthindex = l - footer length size - magic . length ;
this . filestatus = fs . getfilestatus ( file ) ;
this . filestatus = fs . getfilestatus ( filepath ) ;
private static dictionarypage readcompresseddictionary (
public static parquetfilereader open ( configuration conf , path file , parquetmetadata footer ) throws ioexception {
if ( currentblock = = blocks . size ( ) ) {
}
string dotpath = strings . join ( descriptor . getpath ( ) , " . " ) ;
bin , uncompressedpagesize , dictheader . getnum values ( ) ,
byte [ ] dictpagebytes = new byte [ compressedpagesize ] ;
this . columns = new hashmap < string , columnchunkmetadata > ( ) ;
return converter . readparquetmetadata ( f , filter ) ;
return cache . get ( dotpath ) ;
return new parquetfilereader ( conf , file , footer ) ;
columnchunkmetadata column = columns . get ( dotpath ) ;
currentrowgroup . addcolumn ( chunk . descriptor . col , chunk . readallpages ( ) ) ;
set < long > set = new hashset < long > ( ) ;
import org . apache . parquet . filter2 . compat . filtercompat ;
if ( nextdictionaryreader ! = null ) {
compressedpage . getencoding ( ) ) ;
int uncompressedpagesize = pageheader . getuncompressed page size ( ) ;
import java . io . ioexception ;
if ( cache . containskey ( dotpath ) ) {
if ( conf . getboolean ( "parquet . filter . statistics . enabled" , true ) ) {
static final class rangemetadatafilter extends metadatafilter implements offsetmetadatafilter {
private final parquetfilereader reader ;
this . rowgroup = rowgroup ;
this . currentrowgroup = new columnchunkpagereadstore ( block . getrowcount ( ) ) ;
this . offsets = offsets ;
public static metadatafilter offsets ( long . . . offsets ) {
< t , e extends throwable > t accept ( metadatafiltervisitor < t , e > visitor ) throws e {
return new dictionarypage (
this ( configuration , file , no filter ) ;
import org . apache . parquet . io . parquetdecodingexception ;
for ( blockmetadata block : blocks ) {
import java . util . map ;
if ( !cache . containskey ( dotpath ) ) {
levels . add ( dictionary ) ;
this . footer = footer ;
int footer length size = 4 ;
this . nextdictionaryreader = getdictionaryreader ( blocks . get ( currentblock ) ) ;
static final class offsetlistmetadatafilter extends metadatafilter implements offsetmetadatafilter {
try {
return getfooter ( ) . getfilemetadata ( ) ;
dictionarypageheader dictheader = pageheader . getdictionary page header ( ) ;
return offsets . contains ( offset ) ;
pageheader pageheader , fsdatainputstream fin ) throws ioexception {
converter . getencoding ( dictheader . getencoding ( ) ) ) ;
boolean contains ( long offset ) ;
getpath ( ) + " offset " + descriptor . metadata . getfirstdatapageoffset ( ) +
import java . util . hashmap ;
@ deprecated
this . codecfactory = new codecfactory ( conf , 0 ) ;
public static parquetfilereader open ( configuration conf , path file , metadatafilter filter ) throws ioexception {
dictionarypagereader ( parquetfilereader reader , blockmetadata block ) {
levels . add ( statistics ) ;
throw new parquetdecodingexception ( "unable to read file footer" , e ) ;
import org . apache . parquet . column . page . dictionarypage ;
f . close ( ) ;
import java . util . set ;
private dictionarypagereader nextdictionaryreader = null ;
columns . put ( column . getpath ( ) . todotstring ( ) , column ) ;
return true ;
private columnchunkpagereadstore rowgroup = null ;
for ( columndescriptor col : footer . getfilemetadata ( ) . getschema ( ) . getcolumns ( ) ) {
import static org . apache . parquet . filter2 . compat . rowgroupfilter . filterlevel . statistics ;
total + = block . getrowcount ( ) ;
return total ;
if ( !meta . getencodings ( ) . contains ( encoding . plain dictionary ) & &
in . close ( ) ;
"cannot load dictionary , unknown column : " + dotpath ) ;
private final filemetadata filemetadata ;
advancetonextblock ( ) ;
fsdatainputstream in = filesystem . open ( file . getpath ( ) ) ;
long total = 0 ;
private final filestatus filestatus ;
this . blocks = footer . getblocks ( ) ;
public parquetfilereader ( configuration conf , path file , parquetmetadata footer ) throws ioexception {
import org . apache . parquet . column . columndescriptor ;
public long getrecordcount ( ) {
if ( log . debug ) {
int compressedpagesize = pageheader . getcompressed page size ( ) ;
long l = file . getlen ( ) ;
f . seek ( meta . getstartingpos ( ) ) ;
return new dictionarypagereader ( this , block ) ;
private boolean advancetonextblock ( ) {
if ( conf . getboolean ( "parquet . filter . dictionary . enabled" , false ) ) {
throw new parquetdecodingexception (
compressedpage . getdictionarysize ( ) ,
import org . apache . parquet . column . encoding ;
if ( filemetadata ! = null ) {
t visit ( offsetmetadatafilter filter ) throws e ;
for ( long offset : offsets ) {
this . blocks = rowgroupfilter . filterrowgroups ( levels , filter , blocks , this ) ;
private parquetmetadata footer ;
return blocks ;
private final configuration conf ;
log . debug ( "file length " + l ) ;
if ( l < magic . length + footer length size + magic . length ) {
dictionarypage readdictionary ( columnchunkmetadata meta ) throws ioexception {
package org . apache . parquet . hadoop ;
import org . apache . parquet . hadoop . metadata . blockmetadata ;
return new parquetfilereader ( conf , file , filter ) ;
return new offsetlistmetadatafilter ( set ) ;
public boolean contains ( long offset ) {
import org . apache . parquet . column . page . dictionarypagereadstore ;
private static final parquetmetadata readfooter ( filestatus file , fsdatainputstream f , metadatafilter filter ) throws ioexception {
import org . apache . parquet . filter2 . compat . rowgroupfilter ;
f . seek ( footerindex ) ;
public static parquetfilereader open ( configuration conf , path file ) throws ioexception {
this . footer = readfooter ( filestatus , f , filter ) ;
if ( !arrays . equals ( magic , magic ) ) {
private final set < long > offsets ;
import static org . apache . parquet . column . encoding . plain dictionary ;
list < rowgroupfilter . filterlevel > levels = new arraylist < rowgroupfilter . filterlevel > ( ) ;
private columnchunkpagereadstore currentrowgroup = null ;
return advancetonextblock ( ) ;
static filemetadata filterfilemetadata ( filemetadata metadata , offsetmetadatafilter filter ) {
private list < blockmetadata > blocks ;
import static org . apache . parquet . filter2 . compat . rowgroupfilter . filterlevel . dictionary ;
getfilemetadata ( ) . getcreatedby ( ) ,
bytesinput bin = bytesinput . from ( dictpagebytes ) ;
public dictionarypagereadstore getnextdictionaryreader ( ) {
long footerindex = footerlengthindex - footerlength ;
if ( rowgroup ! = null ) {
private final map < string , dictionarypage > cache = new hashmap < string , dictionarypage > ( ) ;
set . add ( offset ) ;
public path getpath ( ) {
dictionarypage compressedpage = readcompresseddictionary ( pageheader , f ) ;
} catch ( ioexception e ) {
return nextdictionaryreader ;
return filestatus . getpath ( ) ;
@ override
this . f = fs . open ( file ) ;
public parquetfilereader ( configuration conf , path file , metadatafilter filter ) throws ioexception {
return null ;
return ( encodings . contains ( plain dictionary ) | | encodings . contains ( rle dictionary ) ) ;
paths . put ( columnpath . get ( col . getpath ( ) ) , col ) ;
!meta . getencodings ( ) . contains ( encoding . rle dictionary ) ) {
codecfactory . release ( ) ;
fin . readfully ( dictpagebytes ) ;
synchronized ( cache ) {
byte [ ] magic = new byte [ magic . length ] ;
this . conf = conf ;
return readfooter ( file , in , filter ) ;
if ( column = = null ) {
if ( codecfactory ! = null ) {
private boolean hasdictionarypage ( columnchunkmetadata column ) {
public boolean skipnextrowgroup ( ) {
if ( nextdictionaryreader = = null & & currentblock < blocks . size ( ) ) {
throw new runtimeexception ( "corrupted file : the footer index is not within the file" ) ;
import org . apache . parquet . strings ;
if ( f ! = null ) {
void setrowgroup ( columnchunkpagereadstore rowgroup ) {
return filemetadata ;
nextdictionaryreader . setrowgroup ( currentrowgroup ) ;
throw new runtimeexception ( file . getpath ( ) + " is not a parquet file . expected magic number at tail " + arrays . tostring ( magic ) + " but found " + arrays . tostring ( magic ) ) ;
if ( f . getpos ( ) ! = meta . getstartingpos ( ) ) {
cache . put ( dotpath , dict ) ;
f . seek ( footerlengthindex ) ;
this . conf = configuration ;
void filterrowgroups ( filtercompat . filter filter ) throws ioexception {
public dictionarypage readdictionarypage ( columndescriptor descriptor ) {
interface offsetmetadatafilter {
private parquetfilereader ( configuration configuration , path file ) throws ioexception {
return visitor . visit ( this ) ;
import static org . apache . parquet . column . encoding . rle dictionary ;
public list < blockmetadata > getrowgroups ( ) {
this . filemetadata = footer . getfilemetadata ( ) ;
log . debug ( "read footer length : " + footerlength + " , footer index : " + footerindex ) ;
this . nextdictionaryreader = null ;
dictionarypage dict = hasdictionarypage ( column ) ? reader . readdictionary ( column ) : null ;
if ( footer = = null ) {
return new parquetfilereader ( conf , file ) ;
this . footer = readfooter ( filestatus , f , skip row groups ) ;
this . reader = reader ;
f . readfully ( magic ) ;
"failed to read dictionary" , e ) ;
decompressor . decompress ( compressedpage . getbytes ( ) , compressedpage . getuncompressedsize ( ) ) ,
filesystem fs = file . getfilesystem ( conf ) ;
throw new runtimeexception ( file . getpath ( ) + " is not a parquet file ( too small ) " ) ;
return currentrowgroup ;
import org . apache . parquet . hadoop . metadata . columnchunkmetadata ;
if ( !pageheader . issetdictionary page header ( ) ) {
return rowgroup . readdictionarypage ( descriptor ) ;
public offsetlistmetadatafilter ( set < long > offsets ) {
log . debug ( "reading footer index at " + footerlengthindex ) ;
class dictionarypagereader implements dictionarypagereadstore {
for ( columnchunkmetadata column : block . getcolumns ( ) ) {
set < encoding > encodings = column . getencodings ( ) ;
public dictionarypagereader getdictionaryreader ( blockmetadata block ) {
pageheader pageheader = util . readpageheader ( f ) ;
public filemetadata visit ( offsetmetadatafilter filter ) throws ioexception {
this . allocator = new heapbytebufferallocator ( ) ;
t value = lt . getvalue ( ) ;
return isallnulls ( meta ) ;
t value = lteq . getvalue ( ) ;
t value = gteq . getvalue ( ) ;
if ( hasnulls ( meta ) ) {
@ suppresswarnings ( "unchecked" )
if ( value ! = null ) {
columnchunkmetadata meta = getcolumnchunk ( filtercolumn . getcolumnpath ( ) ) ;
private static final boolean block cannot match = true ;
}
statistics < t > stats = meta . getstatistics ( ) ;
if ( meta = = null ) {
return columns . get ( columnpath ) ;
return block cannot match ;
t value = gt . getvalue ( ) ;
if ( value = = null ) {
if ( isallnulls ( meta ) ) {
private static final boolean block might match = false ;
return !hasnulls ( meta ) ;
return block might match ;
t value = lt . getvalue ( ) ;
return isallnulls ( meta ) ;
t value = lteq . getvalue ( ) ;
t value = gteq . getvalue ( ) ;
if ( hasnulls ( meta ) ) {
@ suppresswarnings ( "unchecked" )
if ( value ! = null ) {
columnchunkmetadata meta = getcolumnchunk ( filtercolumn . getcolumnpath ( ) ) ;
private static final boolean block cannot match = true ;
}
statistics < t > stats = meta . getstatistics ( ) ;
if ( meta = = null ) {
return columns . get ( columnpath ) ;
return block cannot match ;
t value = gt . getvalue ( ) ;
if ( value = = null ) {
if ( isallnulls ( meta ) ) {
private static final boolean block might match = false ;
return !hasnulls ( meta ) ;
return block might match ;
return readfooter ( file . getlen ( ) , file . getpath ( ) . tostring ( ) , in , filter ) ;
public static final parquetmetadata readfooter ( long filelen , string filepath , fsdatainputstream f , metadatafilter filter ) throws ioexception {
if ( filelen < magic . length + footer length size + magic . length ) {
throw new runtimeexception ( filepath + " is not a parquet file ( too small ) " ) ;
this . footer = readfooter ( filestatus . getlen ( ) , filestatus . getpath ( ) . tostring ( ) , f , filter ) ;
log . debug ( "file length " + filelen ) ;
throw new runtimeexception ( filepath + " is not a parquet file . expected magic number at tail " + arrays . tostring ( magic ) + " but found " + arrays . tostring ( magic ) ) ;
long footerlengthindex = filelen - footer length size - magic . length ;
this . footer = readfooter ( filestatus . getlen ( ) , filestatus . getpath ( ) . tostring ( ) , f , skip row groups ) ;
return readfooter ( file . getlen ( ) , file . getpath ( ) . tostring ( ) , in , filter ) ;
public static final parquetmetadata readfooter ( long filelen , string filepath , fsdatainputstream f , metadatafilter filter ) throws ioexception {
if ( filelen < magic . length + footer length size + magic . length ) {
throw new runtimeexception ( filepath + " is not a parquet file ( too small ) " ) ;
this . footer = readfooter ( filestatus . getlen ( ) , filestatus . getpath ( ) . tostring ( ) , f , filter ) ;
log . debug ( "file length " + filelen ) ;
throw new runtimeexception ( filepath + " is not a parquet file . expected magic number at tail " + arrays . tostring ( magic ) + " but found " + arrays . tostring ( magic ) ) ;
long footerlengthindex = filelen - footer length size - magic . length ;
this . footer = readfooter ( filestatus . getlen ( ) , filestatus . getpath ( ) . tostring ( ) , f , skip row groups ) ;
rowgroupnum , pagenum , "dict" , shortcodec , enc , count , humanreadable ( pervalue ) ,
import com . google . common . collect . maps ;
) ;
import org . apache . parquet . column . page . pagereader ;
this . pagenum = 0 ;
console . info ( string . format ( " \ nrow group % d : count : % d % s records start : % d total : % s % s \ n % s" ,
console . info ( line ) ;
console . info ( string . format ( " % - " + width + "s % - 9s % s % - 7s % - 9d % - 10s % - 7s % s" ,
for ( columnchunkmetadata column : rowgroup . getcolumns ( ) ) {
while ( ( pagestore = reader . readnextrowgroup ( ) ) ! = null ) {
import static org . apache . parquet . cli . util . encodingasstring ;
public string visit ( datapagev1 page ) {
@ parameters ( commanddescription = "print a parquet file's metadata" )
console . info ( string . format ( " % - " + width + "s fixed [ % d ] % s % - 7s % - 9d % - 8s % - 7s % s" ,
@ parameter ( description = " < parquet path > " )
getconf ( ) , qualifiedpath ( source ) , parquetmetadataconverter . no filter ) ;
import org . apache . parquet . column . page . datapage ;
import static org . apache . parquet . cli . util . encodingsasstring ;
}
public string apply ( @ nullable columnchunkmetadata input ) {
import org . apache . parquet . column . page . datapagev2 ;
int size = 0 ;
string formatted = "" ;
import org . apache . commons . lang . stringutils ;
dict . getuncompressedsize ( ) ;
} else {
dictionarypage dict = pages . readdictionarypage ( ) ;
if ( page instanceof dictionarypage ) {
import java . io . ioexception ;
"a parquet file is required . " ) ;
console . info ( "properties : " ) ;
compressioncodecname codec = column . getcodec ( ) ;
if ( typename = = primitivetype . primitivetypename . fixed len byte array ) {
public showpagescommand ( logger console ) {
name , type . gettypelength ( ) , shortcodec ( codec ) , encodingsummary , count ,
import static org . apache . parquet . cli . util . primitive ;
parquetmetadata footer = parquetfilereader . readfooter (
string getheader ( ) {
names = { " - c" , " - - column" , " - - columns" } ,
import org . apache . parquet . format . converter . parquetmetadataconverter ;
console . info ( "schema : \ n { } " , schema ) ;
return input = = null ? "" : input . getpath ( ) . todotstring ( ) ;
return string . format ( " % 3d - % - 3d % - 5s % s % - 2s % - 7d % - 10s % - 10s % - 8s % - 7s % s" ,
rowgroupnum , "dict" , shortcodec , enc , count , humanreadable ( pervalue ) ,
console . info ( string . format ( " \ ncolumn : % s \ n % s" , columnname , stringutils . leftpad ( "" , 80 , ' - ' ) ) ) ;
lines . add ( formatter . format ( page ) ) ;
return string . format ( " % 3d - % - 3d % - 5s % s % - 2s % - 7d % - 10s % - 10s" ,
import static org . apache . parquet . cli . util . columnname ;
printcolumnchunk ( console , size , column , schema ) ;
import java . util . map ;
return string . format ( " % - 6s % - 5s % - 4s % - 7s % - 10s % - 10s % - 8s % - 7s % s" ,
lines = lists . newarraylist ( ) ;
index , rowcount ,
import com . google . common . collect . lists ;
private int rowgroupnum ;
import com . google . common . base . preconditions ;
humanreadable ( pervalue ) , stats = = null ? "" : string . valueof ( stats . getnumnulls ( ) ) ,
import javax . annotation . nullable ;
this . type = type ;
return 0 ;
formatted = ( ( datapage ) page ) . accept ( this ) ;
parquetfilereader reader = parquetfilereader . open ( getconf ( ) , qualifiedpath ( source ) ) ;
string enc = encodingasstring ( dict . getencoding ( ) , true ) ;
long totalsize = page . getcompressedsize ( ) ;
for ( columndescriptor descriptor : schema . getcolumns ( ) ) {
public list < string > getexamples ( ) {
columndescriptor desc = schema . getcolumndescription ( path ) ;
for ( string columnname : formatted . keyset ( ) ) {
if ( kv ! = null & & !kv . isempty ( ) ) {
@ suppresswarnings ( "unchecked" )
columns . put ( descriptor , primitive ( schema , descriptor . getpath ( ) ) ) ;
encodingstats encodingstats = column . getencodingstats ( ) ;
size = math . max ( size , s . length ( ) ) ;
float pervalue = ( ( float ) size ) / count ;
map < string , string > kv = footer . getfilemetadata ( ) . getkeyvaluemetadata ( ) ;
"page" , "type" , "enc" , "count" , "avg size" , "size" , "rows" , "nulls" , "min / max" ) ;
private primitivetype type ;
import org . apache . parquet . schema . messagetype ;
for ( int index = 0 , n = rowgroups . size ( ) ; index < n ; index + = 1 ) {
public class parquetmetadatacommand extends basecommand {
return size ;
super ( console ) ;
pagereadstore pagestore ;
"" , "type" , "encodings" , "count" , "avg size" , "nulls" , "min / max" ) ) ;
import com . google . common . base . function ;
import org . apache . parquet . column . page . dictionarypage ;
console . info ( string . format ( format , entry . getkey ( ) , entry . getvalue ( ) ) ) ;
import java . util . set ;
lines . add ( formatter . format ( dict ) ) ;
this . shortcodec = shortcodec ( codec ) ;
int count = page . getvaluecount ( ) ;
private int pagenum ;
return lists . newarraylist (
private string printdictionarypage ( dictionarypage dict ) {
stats = = null ? "" : string . valueof ( stats . getnumnulls ( ) ) ,
if ( this . columns = = null | | this . columns . isempty ( ) ) {
preconditions . checkargument ( targets ! = null & & targets . size ( ) > = 1 ,
encodingstatsasstring ( encodingstats ) ;
long numnulls = page . getstatistics ( ) . getnumnulls ( ) ;
primitivetype type = primitive ( schema , path ) ;
if ( pagenum = = 0 ) {
import org . apache . parquet . column . statistics . statistics ;
while ( ( page = pages . readpage ( ) ) ! = null ) {
string enc = encodingasstring ( page . getvalueencoding ( ) , false ) ;
console . info ( string . format ( " % - " + size + "s % - 9s % - 9s % - 9s % - 10s % - 7s % s" ,
console . info ( "created by : { } " , footer . getfilemetadata ( ) . getcreatedby ( ) ) ;
columns . put ( descriptor ( column , schema ) , primitive ( column , schema ) ) ;
import org . apache . parquet . column . columndescriptor ;
return string . format ( " % 3d - % - 3d % - 5s % s % - 2s % - 7d % - 10s % - 10s % - 8d % - 7s % s" ,
private void printrowgroup ( logger console , int index , blockmetadata rowgroup , messagetype schema ) {
this . rowgroupnum = rowgroupnum ;
import org . apache . parquet . hadoop . metadata . compressioncodecname ;
import org . apache . parquet . hadoop . metadata . parquetmetadata ;
formatter . setcontext ( rowgroupnum , columns . get ( descriptor ) , codec ) ;
import org . apache . parquet . column . encodingstats ;
import org . apache . parquet . column . page . datapagev1 ;
public int run ( ) throws ioexception {
string filepath = rowgroup . getpath ( ) ;
new function < columnchunkmetadata , string > ( ) {
printrowgroup ( console , index , rowgroups . get ( index ) , schema ) ;
string compression = ( page . iscompressed ( ) ? shortcodec : " " ) ;
int numrows = page . getrowcount ( ) ;
string enc = encodingasstring ( page . getdataencoding ( ) , false ) ;
pagenum + = 1 ;
humanreadable ( totalsize ) , numrows , numnulls , minmax ) ;
humanreadable ( totalsize ) , "" , numnulls , minmax ) ;
string source = targets . get ( 0 ) ;
rowgroupnum , pagenum , "data" , shortcodec , enc , count , humanreadable ( pervalue ) ,
list < string > columns ;
name , typename , shortcodec ( codec ) , encodingsummary , count , humanreadable ( pervalue ) ,
import org . apache . parquet . column . encoding ;
float pervalue = ( ( float ) totalsize ) / count ;
"# show pages for column 'col' from a parquet file" ,
private class pageformatter implements datapage . visitor < string > {
string encodingsummary = encodingstats = = null ?
primitivetype . primitivetypename typename = type . getprimitivetypename ( ) ;
@ parameter (
return formatted ;
import java . util . list ;
import org . apache . parquet . hadoop . metadata . blockmetadata ;
if ( lines = = null ) {
stringutils . leftpad ( "" , 80 , ' - ' ) ) ) ;
for ( columndescriptor descriptor : columns . keyset ( ) ) {
import static org . apache . parquet . cli . util . humanreadable ;
} else if ( page instanceof datapage ) {
map < columndescriptor , primitivetype > columns = maps . newlinkedhashmap ( ) ;
statistics stats = column . getstatistics ( ) ;
private int maxsize ( iterable < string > strings ) {
messagetype schema = reader . getfilemetadata ( ) . getschema ( ) ;
int size = maxsize ( iterables . transform ( rowgroup . getcolumns ( ) ,
compressioncodecname codec = reader . getrowgroups ( ) . get ( 0 ) . getcolumns ( ) . get ( 0 ) . getcodec ( ) ;
import org . apache . parquet . schema . primitivetype ;
import org . slf4j . logger ;
import org . apache . parquet . cli . basecommand ;
encodingsasstring ( encodings , desc ) :
"cannot process multiple parquet files . " ) ;
void setcontext ( int rowgroupnum , primitivetype type , compressioncodecname codec ) {
formatted = printdictionarypage ( ( dictionarypage ) page ) ;
import org . apache . parquet . column . page . page ;
console . info ( formatter . getheader ( ) ) ;
public class showpagescommand extends basecommand {
pagereader pages = pagestore . getpagereader ( descriptor ) ;
import static org . apache . parquet . cli . util . descriptor ;
@ override
list < string > targets ;
console . info ( " \ nfile path : { } " , source ) ;
start , humanreadable ( compressedsize ) ,
pageformatter formatter = new pageformatter ( ) ;
package org . apache . parquet . cli . commands ;
description = "list of columns" )
string format ( page page ) {
rowgroupnum + = 1 ;
import org . apache . parquet . column . page . pagereadstore ;
long count = column . getvaluecount ( ) ;
messagetype schema = footer . getfilemetadata ( ) . getschema ( ) ;
long totalsize = dict . getcompressedsize ( ) ;
list < blockmetadata > rowgroups = footer . getblocks ( ) ;
minmaxasstring ( stats , type . getoriginaltype ( ) ) ) ) ;
for ( string column : this . columns ) {
for ( map . entry < string , string > entry : kv . entryset ( ) ) {
import static org . apache . parquet . cli . util . shortcodec ;
filepath ! = null ? " path : " + filepath : "" ,
long rowcount = rowgroup . getrowcount ( ) ;
datapage page ;
map < string , list < string > > formatted = maps . newlinkedhashmap ( ) ;
@ parameters ( commanddescription = "print page summaries for a parquet file" )
int rowgroupnum = 0 ;
formatted . put ( columnname ( descriptor ) , lines ) ;
console . info ( "" ) ;
humanreadable ( ( ( float ) compressedsize ) / rowcount ) ,
if ( dict ! = null ) {
private string shortcodec ;
rowgroupnum , pagenum , "data" , compression , enc , count , humanreadable ( pervalue ) ,
" - c col sample . parquet"
preconditions . checkargument ( targets . size ( ) = = 1 ,
string format = " % " + maxsize ( kv . keyset ( ) ) + "s : % s" ;
long uncompressedsize = rowgroup . gettotalbytesize ( ) ;
string minmax = minmaxasstring ( page . getstatistics ( ) , type . getoriginaltype ( ) ) ;
import org . apache . parquet . hadoop . parquetfilereader ;
private void printcolumnchunk ( logger console , int width , columnchunkmetadata column , messagetype schema ) {
return string . format ( " % 3d - d % - 5s % s % - 2s % - 7d % - 10s % - 10s" ,
} ) ) ;
public string visit ( datapagev2 page ) {
import com . beust . jcommander . parameters ;
for ( string s : strings ) {
public parquetmetadatacommand ( logger console ) {
preconditions . checknotnull ( type ) ;
string [ ] path = column . getpath ( ) . toarray ( ) ;
import com . google . common . collect . iterables ;
long compressedsize = rowgroup . getcompressedsize ( ) ;
import org . apache . parquet . hadoop . metadata . columnchunkmetadata ;
import com . beust . jcommander . parameter ;
humanreadable ( totalsize ) ) ;
long start = rowgroup . getstartingpos ( ) ;
long size = column . gettotalsize ( ) ;
import static org . apache . parquet . cli . util . encodingstatsasstring ;
console . info ( "properties : ( none ) " ) ;
int count = dict . getdictionarysize ( ) ;
list < string > lines = formatted . get ( columnname ( descriptor ) ) ;
int numnulls = page . getnullcount ( ) ;
set < encoding > encodings = column . getencodings ( ) ;
import static org . apache . parquet . cli . util . minmaxasstring ;
string name = column . getpath ( ) . todotstring ( ) ;
for ( string line : formatted . get ( columnname ) ) {
) ;
bytebufferallocator allocator ,
usedictionaryfilter ( conf . getboolean ( stats filtering enabled , true ) ) ;
filtercompat . filter recordfilter ,
withcodecfactory ( hadoopcodecs . newfactory ( conf , 0 ) ) ;
return conf . get ( property ) ;
public string getproperty ( string property ) {
public static builder builder ( configuration conf ) {
private final configuration conf ;
if ( value ! = null ) {
metadatafilter , codecfactory , allocator , properties
usestatsfilter ( conf . getboolean ( dictionary filtering enabled , true ) ) ;
public static class builder extends parquetreadoptions . builder {
super (
private hadoopreadoptions ( boolean usesignedstringminmax ,
}
public class hadoopreadoptions extends parquetreadoptions {
return value ;
usesignedstringminmax , usestatsfilter , usedictionaryfilter , userecordfilter ,
import static org . apache . parquet . hadoop . parquetinputformat . dictionary filtering enabled ;
return new hadoopreadoptions (
this . conf = conf ;
set ( bad record threshold conf key , badrecordthresh ) ;
configuration conf ) {
usesignedstringminmax ( conf . getboolean ( "parquet . strings . signed - min - max . enabled" , false ) ) ;
public parquetreadoptions build ( ) {
return conf ;
import org . apache . hadoop . conf . configuration ;
import org . apache . parquet . filter2 . compat . filtercompat ;
userecordfilter ( conf . getboolean ( record filtering enabled , true ) ) ;
string value = super . getproperty ( property ) ;
boolean userecordfilter ,
import static org . apache . parquet . hadoop . unmaterializablerecordcounter . bad record threshold conf key ;
import static org . apache . parquet . hadoop . parquetinputformat . stats filtering enabled ;
compressioncodecfactory codecfactory ,
return new builder ( conf ) ;
public builder ( configuration conf ) {
import static org . apache . parquet . hadoop . parquetinputformat . record filtering enabled ;
map < string , string > properties ,
string badrecordthresh = conf . get ( bad record threshold conf key ) ;
public configuration getconf ( ) {
@ override
package org . apache . parquet ;
withrecordfilter ( getfilter ( conf ) ) ;
import org . apache . parquet . hadoop . util . hadoopcodecs ;
import static org . apache . parquet . hadoop . parquetinputformat . getfilter ;
import java . util . map ;
recordfilter , metadatafilter , codecfactory , allocator , properties , conf ) ;
if ( badrecordthresh ! = null ) {
boolean usedictionaryfilter ,
usesignedstringminmax , usestatsfilter , usedictionaryfilter , userecordfilter , recordfilter ,
import org . apache . parquet . format . converter . parquetmetadataconverter . metadatafilter ;
metadatafilter metadatafilter ,
import org . apache . parquet . bytes . bytebufferallocator ;
boolean usestatsfilter ,
import org . apache . parquet . compression . compressioncodecfactory ;
org . apache . parquet . schema . columnorder columnorder = fromparquetcolumnorder ( columnorders . get ( columncount ) ) ;
columnorders . add ( columnorder ) ;
case int32 :
if ( formatstats ! = null ) {
case binary :
return columnorders ;
if ( columnorder . issettype order ( ) ) {
return tostring ( genericgetmax ( ) ) ;
case fixed len byte array :
int columncount ) {
return comparator ;
}
private final primitivetype type ;
( sortordersmatch | | maxequalsmin ) ) {
messagetype fromparquetschema ( list < schemaelement > schema , list < columnorder > columnorders ) {
return type . equals ( stats . type ) & &
private list < columnorder > getcolumnorders ( messagetype schema ) {
else
} else {
( string createdby , statistics formatstats , primitivetype type , sortorder typesortorder ) {
if ( isset ) {
buildchildren ( builder , iterator , root . getnum children ( ) , columnorders , 0 ) ;
return new booleanstatistics ( primitive ) ;
org . apache . parquet . column . statistics . statistics stats = org . apache . parquet . column . statistics . statistics . createstats ( type ) ;
return new binarystatistics ( ) ;
+ long . valueof ( this . getnumnulls ( ) ) . hashcode ( ) ;
import org . apache . parquet . format . typedefinedorder ;
return string . format ( "num nulls : % d , min / max not defined" , this . getnumnulls ( ) ) ;
& & ( schemaelement . type = = type . int96 | | schemaelement . converted type = = convertedtype . interval ) ) {
org . apache . parquet . column . statistics . statistics stats ) {
byte [ ] max = stats . getmaxbytes ( ) ;
this . type = type ;
if ( !corruptstatistics . shouldignorestatistics ( createdby , type . getprimitivetypename ( ) ) & &
return formatstats ;
case int64 :
return 31 * type . hashcode ( ) + 31 * arrays . hashcode ( getmaxbytes ( ) ) + 17 * arrays . hashcode ( getminbytes ( ) )
arrays . equals ( stats . getminbytes ( ) , this . getminbytes ( ) ) & &
throw statisticsclassexception . create ( this , stats ) ;
boolean maxequalsmin = isset ? arrays . equals ( formatstats . getmin ( ) , formatstats . getmax ( ) ) : false ;
return new intstatistics ( ) ;
columnorder . settype order ( type defined order ) ;
return new doublestatistics ( primitive ) ;
if ( columnorder . getcolumnordername ( ) = = columnordername . type defined order
return new intstatistics ( primitive ) ;
@ deprecated
filemetadata . setcolumn orders ( getcolumnorders ( parquetmetadata . getfilemetadata ( ) . getschema ( ) ) ) ;
pageheader . getdata page header ( ) . setstatistics ( toparquetstatistics ( statistics ) ) ;
return org . apache . parquet . schema . columnorder . undefined ( ) ;
createdby , statistics , type , expectedorder ) ;
return type ;
private static final typedefinedorder type defined order = new typedefinedorder ( ) ;
private static org . apache . parquet . schema . columnorder fromparquetcolumnorder ( columnorder columnorder ) {
+ + columncount ;
return string . format ( "min : % s , max : % s , num nulls : % d" , minasstring ( ) , maxasstring ( ) , this . getnumnulls ( ) ) ;
return new longstatistics ( primitive ) ;
formatstats . setmin value ( min ) ;
switch ( primitive . getprimitivetypename ( ) ) {
case int96 :
private final primitivecomparator < t > comparator ;
if ( type . equals ( stats . type ) ) {
new primitivetype ( repetition . optional , type , "fake type" ) , defaultsortorder ( type ) ) ;
switch ( type ) {
for ( int i = 0 , n = schema . getpaths ( ) . size ( ) ; i < n ; + + i ) {
if ( columnorders ! = null ) {
return type . columnorder ( ) . getcolumnordername ( ) = = columnordername . type defined order ;
if ( stats . hasnonnullvalue ( ) ) {
buildchildren ( ( types . groupbuilder ) childbuilder , schema , schemaelement . num children , columnorders , columncount ) ;
case boolean :
stats . getnumnulls ( ) = = this . getnumnulls ( ) ;
columnorder columnorder = new columnorder ( ) ;
return tostring ( genericgetmin ( ) ) ;
byte [ ] min = formatstats . min value . array ( ) ;
import java . util . objects ;
case double :
public string tostring ( ) {
throw new unknowncolumntypeexception ( primitive . getprimitivetypename ( ) ) ;
return new floatstatistics ( primitive ) ;
public final int comparemaxtovalue ( t value ) {
formatstats . setmax ( max ) ;
int childrencount ,
import org . apache . parquet . schema . primitivetype ;
boolean sortordersmatch = sortorder . signed = = typesortorder ;
import org . apache . parquet . schema . columnorder . columnordername ;
else if ( !this . isempty ( ) )
return new binarystatistics ( primitive ) ;
arrays . equals ( stats . getmaxbytes ( ) , this . getmaxbytes ( ) ) & &
@ override
primitivetype primitive = type . asprimitivetype ( ) ;
if ( this . hasnonnullvalue ( ) )
import org . apache . parquet . schema . primitivecomparator ;
public primitivetype type ( ) {
boolean isset = formatstats . issetmax ( ) & & formatstats . issetmin ( ) ;
return new doublestatistics ( ) ;
return org . apache . parquet . schema . columnorder . typedefined ( ) ;
messagetype messagetype = fromparquetschema ( parquetmetadata . getschema ( ) , parquetmetadata . getcolumn orders ( ) ) ;
return new longstatistics ( ) ;
throw new unknowncolumntypeexception ( type ) ;
if ( isminmaxstatssupported ( type ) | | arrays . equals ( min , max ) ) {
string tostring ( t value ) {
primitivebuilder . columnorder ( columnorder ) ;
this . comparator = type . comparator ( ) ;
private static boolean isminmaxstatssupported ( primitivetype type ) {
default :
list < columnorder > columnorders = new arraylist < > ( ) ;
stats . setminmaxfrombytes ( min , max ) ;
columnorder = org . apache . parquet . schema . columnorder . undefined ( ) ;
if ( sortorder ( stats . type ( ) ) = = sortorder . signed | | arrays . equals ( min , max ) ) {
import org . apache . parquet . format . columnorder ;
byte [ ] min = stats . getminbytes ( ) ;
stats . setnumnulls ( formatstats . null count ) ;
return comparator . compare ( genericgetmin ( ) , value ) ;
statistics formatstats = new statistics ( ) ;
list < columnorder > columnorders ,
case float :
return objects . tostring ( value ) ;
public final int comparemintovalue ( t value ) {
formatstats . setmin ( min ) ;
import java . util . arrays ;
if ( formatstats . issetmin value ( ) & & formatstats . issetmax value ( ) ) {
statistics ( primitivetype type ) {
byte [ ] max = formatstats . max value . array ( ) ;
import org . apache . parquet . schema . type ;
public string minasstring ( ) {
return fromparquetstatisticsinternal ( createdby , statistics ,
public final primitivecomparator < t > comparator ( ) {
return comparator . compare ( genericgetmax ( ) , value ) ;
public string maxasstring ( ) {
public static statistics < ? > createstats ( type type ) {
messagetype . gettype ( path . toarray ( ) ) . asprimitivetype ( ) ,
public abstract statistics < t > copy ( ) ;
if ( isminmaxstatssupported ( stats . type ( ) ) | | arrays . equals ( min , max ) ) {
return "no stats for this column" ;
formatstats . setnull count ( stats . getnumnulls ( ) ) ;
if ( !stats . isempty ( ) & & stats . issmallerthan ( max stats size ) ) {
stats . setminmaxfrombytes ( formatstats . min . array ( ) , formatstats . max . array ( ) ) ;
formatstats . setmax value ( max ) ;
return new booleanstatistics ( ) ;
return new floatstatistics ( ) ;
}
org . apache . parquet . column . statistics . statistics . builder statsbuilder =
statsbuilder . withmin ( formatstats . min . array ( ) ) ;
if ( formatstats . issetnull count ( ) ) {
statsbuilder . withnumnulls ( formatstats . null count ) ;
statsbuilder . withmax ( formatstats . max . array ( ) ) ;
statsbuilder . withmin ( min ) ;
org . apache . parquet . column . statistics . statistics . getbuilder ( type ) ;
return statsbuilder . build ( ) ;
statsbuilder . withmax ( max ) ;
return logicaltypeannotation . decimaltype ( decimal . scale , decimal . precision ) ;
import org . apache . parquet . format . timeunit ;
element . setconverted type ( converttoconvertedtype ( grouptype . getlogicaltypeannotation ( ) ) ) ;
convertedtype = convertedtype . json ;
public void visit ( logicaltypeannotation . jsonlogicaltypeannotation logicaltypeannotation ) {
return logicaltypeannotation . timetype ( true , logicaltypeannotation . timeunit . micros ) ;
return logicaltypeannotation . listtype ( ) ;
convertedtype = convertedtype . timestamp millis ;
}
logicaltypeconvertervisitor logicaltypeconvertervisitor = new logicaltypeconvertervisitor ( ) ;
import org . apache . parquet . schema . logicaltypeannotation ;
import org . apache . parquet . format . maptype ;
int scale = ( schemaelement = = null ? 0 : schemaelement . scale ) ;
import org . apache . parquet . format . listtype ;
public void visit ( logicaltypeannotation . datelogicaltypeannotation logicaltypeannotation ) {
return logicaltypeconvertervisitor . convertedtype ;
import org . apache . parquet . format . timetype ;
return logicaltypeannotation . timestamptype ( true , logicaltypeannotation . timeunit . micros ) ;
logicaltype converttologicaltype ( logicaltypeannotation logicaltypeannotation ) {
case enum :
logicaltype = logicaltype . json ( new jsontype ( ) ) ;
case timestamp :
public void visit ( logicaltypeannotation . timelogicaltypeannotation logicaltypeannotation ) {
switch ( unit . getsetfield ( ) ) {
convertedtype = convertedtype . int 8 ;
timetype time = type . gettime ( ) ;
return logicaltypeannotation . inttype ( 64 , true ) ;
return logicaltypeannotation . timeunit . micros ;
case int 8 :
case decimal :
import org . apache . parquet . format . inttype ;
return logicaltypeannotation . inttype ( 32 , true ) ;
switch ( type . getsetfield ( ) ) {
import org . apache . parquet . format . stringtype ;
return logicaltypeannotation . enumtype ( ) ;
return logicaltypeannotation . timetype ( true , logicaltypeannotation . timeunit . millis ) ;
convertedtype = convertedtype . interval ;
case integer :
logicaltypeannotation getoriginaltype ( logicaltype type ) {
convertedtype = convertedtype . time micros ;
case bson :
return logicaltypeannotation . inttype ( integer . bitwidth , integer . issigned ) ;
logicaltype = logicaltype . map ( new maptype ( ) ) ;
return logicaltypeannotation . jsontype ( ) ;
return logicaltypeannotation . inttype ( 64 , false ) ;
case time micros :
convertedtype = convertedtype . date ;
throw new runtimeexception ( "unknown logical type " + type ) ;
case int 32 :
return logicaltypeannotation . inttype ( 8 , false ) ;
import org . apache . parquet . format . datetype ;
element . setconverted type ( converttoconvertedtype ( primitivetype . getlogicaltypeannotation ( ) ) ) ;
logicaltype = logicaltype . date ( new datetype ( ) ) ;
return logicaltypeannotation . stringtype ( ) ;
convertedtype = convertedtype . map ;
logicaltype = logicaltype . decimal ( new decimaltype ( logicaltypeannotation . getscale ( ) , logicaltypeannotation . getprecision ( ) ) ) ;
convertedtype = convertedtype . bson ;
private logicaltypeannotation . timeunit converttimeunit ( timeunit unit ) {
import org . apache . parquet . format . milliseconds ;
throw new runtimeexception ( "unknown original type " + logicaltypeannotation . tooriginaltype ( ) ) ;
public void visit ( logicaltypeannotation . bsonlogicaltypeannotation logicaltypeannotation ) {
return logicaltypeannotation . bsontype ( ) ;
timestamptype timestamp = type . gettimestamp ( ) ;
logicaltype = logicaltype . bson ( new bsontype ( ) ) ;
convertedtype = convertedtype . int 32 ;
childbuilder . as ( getoriginaltype ( schemaelement . converted type , schemaelement ) ) ;
public void visit ( logicaltypeannotation . maplogicaltypeannotation logicaltypeannotation ) {
logicaltype = logicaltype . list ( new listtype ( ) ) ;
return logicaltypeannotation . intervallogicaltypeannotation . getinstance ( ) ;
case time :
element . setlogicaltype ( converttologicaltype ( grouptype . getlogicaltypeannotation ( ) ) ) ;
logicaltype = logicaltype . timestamp ( new timestamptype ( logicaltypeannotation . isadjustedtoutc ( ) , convertunit ( logicaltypeannotation . getunit ( ) ) ) ) ;
return logicaltypeannotation . inttype ( 16 , false ) ;
break ;
logicaltype = logicaltype . enum ( new enumtype ( ) ) ;
return logicaltypeannotation . timestamptype ( timestamp . isadjustedtoutc , converttimeunit ( timestamp . unit ) ) ;
convertedtype = convertedtype . timestamp micros ;
case uint 16 :
import org . apache . parquet . format . nulltype ;
import org . apache . parquet . format . decimaltype ;
childbuilder . as ( getoriginaltype ( schemaelement . logicaltype ) ) ;
convertedtype = convertedtype . uint 64 ;
import org . apache . parquet . format . jsontype ;
case time millis :
decimaltype decimal = type . getdecimal ( ) ;
return logicaltypeannotation . timetype ( time . isadjustedtoutc , converttimeunit ( time . unit ) ) ;
private logicaltype logicaltype ;
return logicaltypeannotation . inttype ( 8 , true ) ;
import org . apache . parquet . format . timestamptype ;
import org . apache . parquet . format . logicaltype ;
convertedtype = convertedtype . utf8 ;
case int 16 :
return org . apache . parquet . format . timeunit . millis ( new milliseconds ( ) ) ;
case micros :
public void visit ( logicaltypeannotation . timestamplogicaltypeannotation logicaltypeannotation ) {
case timestamp micros :
throw new runtimeexception ( "unknown time unit " + unit ) ;
public void visit ( logicaltypeannotation . intlogicaltypeannotation logicaltypeannotation ) {
convertedtype = convertedtype . decimal ;
case uint 32 :
case int 64 :
logicaltypeannotation originaltype = getoriginaltype ( schemaelement . converted type , schemaelement ) ;
private convertedtype convertedtype ;
return logicaltypeannotation . timeunit . millis ;
private static class logicaltypeconvertervisitor implements logicaltypeannotation . logicaltypeannotationvisitor {
int precision = ( schemaelement = = null ? 0 : schemaelement . precision ) ;
return org . apache . parquet . format . timeunit . micros ( new microseconds ( ) ) ;
convertedtype = convertedtype . uint 32 ;
case timestamp millis :
return logicaltypeannotation . mapkeyvaluetypeannotation . getinstance ( ) ;
if ( grouptype . getlogicaltypeannotation ( ) ! = null ) {
throw new runtimeexception ( "unknown converted type for " + logicaltypeannotation . tooriginaltype ( ) ) ;
logicaltype = logicaltype . string ( new stringtype ( ) ) ;
return logicaltypeconvertervisitor . logicaltype ;
import org . apache . parquet . format . bsontype ;
@ override
case unknown :
return null ;
case millis :
logicaltypeannotation getoriginaltype ( convertedtype type , schemaelement schemaelement ) {
convertedtype = convertedtype . uint 16 ;
switch ( logicaltypeannotation . tooriginaltype ( ) ) {
case list :
public void visit ( logicaltypeannotation . mapkeyvaluetypeannotation logicaltypeannotation ) {
return logicaltypeannotation . inttype ( 32 , false ) ;
case string :
convertedtype = convertedtype . uint 8 ;
return logicaltypeannotation . decimaltype ( scale , precision ) ;
case uint 8 :
return logicaltypeannotation . datetype ( ) ;
convertedtype = convertedtype . int 64 ;
if ( primitivetype . getlogicaltypeannotation ( ) ! = null ) {
convertedtype = convertedtype . list ;
switch ( unit ) {
import org . apache . parquet . format . enumtype ;
inttype integer = type . getinteger ( ) ;
element . setlogicaltype ( converttologicaltype ( primitivetype . getlogicaltypeannotation ( ) ) ) ;
convertedtype = convertedtype . enum ;
logicaltype = logicaltype . time ( new timetype ( logicaltypeannotation . isadjustedtoutc ( ) , convertunit ( logicaltypeannotation . getunit ( ) ) ) ) ;
default :
return logicaltypeannotation . maptype ( ) ;
convertedtype = convertedtype . map key value ;
logicaltypeannotation . accept ( logicaltypeconvertervisitor ) ;
public void visit ( logicaltypeannotation . decimallogicaltypeannotation logicaltypeannotation ) {
convertedtype = convertedtype . time millis ;
return logicaltypeannotation . inttype ( 16 , true ) ;
public void visit ( logicaltypeannotation . stringlogicaltypeannotation logicaltypeannotation ) {
throw new runtimeexception ( "can't convert converted type to logical type , unknown converted type " + type ) ;
import org . apache . parquet . format . microseconds ;
case uint 64 :
return logicaltypeannotation . timestamptype ( true , logicaltypeannotation . timeunit . millis ) ;
public void visit ( logicaltypeannotation . intervallogicaltypeannotation logicaltypeannotation ) {
if ( !originaltype . equals ( newlogicaltype ) ) {
public void visit ( logicaltypeannotation . listlogicaltypeannotation logicaltypeannotation ) {
convertedtype converttoconvertedtype ( logicaltypeannotation logicaltypeannotation ) {
logicaltype = logicaltype . unknown ( new nulltype ( ) ) ;
public void visit ( logicaltypeannotation . enumlogicaltypeannotation logicaltypeannotation ) {
logicaltypeannotation newlogicaltype = getoriginaltype ( schemaelement . logicaltype ) ;
logicaltype = logicaltype . integer ( new inttype ( ( byte ) logicaltypeannotation . getbitwidth ( ) , logicaltypeannotation . issigned ( ) ) ) ;
if ( schemaelement . issetlogicaltype ( ) ) {
convertedtype = convertedtype . int 16 ;
static org . apache . parquet . format . timeunit convertunit ( logicaltypeannotation . timeunit unit ) {
}
private static int transpositions ( charsequence first , charsequence second ) {
return transpositions ;
for ( int j = math . max ( 0 , i - limit ) ; !found & & j < math . min ( i + limit , second . length ( ) ) ; j + + ) {
transpositions + + ;
if ( copy . charat ( j ) = = ch ) {
transpositions / = 2 ;
int transpositions = 0 ;
private static string getsetofmatchingcharacterwithin ( final charsequence first , final charsequence second , final int limit ) {
if ( first . charat ( i ) ! = second . charat ( i ) ) {
public static double getjarowinklerdistance ( final charsequence first , final charsequence second ) {
for ( int i = 0 ; i < first . length ( ) ; i + + ) {
return transpositions / 2 ;
final int result = getcommonprefix ( first . tostring ( ) , second . tostring ( ) ) . length ( ) ;
if ( sz > start + 1 & & chars [ start ] = = '0'
)
(
( chars [ start + 1 ] = = 'x' )
& &
( chars [ start + 1 ] = = 'x' ) | |
) {
}
protected fastdateparser ( final string pattern , final timezone timezone , final locale locale ) {
this ( pattern , timezone , locale , null ) ;
}
if ( m1 . length ( ) ! = m2 . length ( ) ) {
if ( m1 . length ( ) = = 0 | | m2 . length ( ) = = 0 ) {
return 0 . 0 ;
long [ ] lvalues = arrayutils . clone ( validate . notnull ( values ) ) ;
validate . istrue ( constants . length < = long . size , cannot store s s values in s bits ,
arrayutils . reverse ( lvalues ) ;
public static < e extends enum < e > > enumset < e > processbitvectors ( final class < e > enumclass , final long . . . values ) {
integer . valueof ( constants . length ) , enumclass . getsimplename ( ) , integer . valueof ( long . size ) ) ;
if ( block < lvalues . length & & ( lvalues [ block ] & 1 < < ( constant . ordinal ( ) % long . size ) ) ! = 0 ) {
public static string tostring ( final type type ) {
private static boolean equals ( final type [ ] t1 , final type [ ] t2 ) {
private static string genericarraytypetostring ( final genericarraytype g ) {
public boolean equals ( final object obj ) {
private static string parameterizedtypetostring ( final parameterizedtype p ) {
private static boolean equals ( final parameterizedtype p , final type t ) {
private wildcardtypeimpl ( final type [ ] upperbounds , final type [ ] lowerbounds ) {
public wildcardtypebuilder withlowerbounds ( final type . . . bounds ) {
private static string typevariabletostring ( final typevariable < ? > v ) {
public static string wrap ( final string str , final char wrapwith ) {
final
private static boolean equals ( final wildcardtype w , final type t ) {
final wildcardtype wild = ( wildcardtype ) type ;
public static boolean isanyblank ( final charsequence . . . css ) {
public static boolean isnoneempty ( final charsequence . . . css ) {
for ( final typevariable < ? > var : variables ) {
public wildcardtypebuilder withupperbounds ( final type . . . bounds ) {
public static boolean isanyempty ( final charsequence . . . css ) {
private static stringbuilder appendallto ( final stringbuilder buf , final string sep , final type . . . types ) {
final char querychar = querylowercase . charat ( queryindex ) ;
public static string toencodedstring ( final byte [ ] bytes , final charset charset ) {
private genericarraytypeimpl ( final type componenttype ) {
private static string wildcardtypetostring ( final wildcardtype w ) {
for ( final type arg : ( ( parameterizedtype ) type ) . getactualtypearguments ( ) ) {
private static string classtostring ( final class < ? > c ) {
public static boolean equals ( final type t1 , final type t2 ) {
private parameterizedtypeimpl ( final class < ? > raw , final type useowner , final type [ ] typearguments ) {
private static int transpositions ( final charsequence first , final charsequence second ) {
private static int commonprefixlength ( final charsequence first , final charsequence second ) {
public static string wrap ( final string str , final string wrapwith ) {
public static boolean isnoneblank ( final charsequence . . . css ) {
public static string tolongstring ( final typevariable < ? > var ) {
private static type [ ] extracttypeargumentsfrom ( final map < typevariable < ? > , type > mappings , final typevariable < ? > [ ] variables ) {
final char firstchar = str . charat ( 0 ) ;
public static boolean containstypevariables ( final type type ) {
final char termchar = termlowercase . charat ( termindex ) ;
for ( final charsequence cs : css ) {
private static boolean equals ( final genericarraytype a , final type t ) {
final int size = str . length ( ) ;
return new string ( newchars , 0 , count - ( whitespacescount > 0 ? 1 : 0 ) ) ;
}
whitespacescount = 0 ;
if ( isempty ( str ) ) {
boolean startwhitespaces = true ;
} else {
final char [ ] newchars = new char [ size ] ;
whitespacescount + + ;
boolean iswhitespace = character . iswhitespace ( actualchar ) ;
int count = 0 ;
startwhitespaces = false ;
newchars [ count + + ] = ( actualchar = = 160 ? 32 : actualchar ) ;
int whitespacescount = 0 ;
char actualchar = str . charat ( i ) ;
return empty ;
if ( startwhitespaces ) {
return str ;
if ( !iswhitespace ) {
if ( whitespacescount = = 0 & & !startwhitespaces ) {
newchars [ count + + ] = space . charat ( 0 ) ;
for ( int i = 0 ; i < size ; i + + ) {
buffer . append ( reflectiontostringbuilder . tostring ( value , this ) ) ;
super ( ) ;
spaces + = indent ;
if ( !classutils . isprimitivewrapper ( value . getclass ( ) ) & & !string . class . equals ( value . getclass ( ) )
protected void appenddetail ( final stringbuffer buffer , final string fieldname , final long [ ] array ) {
}
super . appenddetail ( buffer , fieldname , array ) ;
setarrayend ( systemutils . line separator + spacer ( spaces - indent ) + " } " ) ;
spaces - = indent ;
protected void appenddetail ( final stringbuffer buffer , final string fieldname , final boolean [ ] array ) {
setfieldseparator ( " , " + systemutils . line separator + spacer ( spaces ) ) ;
public void appenddetail ( stringbuffer buffer , string fieldname , object value ) {
} else {
& & accept ( value . getclass ( ) ) ) {
for ( int i = 0 ; i < spaces ; i + + ) {
class multilinerecursivetostringstyle extends recursivetostringstyle {
protected void appenddetail ( final stringbuffer buffer , final string fieldname , final int [ ] array ) {
package org . apache . commons . lang3 . builder ;
protected void appenddetail ( final stringbuffer buffer , final string fieldname , final char [ ] array ) {
private stringbuilder spacer ( int spaces ) {
private static final long serialversionuid = 1l ;
setcontentend ( systemutils . line separator + spacer ( spaces - indent ) + " ] " ) ;
private int indent = 2 ;
resetindent ( ) ;
protected void appenddetail ( final stringbuffer buffer , final string fieldname , final double [ ] array ) {
super . appenddetail ( buffer , fieldname , value ) ;
setcontentstart ( " [ " + systemutils . line separator + spacer ( spaces ) ) ;
protected void appenddetail ( final stringbuffer buffer , final string fieldname , final byte [ ] array ) {
@ override
protected void reflectionappendarraydetail ( final stringbuffer buffer , final string fieldname , final object array ) {
stringbuilder sb = new stringbuilder ( ) ;
protected void appenddetail ( final stringbuffer buffer , final string fieldname , final object [ ] array ) {
setarrayseparator ( " , " + systemutils . line separator + spacer ( spaces ) ) ;
import org . apache . commons . lang3 . systemutils ;
import org . apache . commons . lang3 . classutils ;
sb . append ( " " ) ;
protected void appenddetail ( final stringbuffer buffer , final string fieldname , final float [ ] array ) {
private void resetindent ( ) {
public multilinerecursivetostringstyle ( ) {
return sb ;
setarraystart ( " { " + systemutils . line separator + spacer ( spaces ) ) ;
protected void appenddetail ( final stringbuffer buffer , final string fieldname , final short [ ] array ) {
private int spaces = 2 ;
index = charsequenceutils . indexof ( str , searchstr , index + searchstr . length ( ) ) ;
index = charsequenceutils . lastindexof ( str , searchstr , index - searchstr . length ( ) ) ;
super ( ) ;
super . append ( buffer , fieldname , value , fulldetail ) ;
private string field name prefix = " \ "" ;
public void append ( stringbuffer buffer , string fieldname , char [ ] array ,
this . setcontentstart ( " { " ) ;
buffer . append ( value ) ;
public static final tostringstyle json style = new jsontostringstyle ( ) ;
short [ ] array , boolean fulldetail ) {
}
throw new unsupportedoperationexception (
this . setsizestarttext ( " \ " < size = " ) ;
if ( value . getclass ( ) = = string . class ) {
this . setfieldseparator ( " , " ) ;
boolean fulldetail ) {
"field names are mandatory when using jsontostringstyle" ) ;
object [ ] array , boolean fulldetail ) {
private void appendvalueasstring ( stringbuffer buffer , string value ) {
this . setarraystart ( " [ " ) ;
private object readresolve ( ) {
this . setuseclassname ( false ) ;
public void append ( stringbuffer buffer , string fieldname , object value ,
public void append ( stringbuffer buffer , string fieldname , long [ ] array ,
boolean [ ] array , boolean fulldetail ) {
appendnulltext ( buffer , fieldname ) ;
appendvalueasstring ( buffer , ( string ) value ) ;
double [ ] array , boolean fulldetail ) {
this . setarrayend ( " ] " ) ;
this . setsummaryobjectendtext ( " > \ "" ) ;
this . setfieldnamevalueseparator ( " : " ) ;
if ( !isfulldetail ( fulldetail ) ) {
public void append ( stringbuffer buffer , string fieldname , int [ ] array ,
if ( value = = null ) {
protected void appendfieldstart ( stringbuffer buffer , string fieldname ) {
private static final long serialversionuid = 1l ;
this . setuseidentityhashcode ( false ) ;
super . append ( buffer , fieldname , array , fulldetail ) ;
float [ ] array , boolean fulldetail ) {
public void append ( stringbuffer buffer , string fieldname ,
private static final class jsontostringstyle extends tostringstyle {
if ( fieldname = = null ) {
buffer . append ( " \ "" + value + " \ "" ) ;
return ;
this . setsizeendtext ( " > \ "" ) ;
super . appendfieldstart ( buffer , field name prefix + fieldname
return tostringstyle . json style ;
this . setnulltext ( "null" ) ;
protected void appenddetail ( stringbuffer buffer , string fieldname , object value ) {
jsontostringstyle ( ) {
@ override
this . setsummaryobjectstarttext ( " \ " < " ) ;
public void append ( stringbuffer buffer , string fieldname , byte [ ] array ,
+ field name prefix ) ;
"fulldetail must be true when using jsontostringstyle" ) ;
this . setcontentend ( " } " ) ;
} catch ( final securityexception ex ) {
@ suppresswarnings ( "unchecked" )
enumeration < string > propertynames = ( enumeration < string > ) input . propertynames ( ) ;
systemproperties = system . getproperties ( ) ;
import java . util . enumeration ;
}
properties systemproperties = null ;
while ( propertynames . hasmoreelements ( ) ) {
final map < string , string > propertiesmap = ( map ) properties ;
if ( input = = null ) {
private static final strlookup < string > none lookup = new mapstrlookup < string > ( null ) ;
private static properties copyproperties ( properties input ) {
properties properties = copyproperties ( systemproperties ) ;
output . setproperty ( propertyname , input . getproperty ( propertyname ) ) ;
return new mapstrlookup < string > ( propertiesmap ) ;
import java . util . properties ;
return null ;
properties output = new properties ( ) ;
string propertyname = propertynames . nextelement ( ) ;
return output ;
try {
}
appenddigits ( buffer , value ) ;
public final void appendto ( final stringbuffer buffer , int value ) {
buffer . append ( '0' ) ;
for ( ; value > 0 ; value / = 10 ) {
for ( int digit = 0 ; digit < msize ; + + digit ) {
return msize ;
buffer . setcharat ( - - index , ( char ) ( '0' + value % 10 ) ) ;
buffer . append ( value ) ;
int index = buffer . length ( ) ;
final int max = j > integer . max value - threshold ? n : math . min ( n , j + threshold ) ;
final int halflength = shorter . length ( ) / 2 + 1 ;
private static final string rfc 822 time zone = " [ + - ] \ \ d { 4 } " ;
}
private static final string valid tz = " ( ( ? iu ) " + rfc 822 time zone + " | " + gmt option + " | " + tz database + " ) " ;
private static final string gmt option = "gmt [ + - ] \ \ d { 1 , 2 } : \ \ d { 2 } " ;
regex . append ( valid tz ) ;
else if ( value . regionmatches ( true , 0 , "gmt" , 0 , 3 ) ) {
static class timezonestrategy extends strategy {
tz = timezone . gettimezone ( value . touppercase ( ) ) ;
static final string tz database = " ( ? : \ \ p { l } [ \ \ p { l } \ \ p { mc } \ \ p { nd } \ \ p { zs } \ \ p { p } & & [ ^ - ] ] * - ? \ \ p { zs } ? ) * " ;
str . getchars ( 1 , strlen , newchars , 1 ) ;
final char newchar = character . touppercase ( firstchar ) ;
char [ ] newchars = new char [ strlen ] ;
newchars [ 0 ] = newchar ;
if ( firstchar = = newchar ) {
return string . valueof ( newchars ) ;
final char newchar = character . tolowercase ( firstchar ) ;
}
private boolean isjsonobject ( string valueasstring ) {
& & valueasstring . endswith ( getcontentend ( ) ) ;
return valueasstring . startswith ( getcontentstart ( ) )
if ( isjsonobject ( valueasstring ) | | isjsonarray ( valueasstring ) ) {
& & valueasstring . startswith ( getarrayend ( ) ) ;
private boolean isjsonarray ( string valueasstring ) {
return valueasstring . startswith ( getarraystart ( ) )
this . typearguments = typearguments . clone ( ) ;
final stringbuilder sb = new stringbuilder ( ) ;
switch ( c ) {
continue ;
private final map < string , timezone > tznames = new hashmap < string , timezone > ( ) ;
private static stringbuilder simplequote ( final stringbuilder sb , final string value ) {
final timezone tz = timezone . gettimezone ( tzid ) ;
if ( !tznames . containskey ( zonename ) ) {
final string tzid = zonenames [ id ] ;
case ' . ' :
}
default :
private final string validtimezonechars ;
sb . append ( ' ) ' ) ;
sb . append ( ' \ \ ' ) ;
sb . append ( ' ( ' + rfc 822 time zone + " | ( ? iu ) " + gmt option ) ;
tznames . put ( zonename , tz ) ;
case ' * ' :
if ( tzid . equalsignorecase ( "gmt" ) ) {
case ' | ' :
this . locale = locale ;
case ' ) ' :
case ' [ ' :
tz = tznames . get ( value . tolowercase ( locale ) ) ;
regex . append ( validtimezonechars ) ;
case ' ( ' :
case ' \ \ ' :
for ( int i = 1 ; i < zonenames . length ; + + i ) {
case '$' :
validtimezonechars = sb . tostring ( ) ;
simplequote ( regex , textkeyvalue ) . append ( ' | ' ) ;
string zonename = zonenames [ i ] . tolowercase ( locale ) ;
case ' ^ ' :
case ' + ' :
simplequote ( sb . append ( ' | ' ) , zonename ) ;
sb . append ( c ) ;
private final locale locale ;
for ( final string [ ] zonenames : zones ) {
for ( int i = 0 ; i < value . length ( ) ; + + i ) {
case ' { ' :
case ' ? ' :
return sb ;
char c = value . charat ( i ) ;
private abstract static class statestrategy {
}
} while ( !updatecheckintervaldata ( currentdata , nextdata ) ) ;
return breaker . getopeninginterval ( ) ;
public eventcountcircuitbreaker ( int openingthreshold , long openinginterval ,
public long getopeninginterval ( ) {
private static final map < state , statestrategy > strategy map = createstrategymap ( ) ;
checkunit ) ;
private final int eventcount ;
state currentstate ;
public eventcountcircuitbreaker ( int threshold , long checkinterval , timeunit checkunit ) {
throws circuitbreakingexception {
} else {
public boolean incrementandcheckstate ( ) {
private final long closinginterval ;
return nextdata . geteventcount ( ) > breaker . getopeningthreshold ( ) ;
private final int openingthreshold ;
protected long fetchcheckinterval ( eventcountcircuitbreaker breaker ) {
this ( threshold , checkinterval , checkunit , threshold ) ;
private final int closingthreshold ;
return currentdata = = nextdata
import java . util . enummap ;
public checkintervaldata ( int count , long intervalstart ) {
return nextdata ;
long now ( ) {
import java . util . map ;
return incrementandcheckstate ( 1 ) ;
super . close ( ) ;
private static class checkintervaldata {
public boolean ischeckintervalfinished ( eventcountcircuitbreaker breaker ,
checkintervalstart = intervalstart ;
if ( statestrategy ( currentstate ) . isstatetransition ( this , currentdata , nextdata ) ) {
map . put ( state . open , new statestrategyopen ( ) ) ;
private final atomicreference < checkintervaldata > checkintervaldata ;
checkintervaldata = new atomicreference < checkintervaldata > ( new checkintervaldata ( 0 , 0 ) ) ;
currentstate = state . get ( ) ;
public void open ( ) {
timeunit closingunit ) {
public boolean isstatetransition ( eventcountcircuitbreaker breaker ,
this . openingthreshold = openingthreshold ;
currentdata = checkintervaldata . get ( ) ;
this . openinginterval = openingunit . tonanos ( openinginterval ) ;
changestate ( newstate ) ;
public int geteventcount ( ) {
long time = now ( ) ;
public void close ( ) {
public boolean incrementandcheckstate ( integer increment )
package org . apache . commons . lang3 . concurrent ;
public class eventcountcircuitbreaker extends abstractcircuitbreaker < integer > {
public long getclosinginterval ( ) {
private boolean performstatecheck ( int increment ) {
& & currentdata . geteventcount ( ) < breaker . getclosingthreshold ( ) ;
private static class statestrategyclosed extends statestrategy {
return system . nanotime ( ) ;
super . open ( ) ;
currentstate = currentstate . oppositestate ( ) ;
checkintervaldata . set ( new checkintervaldata ( 0 , now ( ) ) ) ;
map . put ( state . closed , new statestrategyclosed ( ) ) ;
. getcheckintervalstart ( )
super ( ) ;
| | checkintervaldata . compareandset ( currentdata , nextdata ) ;
private static statestrategy statestrategy ( state state ) {
return eventcount ;
private static class statestrategyopen extends statestrategy {
checkintervaldata currentdata , long now ) {
statestrategy strategy = strategy map . get ( state ) ;
checkintervaldata currentdata ;
return breaker . getclosinginterval ( ) ;
private static map < state , statestrategy > createstrategymap ( ) {
checkintervaldata currentdata , checkintervaldata nextdata ) ;
public int getclosingthreshold ( ) {
return openinginterval ;
this ( openingthreshold , checkinterval , checkunit , closingthreshold , checkinterval ,
return now - currentdata . getcheckintervalstart ( ) > fetchcheckinterval ( breaker ) ;
protected abstract long fetchcheckinterval ( eventcountcircuitbreaker breaker ) ;
return ( delta ! = 0 ) ? new checkintervaldata ( geteventcount ( ) + delta ,
public long getcheckintervalstart ( ) {
nextdata = nextcheckintervaldata ( increment , currentdata , currentstate , time ) ;
public abstract boolean isstatetransition ( eventcountcircuitbreaker breaker ,
@ override
do {
public checkintervaldata increment ( int delta ) {
return strategy ;
private checkintervaldata nextcheckintervaldata ( int increment ,
import java . util . concurrent . atomic . atomicreference ;
private void changestateandstartnewcheckinterval ( state newstate ) {
private final long openinginterval ;
return checkintervalstart ;
checkintervaldata nextdata ) {
return performstatecheck ( 1 ) ;
timeunit openingunit , int closingthreshold , long closinginterval ,
return openingthreshold ;
return nextdata . getcheckintervalstart ( ) ! = currentdata
this . closinginterval = closingunit . tonanos ( closinginterval ) ;
map < state , statestrategy > map = new enummap < state , statestrategy > ( state . class ) ;
this . closingthreshold = closingthreshold ;
public eventcountcircuitbreaker ( int openingthreshold , long checkinterval , timeunit checkunit ,
getcheckintervalstart ( ) ) : this ;
public boolean checkstate ( ) {
private final long checkintervalstart ;
return performstatecheck ( 0 ) ;
eventcount = count ;
return closingthreshold ;
import java . util . concurrent . timeunit ;
public int getopeningthreshold ( ) {
nextdata = new checkintervaldata ( increment , time ) ;
checkintervaldata nextdata ;
return map ;
if ( statestrategy ( currentstate ) . ischeckintervalfinished ( this , currentdata , time ) ) {
int closingthreshold ) {
checkintervaldata currentdata , state currentstate , long time ) {
checkintervaldata currentdata , checkintervaldata nextdata ) {
return !isopen ( currentstate ) ;
return closinginterval ;
nextdata = currentdata . increment ( increment ) ;
changestateandstartnewcheckinterval ( currentstate ) ;
private boolean updatecheckintervaldata ( checkintervaldata currentdata ,
private transient list < strategyandwidth > patterns ;
pos . seterrorindex ( pos . getindex ( ) ) ;
private static class iso8601timezonestrategy extends patternstrategy {
return v ;
import java . util . treeset ;
return false ;
if ( !matcher . lookingat ( ) ) {
int modify ( fastdateparser parser , final int ivalue ) {
pos . seterrorindex ( idx ) ;
if ( !character . isdigit ( c ) ) {
regex . append ( " ) " ) ;
for ( ; ; ) {
}
int width = currentidx - begin ;
final int width ;
strategyandwidth pattern = lt . next ( ) ;
patterns = new arraylist < strategyandwidth > ( ) ;
int value = integer . parseint ( source . substring ( pos . getindex ( ) , idx ) ) ;
} else {
int sidx = idx + pos . getindex ( ) ;
return width > = 3 ? getlocalespecificstrategy ( calendar . month , definingcalendar ) : number month strategy ;
stringbuilder regex = new stringbuilder ( ) ;
if ( !character . iswhitespace ( c ) ) {
if ( sidx = = source . length ( ) ) {
public boolean parse ( final string source , final parseposition pos , final calendar calendar ) {
if ( values . put ( symbol . tolowercase ( locale ) , entry . getvalue ( ) ) = = null ) {
else if ( c = = ' \ '' ) {
private static class strategyandwidth {
if ( v! = 0 ) {
while ( currentidx < pattern . length ( ) ) {
import java . util . comparator ;
setcalendar ( parser , calendar , matcher . group ( 1 ) ) ;
listiterator < strategyandwidth > lt = patterns . listiterator ( ) ;
strategyandwidth field = fm . getnextstrategy ( ) ;
return literal ( ) ;
string zonename = zonenames [ i ] ;
return 0 ;
simplequote ( regex , symbol ) . append ( ' | ' ) ;
return parse ( source , pos , cal ) ? cal . gettime ( ) : null ;
abstract boolean parse ( fastdateparser parser , calendar calendar , string source , parseposition pos , int maxwidth ) ;
boolean isnumber ( ) {
strategyandwidth ( strategy strategy , int width ) {
int getmaxwidth ( listiterator < strategyandwidth > lt ) {
createpattern ( pattern ) ;
if ( symbol . length ( ) > 0 ) {
private int currentidx ;
final locale locale ;
private pattern pattern ;
if ( !activequote & & isformatletter ( c ) ) {
string formatfield = sb . tostring ( ) ;
? new timezonestrategy ( definingcalendar , locale )
while ( lt . hasnext ( ) ) {
treeset < map . entry < string , integer > > sort = new treeset < map . entry < string , integer > > ( alternatives ordering ) ;
if ( activequote ) {
protected fastdateparser ( final string pattern , final timezone timezone , final locale locale , final date centurystart ) {
final date date = parse ( source , pp ) ;
patterns . add ( field ) ;
if ( pattern . charat ( currentidx ) ! = c ) {
return right . getkey ( ) . comparetoignorecase ( left . getkey ( ) ) ;
sb . append ( " ) " ) ;
import java . util . set ;
import java . util . listiterator ;
return true ;
if ( tznames . put ( zonename . tolowercase ( locale ) , tz ) = = null ) {
final private string pattern ;
timezonestrategy ( calendar cal , final locale locale ) {
string symbol = entry . getkey ( ) ;
sort . addall ( displaynames ) ;
return new strategyandwidth ( new copyquotedstrategy ( formatfield ) , formatfield . length ( ) ) ;
lt . previous ( ) ;
if ( + + currentidx = = pattern . length ( ) | | pattern . charat ( currentidx ) ! = ' \ '' ) {
private class strategyparser {
void createpattern ( stringbuilder regex ) {
return iso8601timezonestrategy . getstrategy ( width ) ;
else {
strategyparser ( string pattern , calendar definingcalendar ) {
pos . setindex ( idx ) ;
if ( last > end ) {
if ( maxwidth = = 0 ) {
static class timezonestrategy extends patternstrategy {
if ( width = = 2 ) {
+ + currentidx ;
return ivalue < 100 ? parser . adjustyear ( ivalue ) : ivalue ;
boolean parse ( fastdateparser parser , calendar calendar , string source , parseposition pos , int maxwidth ) {
if ( !strategy . isnumber ( ) | | !lt . hasnext ( ) ) {
if ( currentidx > = pattern . length ( ) ) {
int idx = pos . getindex ( ) ;
if ( field = = null ) {
int end = idx + maxwidth ;
for ( ; idx < last ; + + idx ) {
sb . append ( c ) ;
break ;
this ( pattern , timezone , locale , null ) ;
private static boolean isformatletter ( char c ) {
pos . setindex ( pos . getindex ( ) + matcher . end ( 1 ) ) ;
createpattern ( regex ) ;
createpattern ( regex . tostring ( ) ) ;
private final map < string , integer > lkeyvalues = new hashmap < string , integer > ( ) ;
return nextstrategy . isnumber ( ) ? width : 0 ;
return letterpattern ( c ) ;
void setcalendar ( fastdateparser parser , calendar cal , string value ) {
throw new illegalargumentexception ( "unterminated quote" ) ;
public int compare ( map . entry < string , integer > left , map . entry < string , integer > right ) {
throw new parseexception ( "unparseable date : " + source , pp . geterrorindex ( ) ) ;
this . pattern = pattern . compile ( regex ) ;
private static void appenddisplaynames ( calendar cal , locale locale , int field ,
activequote = !activequote ;
int begin = currentidx ;
pos . setindex ( formatfield . length ( ) + pos . getindex ( ) ) ;
final private string formatfield ;
for ( map . entry < string , integer > entry : sort ) {
matcher matcher = pattern . matcher ( source . substring ( pos . getindex ( ) ) ) ;
private static abstract class strategy {
strategy nextstrategy = lt . next ( ) . strategy ;
this . locale = locale ;
this . pattern = pattern ;
char c = source . charat ( idx ) ;
import java . util . map . entry ;
"unparseable date : \ "" + source , pp . geterrorindex ( ) ) ;
if ( isformatletter ( c ) ) {
this . formatfield = formatfield ;
for ( int idx = 0 ; idx < formatfield . length ( ) ; + + idx ) {
@ override
this . width = width ;
return null ;
int v = left . getvalue ( ) - right . getvalue ( ) ;
private static final comparator < map . entry < string , integer > > alternatives ordering = new comparator < map . entry < string , integer > > ( ) {
private strategyandwidth letterpattern ( char c ) {
stringbuilder sb = new stringbuilder ( ) ;
boolean activequote = false ;
void createpattern ( string regex ) {
private strategy getstrategy ( char f , int width , final calendar definingcalendar ) {
strategyparser fm = new strategyparser ( pattern , definingcalendar ) ;
createpattern ( sb ) ;
final private calendar definingcalendar ;
appenddisplaynames ( definingcalendar , locale , field , regex , lkeyvalues ) ;
calendar . set ( field , modify ( parser , value ) ) ;
abstract void setcalendar ( fastdateparser parser , calendar cal , string value ) ;
this . definingcalendar = definingcalendar ;
continue ;
private strategyandwidth literal ( ) {
throw new illegalargumentexception ( "format '" + f + "' not supported" ) ;
last = end ;
int maxwidth = pattern . getmaxwidth ( lt ) ;
stringbuilder regex , map < string , integer > values ) {
while ( + + currentidx < pattern . length ( ) ) {
if ( !pattern . strategy . parse ( this , calendar , source , pos , maxwidth ) ) {
char c = pattern . charat ( currentidx ) ;
final strategy strategy ;
this . field = field ;
switch ( f ) {
if ( formatfield . charat ( idx ) ! = source . charat ( sidx ) ) {
if ( pos . getindex ( ) = = idx ) {
parseposition pp = new parseposition ( 0 ) ;
regex . setlength ( regex . length ( ) - 1 ) ;
set < entry < string , integer > > displaynames = cal . getdisplaynames ( field , calendar . all styles , locale ) . entryset ( ) ;
strategyandwidth getnextstrategy ( ) {
int last = source . length ( ) ;
} ;
return width > 2 ? literal year strategy : abbreviated year strategy ;
this . strategy = strategy ;
return new strategyandwidth ( getstrategy ( c , width , definingcalendar ) , width ) ;
return c > = 'a' & & c < = 'z' | | c > = 'a' & & c < = 'z' ;
pos . seterrorindex ( sidx ) ;
private static class caseinsensitivetextstrategy extends patternstrategy {
private static abstract class patternstrategy extends strategy {
return iso8601timezonestrategy . iso 8601 3 strategy ;
public static final fastdateformat iso8601 date format
public static final fastdateformat iso time no t time zone format = iso8601 time time zone format ;
@ deprecated
public static final fastdateformat iso8601 datetime time zone format
public static final fastdateformat iso time no t format = iso8601 time format ;
public static final fastdateformat iso datetime format = iso8601 datetime format ;
public static final fastdateformat iso8601 time format
public static final fastdateformat iso datetime time zone format = iso8601 datetime time zone format ;
public static final fastdateformat iso8601 datetime format
public static final fastdateformat iso8601 time time zone format
public static final fastdateformat iso date format = iso8601 date format ;
}
break ;
if ( zonename = = null ) {
string key = displayname . getkey ( ) . tolowercase ( locale ) ;
tzinfo tzinfo = tznames . get ( value . tolowercase ( locale ) ) ;
if ( sorted . add ( key ) ) {
tzinfo tzinfo = standard ;
map < string , integer > values = new hashmap < string , integer > ( ) ;
return right . compareto ( left ) ;
private final map < string , integer > lkeyvalues ;
return values ;
tzinfo standard = new tzinfo ( tz , false ) ;
final set < string > sorted = new treeset < string > ( longer first lowercase ) ;
public int compare ( string left , string right ) {
private static final comparator < string > longer first lowercase = new comparator < string > ( ) {
}
cal . set ( calendar . zone offset , tzinfo . zone . getrawoffset ( ) ) ;
tznames . put ( key , tzinfo ) ;
lkeyvalues = appenddisplaynames ( definingcalendar , locale , field , regex ) ;
case 5 :
treeset < string > sorted = new treeset < string > ( longer first lowercase ) ;
cal . set ( calendar . dst offset , tzinfo . dstoffset ) ;
tzinfo = new tzinfo ( tz , true ) ;
string key = zonenames [ i ] . tolowercase ( locale ) ;
for ( string zonename : sorted ) {
for ( string symbol : sorted ) {
int dstoffset ;
timezone zone ;
map < string , integer > displaynames = cal . getdisplaynames ( field , calendar . all styles , locale ) ;
tzinfo = standard ;
private static class tzinfo {
private final map < string , tzinfo > tznames = new hashmap < string , tzinfo > ( ) ;
private static map < string , integer > appenddisplaynames ( calendar cal , locale locale , int field , stringbuilder regex ) {
zone = tz ;
tzinfo ( timezone tz , boolean usedst ) {
for ( int i = 1 ; i < zonenames . length ; + + i ) {
timezone tz = timezone . gettimezone ( "gmt" + value ) ;
switch ( i ) {
values . put ( key , displayname . getvalue ( ) ) ;
@ override
sb . append ( " ( ( ? iu ) " + rfc 822 time zone + " | " + gmt option ) ;
simplequote ( sb . append ( ' | ' ) , zonename ) ;
} ;
dstoffset = usedst ? tz . getdstsavings ( ) : 0 ;
timezone tz = timezone . gettimezone ( value . touppercase ( ) ) ;
for ( map . entry < string , integer > displayname : displaynames . entryset ( ) ) {
simplequote ( regex , symbol ) . append ( ' | ' ) ;
break ;
case 3 :
cal . settimezone ( tz ) ;
private static executable of ( constructor < ? > constructor ) { return new executable ( constructor ) ; }
parametertypes = constructor . getparametertypes ( ) ;
private static boolean ismatchingexecutable ( executable method , class < ? > [ ] parametertypes ) {
private executable ( constructor < ? > constructor ) {
isvarargs = constructor . isvarargs ( ) ;
return false ;
if ( srcargs . length < normalargslen )
class < ? > destclass = destargs [ destargs . length - 1 ] . getcomponenttype ( ) ;
final class < ? > [ ] destargs = executable . getparametertypes ( ) ;
if ( novarargspassed ) {
if ( method . isvarargs ( ) ) {
public class < ? > [ ] getparametertypes ( ) { return parametertypes ; }
final class < ? > [ ] methodparametertypes = method . getparametertypes ( ) ;
}
return classutils . isassignable ( parametertypes , methodparametertypes , true ) ;
public boolean isvarargs ( ) { return isvarargs ; }
private executable ( method method ) {
for ( int i = destargs . length - 1 ; i < srcargs . length ; i + + ) {
static boolean ismatchingmethod ( method method , class < ? > [ ] parametertypes ) {
if ( isvarargs ) {
return float . max value ;
private static int compareparametertypes ( final executable left , final executable right , final class < ? > [ ] actual ) {
class < ? > sourceclass = srcargs [ srcargs . length - 1 ] . getcomponenttype ( ) ;
private final class < ? > [ ] parametertypes ;
return true ;
totalcost + = getobjecttransformationcost ( srcargs [ i ] , destargs [ i ] ) ;
int i ;
final boolean novarargspassed = srcargs . length < destargs . length ;
import java . lang . reflect . method ;
return memberutils . ismatchingexecutable ( executable . of ( method ) , parametertypes ) ;
totalcost + = getobjecttransformationcost ( srcclass , destclass ) + varargscost ;
private static float gettotaltransformationcost ( final class < ? > [ ] srcargs , final executable executable ) {
else {
for ( i = 0 ; i < methodparametertypes . length - 1 & & i < parametertypes . length ; i + + ) {
isvarargs = method . isvarargs ( ) ;
class < ? > srcclass = srcargs [ i ] ;
private static final class executable {
else if ( explicitarrayforvarags ) {
totalcost + = getobjecttransformationcost ( destclass , object . class ) + varargscost ;
private static executable of ( method method ) { return new executable ( method ) ; }
final float varargscost = 0 . 001f ;
if ( !classutils . isassignable ( parametertypes [ i ] , varargparametertype , true ) ) {
totalcost + = getobjecttransformationcost ( sourceclass , destclass ) + varargscost ;
if ( !classutils . isassignable ( parametertypes [ i ] , methodparametertypes [ i ] , true ) ) {
static boolean ismatchingconstructor ( constructor < ? > method , class < ? > [ ] parametertypes ) {
return compareparametertypes ( executable . of ( left ) , executable . of ( right ) , actual ) ;
for ( ; i < parametertypes . length ; i + + ) {
for ( int i = 0 ; i < normalargslen ; i + + ) {
parametertypes = method . getparametertypes ( ) ;
final boolean isvarargs = executable . isvarargs ( ) ;
import java . lang . reflect . constructor ;
private final boolean isvarargs ;
final long normalargslen = isvarargs ? destargs . length - 1 : destargs . length ;
class < ? > varargparametertype = methodparametertypes [ methodparametertypes . length - 1 ] . getcomponenttype ( ) ;
final boolean explicitarrayforvarags = ( srcargs . length = = destargs . length ) & & srcargs [ srcargs . length - 1 ] . isarray ( ) ;
static int comparemethodfit ( final method left , final method right , final class < ? > [ ] actual ) {
static int compareconstructorfit ( final constructor < ? > left , final constructor < ? > right , final class < ? > [ ] actual ) {
if ( stringutils . isblank ( wrapon ) ) {
matcher = patterntowrapon . matcher ( str . substring ( offset + wraplength ) ) ;
continue ;
offset + = matcher . end ( ) ;
return wrap ( str , wraplength , newlinestr , wraplongwords , " " ) ;
}
if ( matcher . start ( ) = = 0 ) {
public static string wrap ( final string str , int wraplength , string newlinestr , final boolean wraplongwords , string wrapon ) {
import java . util . regex . matcher ;
wrapon = " " ;
if ( matcher . find ( ) ) {
} else {
spacetowrapat = matcher . start ( ) ;
spacetowrapat = matcher . start ( ) + offset + wraplength ;
int spacetowrapat = - 1 ;
pattern patterntowrapon = pattern . compile ( wrapon ) ;
matcher matcher = patterntowrapon . matcher ( str . substring ( offset , math . min ( offset + wraplength + 1 , inputlinelength ) ) ) ;
while ( matcher . find ( ) ) {
spacetowrapat = matcher . start ( ) + offset ;
final timezone tz = timezone . gettimezone ( "gmt" + value ) ;
final stringbuilder sb = new stringbuilder ( ) ;
final strategyandwidth field = fm . getnextstrategy ( ) ;
final int width = currentidx - begin ;
final string key = displayname . getkey ( ) . tolowercase ( locale ) ;
final int value = integer . parseint ( source . substring ( pos . getindex ( ) , idx ) ) ;
final strategy nextstrategy = lt . next ( ) . strategy ;
for ( final string zonename : sorted ) {
final tzinfo standard = new tzinfo ( tz , false ) ;
final int end = idx + maxwidth ;
final int begin = currentidx ;
final char c = value . charat ( i ) ;
final string key = zonenames [ i ] . tolowercase ( locale ) ;
final char c = pattern . charat ( currentidx ) ;
for ( final map . entry < string , integer > displayname : displaynames . entryset ( ) ) {
final stringbuilder regex = new stringbuilder ( ) ;
for ( final string symbol : sorted ) {
final treeset < string > sorted = new treeset < > ( longer first lowercase ) ;
final matcher matcher = pattern . matcher ( source . substring ( pos . getindex ( ) ) ) ;
final map < string , integer > displaynames = cal . getdisplaynames ( field , calendar . all styles , locale ) ;
final int maxwidth = pattern . getmaxwidth ( lt ) ;
final strategyparser fm = new strategyparser ( pattern , definingcalendar ) ;
final strategyandwidth pattern = lt . next ( ) ;
final tzinfo tzinfo = tznames . get ( value . tolowercase ( locale ) ) ;
final timezone tz = timezone . gettimezone ( value . touppercase ( ) ) ;
final string formatfield = sb . tostring ( ) ;
final listiterator < strategyandwidth > lt = patterns . listiterator ( ) ;
final map < string , integer > values = new hashmap < > ( ) ;
final char c = source . charat ( idx ) ;
final parseposition pp = new parseposition ( 0 ) ;
final int sidx = idx + pos . getindex ( ) ;
private static final string field name quote = " \ "" ;
+ field name quote ) ;
super . appendfieldstart ( buffer , field name quote + fieldname
final string country = segments [ 1 ] ;
if ( isiso639languagecode ( language ) & & isiso3166countrycode ( country ) | |
} else if ( segments . length = = 3 ) {
variant . length ( ) > 0 ) {
if ( segments . length = = 2 ) {
return new locale ( language , country ) ;
isnumericareacode ( country ) ) {
final string variant = segments [ 2 ] ;
return new locale ( language , country , variant ) ;
( country . length ( ) = = 0 | | isiso3166countrycode ( country ) ) & &
}
return ( isinclusivehostcount ( ) ? broadcast ( ) :
public int getaddresscount ( ) {
int count = broadcast ( ) - network ( ) + ( isinclusivehostcount ( ) ? 1 : - 1 ) ;
private int low ( ) {
private int high ( ) {
return count < 0 ? 0 : count ;
broadcast ( ) - network ( ) > 1 ? broadcast ( ) - 1 : 0 ) ;
broadcast ( ) - network ( ) > 1 ? network ( ) + 1 : 0 ) ;
return ( isinclusivehostcount ( ) ? network ( ) :
replystring = null ;
replylines = new arraylist < string > ( ) ;
commandsupport = new protocolcommandsupport ( this ) ;
newreplystring = false ;
setdefaultport ( default port ) ;
this ( default encoding ) ;
private final string encoding ;
import org . apache . commons . net . io . crlflinereader ;
new crlflinereader ( new inputstreamreader ( input , getcontrolencoding ( ) ) ) ;
if ( !ftpreply . ispositiveintermediate ( replycode ) ) {
| | ! ( delim3 = = delim4 ) ) {
if ( pwd ( ) ! = ftpreply . pathname created ) {
if ( offset > = 0 ) {
if ( filetype = = ascii file type ) {
if ( !ftpreply . ispositiveintermediate ( rnfr ( from ) ) ) {
dataconnectionmode = = passive remote data connection mode ) {
}
if ( systemname = = null & & ftpreply . ispositivecompletion ( syst ( ) ) ) {
if ( pasv ( ) ! = ftpreply . entering passive mode ) {
if ( ftpreply . ispositivecompletion ( mdtm ( pathname ) ) ) {
dataconnectionmode ! = passive local data connection mode ) {
if ( ftpreply . ispositivecompletion ( help ( ) ) ) {
if ( ftpreply . ispositivecompletion ( stat ( pathname ) ) ) {
while ( ( line = reader . readline ( ) ) ! = null ) {
if ( ( socket = opendataconnection ( command , remote ) ) = = null ) {
if ( ftpreply . ispositivecompletion ( replycode ) ) {
if ( activemaxport = = activeminport ) {
if ( ftpreply . ispositivecompletion ( help ( command ) ) ) {
if ( datatimeout > = 0 ) {
if ( ( socket = opendataconnection ( ftpcommand . nlst , getlistarguments ( pathname ) ) ) = = null ) {
if ( ftpreply . ispositivecompletion ( stat ( ) ) ) {
boolean ok = completependingcommand ( ) ;
throw new ioexception ( "no response from proxy" ) ;
if ( size = = 0 ) {
string code = null ;
response . add ( line ) ;
if ( ( getrestartoffset ( ) > 0 ) & & !restart ( getrestartoffset ( ) ) ) {
list < string > response = new arraylist < string > ( ) ;
socket = new socket ( proxyhost , proxyport ) ;
& & line . length ( ) > 0 ; line = reader . readline ( ) ) {
}
code = resp . substring ( 9 , 12 ) ;
if ( attemptepsv & & epsv ( ) = = ftpreply . entering epsv mode ) {
if ( pasv ( ) ! = ftpreply . entering passive mode ) {
tunnelhandshake ( this . getpassivehost ( ) , this . getpassiveport ( ) , is , os ) ;
if ( resp . startswith ( "http / " ) & & resp . length ( ) > = 12 ) {
new inputstreamreader ( input ) ) ;
boolean attemptepsv = isuseepsvwithipv4 ( ) | | isinet6address ;
ioexception ioe = new ioexception ( "could not connect to " + host + " using port " + port ) ;
stringbuilder msg = new stringbuilder ( ) ;
output . write ( header . getbytes ( "utf - 8" ) ) ;
output . write ( connectstring . getbytes ( "utf - 8" ) ) ;
parseextendedpassivemodereply ( replylines . get ( 0 ) ) ;
} else {
final string auth = proxyusername + " : " + proxypassword ;
throw new ioexception ( "invalid response from proxy : " + resp ) ;
for ( string line : response ) {
socket socket ;
if ( isinet6address ) {
int size = response . size ( ) ;
bufferedreader reader = new bufferedreader (
final string hoststring = "host : " + host + " : " + port ;
string resp = response . get ( 0 ) ;
throw new ioexception ( msg . tostring ( ) ) ;
for ( string line = reader . readline ( ) ; line ! = null
msg . append ( line ) ;
if ( !"200" . equals ( code ) ) {
output . write ( crlf ) ;
socket . close ( ) ;
protected socket opendataconnection ( int command , string arg )
if ( !ftpreply . ispositivepreliminary ( sendcommand ( command , arg ) ) ) {
output . write ( hoststring . getbytes ( "utf - 8" ) ) ;
import java . net . inet6address ;
public void connect ( string host , int port ) throws socketexception , ioexception {
final boolean isinet6address = getremoteaddress ( ) instanceof inet6address ;
enterlocalpassivemode ( ) ;
if ( getdataconnectionmode ( ) ! = passive local data connection mode ) {
return null ;
parsepassivemodereply ( replylines . get ( 0 ) ) ;
+ base64 . encodetostring ( auth . getbytes ( "utf - 8" ) ) ;
super . connectaction ( ) ;
return bytesavailable + super . available ( ) ;
continue sendpacket ;
do {
} while ( timeouts < maxtimeouts ) ;
while ( timeouts < maxtimeouts ) ;
} else if ( istype = = member list type ) {
if ( istype = = file list type ) {
}
if ( input ! = null ) {
if ( listener ! = null ) {
if ( output ! = null ) {
if ( lastblock = = ( block = = 0 ? 65535 : ( block - 1 ) ) ) {
if ( !isparsed ) {
if ( file = = null ) {
if ( numtimeouts < 1 ) {
} else {
if ( message . length < 1 ) {
if ( mode = = tftp . ascii mode ) {
if ( length < reply code len ) {
if ( length > reply code len & & line . charat ( reply code len ) = = ' - ' )
return ( ! ( line . startswith ( code ) & & line . charat ( reply code len ) = = ' ' ) ) ;
public static final int reply code len = 3 ;
return ( ! ( line . length ( ) > reply code len & & line . charat ( reply code len ) ! = ' - ' & &
code = line . substring ( 0 , reply code len ) ;
}
private static string parsepathname ( string reply )
return reply . substring ( reply code len + 1 ) ;
int end = reply . lastindexof ( " \ " " ) ;
if ( begin = = - 1 ) {
return reply . substring ( begin + 1 , end ) . replace ( " \ " \ "" , " \ "" ) ;
int begin = reply . indexof ( '"' ) ;
if ( end ! = - 1 ) {
if ( host . issitelocaladdress ( ) ) {
}
" [ replacing site local address " + passivehost + " with " + hostaddress + " ] \ n" ) ;
passivehost = hostaddress ;
firereplyreceived ( 0 ,
inetaddress remote = getremoteaddress ( ) ;
if ( !remote . issitelocaladdress ( ) ) {
string hostaddress = remote . gethostaddress ( ) ;
}
end = param . length ( ) - 1 ;
int end ;
if ( end ! = - 1 ) {
return param . substring ( 1 , end ) . replace ( " \ " \ "" , " \ "" ) ;
if ( param . startswith ( " \ "" ) ) {
end = param . lastindexof ( " \ " " ) ;
} else {
if ( param . endswith ( " \ "" ) ) {
if ( host . issitelocaladdress ( ) ) {
}
this . passivenatworkaround = enabled ;
inetaddress host = inetaddress . getbyname ( passivehost ) ;
public void setpassivenatworkaround ( boolean enabled ) {
" [ replacing site local address " + passivehost + " with " + hostaddress + " ] \ n" ) ;
passivehost = hostaddress ;
private boolean passivenatworkaround = true ;
firereplyreceived ( 0 ,
inetaddress remote = getremoteaddress ( ) ;
"could not parse passive host information . \ nserver reply : " + reply ) ;
throw new malformedserverreplyexception (
if ( passivenatworkaround ) {
if ( !remote . issitelocaladdress ( ) ) {
} catch ( unknownhostexception e ) {
string hostaddress = remote . gethostaddress ( ) ;
try {
throw new illegalargumentexception ( "value [ " + value + " ] not in range [ " + begin + " , " + end + " ] " ) ;
int n = ( rangecheck ( integer . parseint ( matcher . group ( i ) ) , 0 , 255 ) ) ;
int cidrpart = rangecheck ( integer . parseint ( matcher . group ( 5 ) ) , 1 , nbits ) ;
if ( value > = begin & & value < = end ) {
& & imapreply . issuccess ( senddata ( message ) ) ;
return imapreply . iscontinuation ( status )
return docommand ( imapcommand . append , args . tostring ( ) ) ;
if ( datetime . charat ( 0 ) = = dquote ) {
}
{
if ( flags ! = null ) {
@ deprecated
args . append ( " " ) . append ( flags ) ;
stringbuilder args = new stringbuilder ( mailboxname ) ;
} else {
public boolean append ( string mailboxname , string flags , string datetime , string message ) throws ioexception
args . append ( " " ) ;
args . append ( message ) ;
if ( datetime ! = null ) {
private static final string dquote s = " \ "" ;
args . append ( ' { ' ) . append ( message . length ( ) ) . append ( ' } ' ) ;
if ( message . startswith ( dquote s ) & & message . endswith ( dquote s ) ) {
final int status = sendcommand ( imapcommand . append , args . tostring ( ) ) ;
private static final char dquote = '"' ;
args . append ( dquote ) . append ( datetime ) . append ( dquote ) ;
args . append ( datetime ) ;
}
throw new illegalargumentexception ( "cannot have / 0 cidr with non - zero address" ) ;
if ( cidrpart = = 0 & & address ! = 0 ) {
netmask | = ( 1 < < 31 - j ) ;
int cidrpart = rangecheck ( integer . parseint ( matcher . group ( 5 ) ) , 0 , nbits ) ;
}
long countlong = getaddresscountlong ( ) ;
@ deprecated
public int getaddresscount ( ) {
public long getaddresscountlong ( ) {
long n = network ( ) & unsigned int mask ;
throw new runtimeexception ( "count is larger than an integer : " + countlong ) ;
if ( countlong > integer . max value ) {
long b = broadcast ( ) & unsigned int mask ;
private static final long unsigned int mask = 0x0ffffffffl ;
long count = b - n + ( isinclusivehostcount ( ) ? 1 : - 1 ) ;
return ( int ) countlong ;
}
parser = new unixftpentryparser ( config , false ) ;
{
parser = new unixftpentryparser ( config , true ) ;
else if ( ukey . indexof ( ftpclientconfig . syst unix trim leading ) > = 0 )
config ! = null & & ftpclientconfig . syst unix trim leading . equals ( config . getserversystemkey ( ) ) )
new unixftpentryparser ( config ,
hostname = null ;
hostname = hostname ;
protected string hostname ;
totalbytessent = 0l ;
public long gettotalbytessent ( ) {
}
public long gettotalbytesreceived ( ) {
totalbytessent + = totalthispacket ;
totalbytesreceived + = datalength ;
totalbytesreceived = 0 ;
private long totalbytesreceived = 0 ;
return totalbytessent ;
return totalbytesreceived ;
private long totalbytessent = 0 ;
return ( isinclusivehostcount ( ) ? broadcast :
return ( isinclusivehostcount ( ) ? network :
return format ( toarray ( network ) ) ;
format ( toarray ( netmask ) )
return format ( toarray ( broadcast ) ) ;
broadcastlong ( ) - networklong ( ) > 1 ? broadcast - 1 : 0 ) ;
return format ( toarray ( netmask ) ) ;
format ( toarray ( address ) ) ,
broadcastlong ( ) - networklong ( ) > 1 ? network + 1 : 0 ) ;
return format ( toarray ( address ) ) ;
return docommand ( imapcommand . list , quotestring ( refname ) + " " + quotestring ( mailboxname ) ) ;
return docommand ( imapcommand . lsub , quotestring ( refname ) + " " + quotestring ( mailboxname ) ) ;
final k key = keyiterator . next ( ) ;
return false ;
return chain ;
while ( it . hasnext ( ) ) {
private final collection < v > values ;
public boolean contains ( object o ) {
protected map < k , collection < v > > getmap ( ) {
if ( values = = null | | values . iterator ( ) = = null | | !values . iterator ( ) . hasnext ( ) ) {
this . key = key ;
put ( ( k ) entry . getkey ( ) , ( v ) entry . getvalue ( ) ) ;
return new transformiterator < v , entry < k , v > > ( new valuesiterator ( key ) , entrytransformer ) ;
public int getcount ( object object ) {
return entryvaluesview ! = null ? entryvaluesview : ( entryvaluesview = new entryvalues ( ) ) ;
import java . util . abstractcollection ;
}
public boolean remove ( object object , int ncopies ) {
if ( map = = null ) {
return containsall ( ( bag < ? > ) coll ) ;
return getmap ( ) . keyset ( ) ;
import org . apache . commons . collections4 . multivaluedmap ;
return key ;
protected iterator < ? extends k > nextiterator ( int count ) {
public boolean containsmapping ( object key , object value ) {
int size = 0 ;
if ( coll = = null ) {
return getmap ( ) . tostring ( ) ;
public iterator < entry < k , v > > iterator ( ) {
return new valuesiterator ( key ) ;
final k current = it . next ( ) ;
final iterator < k > keyiterator = getmap ( ) . keyset ( ) . iterator ( ) ;
array [ i + + ] = unchecked ;
this . collectionfactory = new instantiatefactory < c > ( collectionclazz ) ;
public class abstractmultivaluedmap < k , v > implements multivaluedmap < k , v > , serializable {
public collection < entry < k , v > > entries ( ) {
} else {
result = true ;
public v setvalue ( v value ) {
return collectionfactory . create ( ) ;
public boolean containsvalue ( final object value ) {
if ( array . length < size ) {
public bag < k > keys ( ) {
coliterator . next ( ) ;
getmap ( ) . put ( key , coll ) ;
public set < k > keyset ( ) {
if ( !containskey ( key ) ) {
import org . apache . commons . collections4 . iterators . lazyiteratorchain ;
return getmap ( ) . containskey ( key ) ;
boolean result = false ;
public iterator < v > iterator ( ) {
public entry < k , v > transform ( final v input ) {
final object [ ] result = new object [ size ( ) ] ;
if ( !result & & tmpresult ) {
abstractmultivaluedmap . this . clear ( ) ;
public iterator < k > iterator ( ) {
public boolean equals ( object obj ) {
import java . util . map ;
if ( coll . size ( ) > 0 ) {
return 0 ;
for ( final map . entry < k , collection < v > > entry : pairs ) {
throw new illegalargumentexception ( "map must not be null" ) ;
if ( othercol = = null ) {
array [ i + + ] = null ;
this . values = get ( key ) ;
collection < v > coll = get ( key ) ;
count = col . size ( ) ;
chain . additerator ( new valuesiterator ( k ) ) ;
if ( coll instanceof bag ) {
collection < v > col = abstractmultivaluedmap . this . getmap ( ) . get ( object ) ;
return coll . size ( ) ;
return new lazyiteratorchain < entry < k , v > > ( ) {
final transformer < v , entry < k , v > > entrytransformer = new transformer < v , entry < k , v > > ( ) {
if ( !othercol . contains ( value ) ) {
array = unchecked ;
@ suppresswarnings ( "unchecked" )
return new entry < k , v > ( ) {
if ( other . size ( ) ! = size ( ) ) {
coll . add ( it . next ( ) ) ;
protected < c extends collection < v > > abstractmultivaluedmap ( final map < k , ? super c > map ,
if ( col . isempty ( ) ) {
if ( entry . getvalue ( ) . contains ( value ) ) {
final collection < v > vs = valuesview ;
return new lazyiteratorchain < k > ( ) {
return size ;
for ( int index = getcount ( current ) ; index > 0 ; index - - ) {
private class entryvalues extends abstractcollection < entry < k , v > > {
final iterator < k > it = getmap ( ) . keyset ( ) . iterator ( ) ;
final t unchecked = ( t ) current ;
import org . apache . commons . collections4 . bag . hashbag ;
import java . util . set ;
if ( this = = obj ) {
if ( col = = null ) {
return true ;
iterator . remove ( ) ;
final set < map . entry < k , collection < v > > > pairs = getmap ( ) . entryset ( ) ;
public boolean add ( k object , int ncopies ) {
return vs ! = null ? vs : ( valuesview = new values ( ) ) ;
private final map < k , collection < v > > map ;
this . iterator = values . iterator ( ) ;
private transient keysbag keysbagview ;
private transient entryvalues entryvaluesview ;
import org . apache . commons . collections4 . transformer ;
getmap ( ) . clear ( ) ;
return getmap ( ) . remove ( key ) ;
return containsall ( new hashbag < object > ( coll ) ) ;
return input ;
public < t > t [ ] toarray ( t [ ] array ) {
return result ? value : null ;
result = coll . add ( value ) ;
return iterator . hasnext ( ) ;
if ( col . size ( ) ! = othercol . size ( ) ) {
if ( values . isempty ( ) ) {
import java . io . serializable ;
return iterator . next ( ) ;
public boolean removemapping ( k key , v item ) {
import java . util . arraylist ;
public valuesiterator ( final object key ) {
final iterator < ? > it = other . uniqueset ( ) . iterator ( ) ;
return count ;
return col . contains ( value ) ;
import org . apache . commons . collections4 . iterators . emptyiterator ;
public int size ( ) {
public v next ( ) {
remove ( key ) ;
object key = it . next ( ) ;
if ( col ! = null ) {
public set < k > uniqueset ( ) {
coll . add ( value ) ;
iterator < k > nextit = new iterator < k > ( ) {
protected collection < v > createcollection ( ) {
this . map = ( map < k , collection < v > > ) map ;
import java . lang . reflect . array ;
public boolean remove ( object object ) {
for ( final map . entry < ? extends k , ? extends v > entry : map . entryset ( ) ) {
return keyset ( ) ;
collection < ? > othercol = other . get ( key ) ;
int i = 0 ;
if ( pairs ! = null ) {
package org . apache . commons . collections4 . multimap ;
public void putall ( final map < ? extends k , ? extends v > map ) {
final collection < v > coll = get ( key ) ;
import java . util . iterator ;
iterator < ? extends v > it = values . iterator ( ) ;
public string tostring ( ) {
final object current = it . next ( ) ;
return getmap ( ) . get ( key ) ;
final int size = size ( ) ;
public iterator < v > iterator ( final object key ) {
final iterator < v > coliterator = getmap ( ) . get ( key ) . iterator ( ) ;
import org . apache . commons . collections4 . functors . instantiatefactory ;
import org . apache . commons . collections4 . bag ;
final collection < k > keyscol = new arraylist < k > ( getmap ( ) . keyset ( ) ) ;
final iterator < k > keyiterator = keyscol . iterator ( ) ;
coll = createcollection ( ) ;
int count = 0 ;
private class keysbag implements bag < k > {
if ( map ! = null ) {
for ( final k key : map . keyset ( ) ) {
private class valuesiterator implements iterator < v > {
import java . util . map . entry ;
public boolean addall ( collection < ? extends k > c ) {
private static final long serialversionuid = 7994988366330224277l ;
public void remove ( ) {
public void clear ( ) {
public boolean isempty ( ) {
while ( i < array . length ) {
final collection < v > col = get ( key ) ;
import org . apache . commons . collections4 . iterators . transformiterator ;
result = col . remove ( item ) ;
@ override
public object [ ] toarray ( ) {
import java . util . collection ;
return abstractmultivaluedmap . this . size ( ) ;
final class < c > collectionclazz ) {
public k getkey ( ) {
for ( final k k : keyset ( ) ) {
private final factory < ? extends collection < v > > collectionfactory ;
return keysbagview ! = null ? keysbagview : ( keysbagview = new keysbag ( ) ) ;
return array ;
public boolean removeall ( collection < ? > coll ) {
return null ;
return getmap ( ) . containskey ( o ) ;
public boolean hasnext ( ) {
return getmap ( ) . hashcode ( ) ;
collection < ? > col = get ( key ) ;
size + = col . size ( ) ;
final t [ ] unchecked = ( t [ ] ) array . newinstance ( array . getclass ( ) . getcomponenttype ( ) , size ) ;
final iteratorchain < v > chain = new iteratorchain < v > ( ) ;
public k next ( ) {
if ( !keyiterator . hasnext ( ) ) {
public collection < v > values ( ) {
return result ;
public v put ( k key , v value ) {
public boolean add ( k object ) {
public v getvalue ( ) {
for ( collection < v > col : getmap ( ) . values ( ) ) {
public int size ( final object key ) {
return nextit ;
private final iterator < v > iterator ;
private transient collection < v > valuesview ;
result [ i + + ] = current ;
public boolean containskey ( object key ) {
return getmap ( ) . isempty ( ) ;
return coliterator . hasnext ( ) ;
boolean tmpresult = coll . add ( it . next ( ) ) ;
if ( obj instanceof multivaluedmap = = false ) {
public collection < v > remove ( object key ) {
multivaluedmap < ? , ? > other = ( multivaluedmap < ? , ? > ) obj ;
private boolean containsall ( final bag < ? > other ) {
public boolean containsall ( collection < ? > coll ) {
return map ;
private class values extends abstractcollection < v > {
public int hashcode ( ) {
public void putall ( multivaluedmap < ? extends k , ? extends v > map ) {
public boolean retainall ( collection < ? > coll ) {
return emptyiterator . < v > emptyiterator ( ) ;
@ suppresswarnings ( "rawtypes" )
public collection < v > get ( object key ) {
protected iterator < ? extends entry < k , v > > nextiterator ( int count ) {
throw new unsupportedoperationexception ( ) ;
} ;
private final object key ;
abstractmultivaluedmap . this . remove ( key ) ;
if ( getcount ( current ) < other . getcount ( current ) ) {
if ( !result ) {
public boolean putall ( final k key , final iterable < ? extends v > values ) {
iterator it = keyset ( ) . iterator ( ) ;
putall ( key , ( collection < v > ) map . get ( key ) ) ;
if ( obj = = null ) {
import org . apache . commons . collections4 . iterators . iteratorchain ;
for ( object value : col ) {
import org . apache . commons . collections4 . factory ;
if ( current = = null ) {
return false ;
public boolean contains ( object o ) {
this . key = key ;
return col . tostring ( ) ;
}
if ( map = = null ) {
return addedval ! = null ? true : false ;
return new valuesiterator ( key ) ;
return col . hashcode ( ) ;
private collection < v > getmapping ( ) {
return col . toarray ( a ) ;
return new multivaluedmapentry ( key , input ) ;
return new wrappedcollection ( key ) ;
public v setvalue ( v value ) {
return collectionutils . empty collection . tostring ( ) ;
public multivaluedmapentry ( k key , v value ) {
return col . contains ( o ) ;
public iterator < v > iterator ( ) {
col . clear ( ) ;
if ( initialcollectioncapacity < 0 ) {
if ( ! ( other instanceof collection ) ) {
return 0 ;
throw new illegalargumentexception ( "map must not be null" ) ;
if ( !othercol . contains ( value ) ) {
private entry < k , v > current = null ;
public wrappedcollection ( object key ) {
@ suppresswarnings ( "unchecked" )
return col . addall ( c ) ;
protected < c extends collection < v > > abstractmultivaluedmap ( final map < k , ? super c > map ,
public < t > t [ ] toarray ( t [ ] a ) {
if ( col . isempty ( ) ) {
public mapiterator < k , v > mapiterator ( ) {
collection < v > coll = getmap ( ) . get ( key ) ;
this . it = abstractmultivaluedmap . this . entries ( ) . iterator ( ) ;
import org . apache . commons . collections4 . set . unmodifiableset ;
super ( key , value ) ;
int initialcollectioncapacity , final class < c > collectionclazz ) {
v addedval = abstractmultivaluedmap . this . put ( ( k ) key , value ) ;
if ( col = = null ) {
return true ;
this . collectionfactory = new instantiatefactory < c > ( collectionclazz , new class [ ] { integer . type } ,
return col . containsall ( o ) ;
throw new illegalstateexception ( ) ;
return col . toarray ( ) ;
return ( t [ ] ) collectionutils . empty collection . toarray ( a ) ;
return it . hasnext ( ) ;
public boolean containsall ( collection < ? > o ) {
new object [ ] { new integer ( initialcollectioncapacity ) } ) ;
if ( other = = null ) {
if ( col . size ( ) ! = othercol . size ( ) ) {
return collectionutils . empty collection . toarray ( ) ;
public map < k , collection < v > > asmap ( ) {
boolean result = col . remove ( item ) ;
boolean result = col . removeall ( c ) ;
public int size ( ) {
return getmap ( ) ;
public boolean addall ( collection < ? extends v > c ) {
if ( col ! = null ) {
import org . apache . commons . collections4 . mapiterator ;
this . map = ( map < k , collection < v > > ) map ;
public multivaluedmapiterator ( ) {
private class multivaluedmapiterator implements mapiterator < k , v > {
public string tostring ( ) {
public boolean retainall ( collection < ? > c ) {
final collection < v > col = getmap ( ) . get ( key ) ;
return getmap ( ) . get ( key ) ;
public boolean remove ( object item ) {
import org . apache . commons . collections4 . keyvalue . abstractmapentry ;
return col . add ( value ) ;
return col . isempty ( ) ;
this . values = getmap ( ) . get ( key ) ;
public void remove ( ) {
public void clear ( ) {
public boolean equals ( object other ) {
public boolean removeall ( collection < ? > c ) {
public boolean isempty ( ) {
@ override
import org . apache . commons . collections4 . collectionutils ;
public object [ ] toarray ( ) {
current = it . next ( ) ;
public k getkey ( ) {
final collection < v > col = getmapping ( ) ;
return ( iterator < v > ) iteratorutils . empty iterator ;
public boolean hasnext ( ) {
public k next ( ) {
private class wrappedcollection implements collection < v > {
return new multivaluedmapiterator ( ) ;
return unmodifiableset . < k > unmodifiableset ( keyset ( ) ) ;
return result ;
return abstractmultivaluedmap . this . putall ( ( k ) key , c ) ;
return current . setvalue ( value ) ;
public v getvalue ( ) {
it . remove ( ) ;
return current . getvalue ( ) ;
return collectionutils . empty collection . equals ( other ) ;
import org . apache . commons . collections4 . iteratorutils ;
private class multivaluedmapentry extends abstractmapentry < k , v > {
throw new illegalargumentexception ( "illegal capacity : " + initialcollectioncapacity ) ;
return collectionutils . empty collection . hashcode ( ) ;
public int hashcode ( ) {
public boolean add ( v value ) {
import org . apache . commons . collections4 . iterators . emptymapiterator ;
collection othercol = ( collection ) other ;
@ suppresswarnings ( "rawtypes" )
return emptymapiterator . < k , v > emptymapiterator ( ) ;
throw new unsupportedoperationexception ( ) ;
private final object key ;
abstractmultivaluedmap . this . remove ( key ) ;
boolean result = col . retainall ( c ) ;
return current . getkey ( ) ;
private final iterator < entry < k , v > > it ;
if ( size ( ) = = 0 ) {
return col . size ( ) ;
for ( object value : col ) {
public valueslistiterator ( object key ) {
return list . get ( index ) ;
public list < v > get ( object key ) {
import org . apache . commons . collections4 . multimap . multivaluedhashmap ;
this . key = key ;
private static final long serialversionuid = 6024950625989666915l ;
list = ( list < v > ) abstractlistvaluedmap . this . createcollection ( ) ;
}
private multimaputils ( ) {
return ( bag < v > ) col ;
protected class wrappedcollection implements collection < v > {
if ( list . isempty ( ) ) {
final multivaluedmap < ? extends k , ? extends v > map ) {
return multivaluedhashmap . < k , v , c > setvaluedhashmap ( setclass ) ;
import org . apache . commons . collections4 . listutils ;
return listutils . emptyifnull ( ( list < v > ) getmap ( ) . remove ( key ) ) ;
import org . apache . commons . collections4 . setutils ;
list < v > list = ( list < v > ) createcollection ( ) ;
public static final multivaluedmap empty multi valued map =
protected class wrappedset extends wrappedcollection implements set < v > {
return map = = null | | map . isempty ( ) ;
public abstract class abstractsetvaluedmap < k , v > extends abstractmultivaluedmap < k , v > implements setvaluedmap < k , v > {
if ( col instanceof set ) {
private listiterator < v > iterator ;
super ( map , listclazz , initiallistcapacity ) ;
import java . util . map ;
if ( col instanceof bag ) {
super ( map , setclazz ) ;
private list < v > values ;
super ( map , listclazz ) ;
public class multimaputils {
public wrappedset ( object key ) {
final transformer < ? super k , ? extends k > keytransformer ,
@ suppresswarnings ( "unchecked" )
import org . apache . commons . collections4 . setvaluedmap ;
return list . addall ( index , c ) ;
return new hashbag < v > ( col ) ;
public int nextindex ( ) {
public boolean hasprevious ( ) {
import java . util . hashmap ;
public static < k , v > listvaluedmap < k , v > createlistvaluedhashmap ( ) {
public v remove ( int index ) {
public abstract class abstractlistvaluedmap < k , v > extends abstractmultivaluedmap < k , v >
if ( result ) {
super ( map , setclazz , initialsetcapacity ) ;
return map = = null ? empty multi valued map : map ;
public list < v > remove ( object key ) {
protected < c extends list < v > > abstractlistvaluedmap ( map < k , ? super c > map , class < c > listclazz ,
return list . indexof ( o ) ;
v value = list . remove ( index ) ;
import org . apache . commons . collections4 . bag . hashbag ;
import java . util . set ;
import java . util . listiterator ;
iterator . remove ( ) ;
import org . apache . commons . collections4 . multimap . transformedmultivaluedmap ;
return new arraylist < v > ( col ) ;
public static < k , v > multivaluedmap < k , v > emptyifnull ( final multivaluedmap < k , v > map ) {
return iterator . hasnext ( ) ;
public v set ( int index , v value ) {
return transformedmultivaluedmap . transformingmap ( map , keytransformer , valuetransformer ) ;
public static < k , v , c extends list < v > > listvaluedmap < k , v > createlistvaluedhashmap ( final class < c > listclass ) {
package org . apache . commons . collections4 ;
list = ( list < v > ) createcollection ( ) ;
if ( values . isempty ( ) ) {
import java . io . serializable ;
this . iterator = values . listiterator ( ) ;
return iterator . next ( ) ;
private class valueslistiterator implements listiterator < v > {
list . add ( index , value ) ;
import java . util . arraylist ;
return new hashset < v > ( col ) ;
list < v > list = ( list < v > ) getmapping ( ) ;
@ suppresswarnings ( { "rawtypes" , "unchecked" } )
public v next ( ) {
protected < c extends list < v > > abstractlistvaluedmap ( map < k , ? super c > map , class < c > listclazz ) {
return new wrappedlist ( key ) ;
return multivaluedhashmap . < k , v > setvaluedhashmap ( ) ;
public static < k , v > collection < v > getcollection ( final multivaluedmap < k , v > map , final k key ) {
private class wrappedlist extends wrappedcollection implements list < v > {
return list . sublist ( fromindex , toindex ) ;
this . iterator . add ( value ) ;
this . values = listutils . emptyifnull ( ( list < v > ) getmap ( ) . get ( key ) ) ;
if ( list = = null ) {
import org . apache . commons . collections4 . listvaluedmap ;
protected < c extends set < v > > abstractsetvaluedmap ( map < k , ? super c > map , class < c > setclazz ,
this . iterator = list . listiterator ( ) ;
package org . apache . commons . collections4 . multimap ;
import java . util . list ;
public v previous ( ) {
public int lastindexof ( object o ) {
return empty multi valued map ;
public static < k , v > list < v > getlist ( multivaluedmap < k , v > map , k key ) {
public int previousindex ( ) {
import java . util . hashset ;
return value ;
this . iterator = values . listiterator ( index ) ;
boolean result = list . addall ( index , c ) ;
return list . set ( index , value ) ;
public static < k , v > multivaluedmap < k , v > transformedmultivaluedmap ( final multivaluedmap < k , v > map ,
public set < v > get ( object key ) {
collection < v > col = map . get ( key ) ;
if ( map ! = null ) {
private static final long serialversionuid = 3383617478898639862l ;
super ( key ) ;
return multivaluedhashmap . < k , v , c > listvaluedhashmap ( listclass ) ;
public void remove ( ) {
if ( col instanceof list ) {
if ( getmap ( ) . get ( key ) = = null ) {
public boolean addall ( int index , collection < ? extends v > c ) {
abstractlistvaluedmap . this . remove ( key ) ;
public valueslistiterator ( object key , int index ) {
implements listvaluedmap < k , v > , serializable {
@ override
this . values = list ;
public v get ( int index ) {
import java . util . collection ;
final transformer < ? super v , ? extends v > valuetransformer ) {
import org . apache . commons . collections4 . multimap . unmodifiablemultivaluedmap ;
unmodifiablemultivaluedmap . unmodifiablemultivaluedmap ( new multivaluedhashmap ( ) ) ;
return null ;
return new wrappedset ( key ) ;
public static < k , v > bag < v > getbag ( multivaluedmap < k , v > map , k key ) {
public boolean hasnext ( ) {
return new valueslistiterator ( key , index ) ;
final class < c > collectionclazz , final int initialcollectioncapacity ) {
public static < k , v > setvaluedmap < k , v > createsetvaluedhashmap ( ) {
protected collection < v > getmapping ( ) {
public list < v > sublist ( int fromindex , int toindex ) {
protected < c extends set < v > > abstractsetvaluedmap ( map < k , ? super c > map , class < c > setclazz ) {
return map . get ( key ) ;
protected final object key ;
return result ;
getmap ( ) . remove ( key ) ;
return iterator . previous ( ) ;
int initiallistcapacity ) {
public static < k , v > multivaluedmap < k , v > unmodifiablemultivaluedmap (
return ( list < v > ) col ;
getmap ( ) . put ( ( k ) key , list ) ;
public void add ( v value ) {
public listiterator < v > listiterator ( int index ) {
public void add ( int index , v value ) {
return unmodifiablemultivaluedmap . < k , v > unmodifiablemultivaluedmap ( map ) ;
public static boolean isempty ( final multivaluedmap < ? , ? > map ) {
return iterator . previousindex ( ) ;
return ( set < v > ) col ;
return multivaluedhashmap . < k , v > listvaluedhashmap ( ) ;
return iterator . hasprevious ( ) ;
return collectionutils . emptyifnull ( getmap ( ) . remove ( key ) ) ;
return iterator . nextindex ( ) ;
public void set ( v value ) {
int initialsetcapacity ) {
return new valueslistiterator ( key ) ;
public set < v > remove ( object key ) {
public int indexof ( object o ) {
final list < v > list = listutils . emptyifnull ( ( list < v > ) getmapping ( ) ) ;
public static < k , v , c extends set < v > > setvaluedmap < k , v > createsetvaluedhashmap ( final class < c > setclass ) {
iterator . set ( value ) ;
private final object key ;
public static < k , v > multivaluedmap < k , v > emptymultivaluedmap ( ) {
public listiterator < v > listiterator ( ) {
public wrappedlist ( object key ) {
public static < k , v > set < v > getset ( multivaluedmap < k , v > map , k key ) {
return setutils . emptyifnull ( ( set < v > ) getmap ( ) . remove ( key ) ) ;
k key = entry . getkey ( ) ;
if ( list = = null ) {
return h ;
return false ;
if ( other . size ( ) ! = size ( ) ) {
iterator < ? > it = keyset ( ) . iterator ( ) ;
if ( ! ( other instanceof set ) ) {
while ( it . hasnext ( ) ) {
return collections . emptyset ( ) . hashcode ( ) ;
if ( set = = null ) {
}
import java . util . iterator ;
set < ? > set = get ( key ) ;
int h = 0 ;
final list < v > list = ( list < v > ) getmapping ( ) ;
set < ? > otherset = ( set < ? > ) other ;
return collections . emptylist ( ) . hashcode ( ) ;
list < ? > otherlist = other . get ( key ) ;
list < v > valuelist = ( list < v > ) entry . getvalue ( ) ;
list < ? > otherlist = ( list < ? > ) other ;
if ( this = = obj ) {
return true ;
set < v > valueset = ( set < v > ) entry . getvalue ( ) ;
if ( otherlist = = null ) {
h + = ( key = = null ? 0 : key . hashcode ( ) ) ^ setutils . hashcodeforset ( valueset ) ;
list < ? > list = get ( key ) ;
listvaluedmap < ? , ? > other = ( listvaluedmap < ? , ? > ) obj ;
iterator < entry < k , collection < v > > > it = getmap ( ) . entryset ( ) . iterator ( ) ;
if ( ! ( other instanceof list ) ) {
if ( obj instanceof listvaluedmap = = false ) {
h + = ( key = = null ? 0 : key . hashcode ( ) ) ^ listutils . hashcodeforlist ( valuelist ) ;
if ( setutils . isequalset ( set , otherset ) = = false ) {
final set < v > set = ( set < v > ) getmapping ( ) ;
setvaluedmap < ? , ? > other = ( setvaluedmap < ? , ? > ) obj ;
return collections . emptylist ( ) . equals ( other ) ;
import java . util . collections ;
import java . util . map . entry ;
if ( listutils . isequallist ( list , otherlist ) = = false ) {
public boolean equals ( object other ) {
public int hashcode ( ) {
set < ? > otherset = other . get ( key ) ;
if ( other = = null ) {
@ override
public boolean equals ( object obj ) {
import java . util . collection ;
if ( obj instanceof setvaluedmap = = false ) {
return collections . emptyset ( ) . equals ( other ) ;
entry < k , collection < v > > entry = it . next ( ) ;
return setutils . hashcodeforset ( set ) ;
if ( otherset = = null ) {
if ( obj = = null ) {
return listutils . hashcodeforlist ( list ) ;
object key = it . next ( ) ;
new object [ ] { integer . valueof ( initialcollectioncapacity ) } ) ;
private multimaputils ( ) { }
public static < k , v > listvaluedmap < k , v > newlistvaluedhashmap ( ) {
final int initiallistcapacity ) {
public static < k , v , c extends list < v > > listvaluedmap < k , v > newlistvaluedhashmap ( final class < c > listclass ) {
public static < k , v , c extends set < v > > setvaluedmap < k , v > newsetvaluedhashmap ( final class < c > setclass ) {
public static < k , v > set < v > getvaluesasset ( final multivaluedmap < k , v > map , final k key ) {
public static < k , v > setvaluedmap < k , v > newsetvaluedhashmap ( ) {
public static < k , v > bag < v > getvaluesasbag ( final multivaluedmap < k , v > map , final k key ) {
protected < c extends list < v > > abstractlistvaluedmap ( final map < k , ? super c > map , class < c > listclazz ,
public static < k , v > list < v > getvaluesaslist ( final multivaluedmap < k , v > map , final k key ) {
protected < c extends list < v > > abstractlistvaluedmap ( final map < k , ? super c > map , class < c > listclazz ) {
return ( list < v > ) getmap ( ) . get ( key ) ;
if ( coll . isempty ( ) ) {
return false ;
if ( values instanceof collection < ? > ) {
public collection < v > get ( final k key ) {
public boolean putall ( final map < ? extends k , ? extends v > map ) {
}
if ( map = = null ) {
import org . apache . commons . collections4 . fluentiterable ;
return it . hasnext ( ) & & collectionutils . addall ( decorated ( ) . get ( transformkey ( key ) ) , it ) ;
if ( coll = = null ) {
} else {
return emptymapiterator . emptymapiterator ( ) ;
private final k key ;
return changed ;
decorated . putall ( mapcopy ) ;
list < v > list = getmapping ( ) ;
collection < v > coll = getmapping ( ) ;
return list . lastindexof ( o ) ;
public boolean put ( final k key , final v value ) {
return ( list < v > ) super . createcollection ( ) ;
boolean result = coll . removeall ( c ) ;
final transformer < ? super k , ? extends k > keytransformer ,
return asmap ( ) . equals ( ( ( multivaluedmap < ? , ? > ) obj ) . asmap ( ) ) ;
final iterator < ? extends v > it = transformedvalues . iterator ( ) ;
return asmap ( ) . equals ( ( ( setvaluedmap < ? , ? > ) obj ) . asmap ( ) ) ;
final list < v > list = getmapping ( ) ;
collection < v > coll = getmap ( ) . get ( key ) ;
return coll . tostring ( ) ;
if ( changed ) {
final list < v > list = listutils . emptyifnull ( getmapping ( ) ) ;
final collection < v > coll = getmap ( ) . get ( key ) ;
if ( !map . isempty ( ) ) {
public wrappedlist ( final k key ) {
return coll . toarray ( a ) ;
return true ;
return coll . addall ( other ) ;
public boolean contains ( object obj ) {
return coll = = null ? 0 : coll . size ( ) ;
return unmodifiableset . unmodifiableset ( keyset ( ) ) ;
getmap ( ) . put ( key , list ) ;
for ( map . entry < ? extends k , ? extends v > entry : map . entries ( ) ) {
return coll ! = null & & coll . contains ( value ) ;
public list < v > get ( final k key ) {
public abstract class abstractsetvaluedmap < k , v > extends abstractmultivaluedmap < k , v >
return asmap ( ) . equals ( ( ( listvaluedmap < ? , ? > ) obj ) . asmap ( ) ) ;
this . collectionfactory = new instantiatefactory < c > ( collectionclazz ,
private class valueslistiterator implements listiterator < v > {
public boolean containsall ( collection < ? > other ) {
throw new nullpointerexception ( "map must not be null . " ) ;
boolean result = coll . retainall ( c ) ;
implements setvaluedmap < k , v > {
protected transformedmultivaluedmap ( final multivaluedmap < k , v > map ,
if ( coll . add ( value ) ) {
public valueslistiterator ( final k key , int index ) {
if ( obj instanceof multivaluedmap ) {
return setutils . isequalset ( set , otherset ) ;
public wrappedset ( final k key ) {
return coll = = null ? false : coll . contains ( obj ) ;
this . iterator . add ( value ) ;
boolean changed = coll . remove ( value ) ;
coll . clear ( ) ;
protected list < v > createcollection ( ) {
final iterable < v > transformedvalues = fluentiterable . of ( values ) . transform ( valuetransformer ) ;
return values ( ) . contains ( value ) ;
iterator < ? extends v > it = values . iterator ( ) ;
if ( obj instanceof setvaluedmap ) {
return it . hasnext ( ) & & collectionutils . addall ( get ( key ) , it ) ;
return listutils . isequallist ( list , otherlist ) ;
return asmap ( ) . hashcode ( ) ;
coll = createcollection ( ) ;
abstractmultivaluedmap . this . map . put ( key , coll ) ;
private static final long serialversionuid = 20150612l ;
public wrappedcollection ( final k key ) {
public valueslistiterator ( final k key ) {
list < v > list = createcollection ( ) ;
@ override
import org . apache . commons . collections4 . collectionutils ;
final transformer < ? super v , ? extends v > valuetransformer ) {
if ( coll ! = null ) {
return decorated ( ) . put ( transformkey ( key ) , transformvalue ( value ) ) ;
return !valuecollection . isempty ( ) & & get ( key ) . addall ( valuecollection ) ;
return getmap ( ) . hashcode ( ) ;
return coll = = null ? true : coll . isempty ( ) ;
return coll . toarray ( ) ;
final collection < v > coll = getmapping ( ) ;
getmap ( ) . remove ( key ) ;
protected list < v > getmapping ( ) {
return coll = = null ? false : coll . containsall ( other ) ;
collection < ? extends v > valuecollection = ( collection < ? extends v > ) values ;
boolean changed = false ;
final multivaluedmap < k , v > mapcopy = new multivaluedhashmap < k , v > ( map ) ;
public set < v > get ( final k key ) {
boolean result = coll . remove ( item ) ;
changed | = put ( entry . getkey ( ) , entry . getvalue ( ) ) ;
public abstract class abstractmultivaluedmap < k , v > implements multivaluedmap < k , v > , serializable {
throw new illegalargumentexception ( "initialcapacity must not be negative . " ) ;
if ( obj instanceof listvaluedmap ) {
return coll . add ( value ) ;
public boolean removemapping ( final object key , final object value ) {
list = createcollection ( ) ;
public boolean addall ( collection < ? extends v > other ) {
new class [ ] { integer . type } ,
for ( final collection < v > col : getmap ( ) . values ( ) ) {
protected final k key ;
for ( map . entry < ? extends k , ? extends v > entry : map . entryset ( ) ) {
public boolean putall ( final k key , final iterable < ? extends v > values ) {
boolean changed = list . addall ( index , c ) ;
public boolean putall ( final multivaluedmap < ? extends k , ? extends v > map ) {
import java . security . accesscontroller ;
return system . getproperty ( deserialize ) ;
public object run ( ) {
( string ) accesscontroller . doprivileged ( new privilegedaction ( ) {
throw new unsupportedoperationexception ( "deserialization of invokertransformer is disabled , " ) ;
}
} ) ;
string deserializeproperty ;
private void readobject ( objectinputstream is ) throws classnotfoundexception , ioexception {
import java . io . ioexception ;
= "org . apache . commons . collections . invokertransformer . enabledeserialization" ;
deserializeproperty = null ;
deserializeproperty =
import java . io . objectinputstream ;
} catch ( securityexception ex ) {
is . defaultreadobject ( ) ;
if ( deserializeproperty = = null | | !deserializeproperty . equalsignorecase ( "true" ) ) {
import java . security . privilegedaction ;
public final static string deserialize
try {
throw new unsupportedoperationexception (
"to re - enable it set system property '" + deserialize + "' to 'true'" ) ;
if ( !"true" . equalsignorecase ( deserializeproperty ) ) {
"deserialization of invokertransformer is disabled for security reasons . " +
import java . io . objectoutputstream ;
import java . io . file ;
public vector getvector ( string key ,
line = line . substring ( 0 , line . length ( ) - 1 ) ;
public long getlong ( string key )
return v ;
public void setproperty ( string key , object value )
public float getfloat ( string key ,
if ( value instanceof float )
return matchingkeys . iterator ( ) ;
while ( valuesenum . hasmoreelements ( ) )
return getinteger ( name ) ;
buffer . append ( line ) ;
object key = keys . next ( ) ;
"' does not contain " +
public short getshort ( string key ,
string pkey = token . substring ( 0 , equalsign ) . trim ( ) ;
else
propertiestokenizer tokenizer =
short defaultvalue )
return ( long ) value ;
if ( getinclude ( ) ! = null & &
addproperty ( key , value ) ;
string s = ( string ) e . nextelement ( ) ;
basepath = new file ( file ) . getabsolutepath ( ) ;
return defaults . getfloat ( key , defaultvalue ) ;
key + " doesn't map to a string object" ) ;
string key = line . substring ( 0 , equalsign ) . trim ( ) ;
return getlong ( key , new long ( defaultvalue ) ) . longvalue ( ) ;
if ( b ! = null )
properties defaults )
file = new file ( value ) ;
if ( s ! = null )
public vector getvector ( string key )
string key = ( string ) thekeys . nextelement ( ) ;
boolean b = getboolean ( key , ( boolean ) null ) ;
public propertiestokenizer ( string string )
float defaultvalue )
matchingkeys . add ( key ) ;
put ( key , v ) ;
protected string basepath ;
public boolean hasmoretokens ( )
return s ;
package org . apache . commons . collections ;
short s = getshort ( key , null ) ;
return getinteger ( name , def ) ;
long l = getlong ( key , null ) ;
else if ( value instanceof string )
double defaultvalue )
boolean b = new boolean ( s ) ;
key + "doesn't map to an existing object" ) ;
super ( ) ;
string defaultvalue )
byte defaultvalue )
if ( containskey ( key ) )
return getfloat ( key , new float ( defaultvalue ) ) . floatvalue ( ) ;
return ( float ) value ;
return defaults . getdouble ( key , defaultvalue ) ;
put ( key , f ) ;
public double getdouble ( string key )
string s = testboolean ( ( string ) value ) ;
string token = super . nexttoken ( ) ;
vector values = ( vector ) value ;
c . setproperty ( s , p . getproperty ( s ) ) ;
vector = ( vector ) value ;
o = defaults . get ( key ) ;
vector = new vector ( 1 ) ;
return o ;
public boolean getboolean ( string key , boolean defaultvalue )
public int getint ( string name ,
if ( o instanceof string )
import java . io . inputstreamreader ;
return tokens ;
validsubset = true ;
if ( value . startswith ( fileseparator ) )
return null ;
if ( value ! = null )
return i ;
public properties getproperties ( string key )
public string nexttoken ( )
public extendedproperties ( string file , string defaultfile )
throw new classcastexception (
return f ;
boolean validsubset = false ;
import java . util . hashtable ;
throw new illegalargumentexception ( "'" +
return defaults . getinteger ( key , defaultvalue ) ;
short defaultvalue )
object value = get ( ( object ) key ) ;
string currentelement =
else if ( value instanceof vector )
return b ;
return getinteger ( key , new integer ( defaultvalue ) ) . intvalue ( ) ;
if ( file ! = null & & file . exists ( ) & & file . canread ( ) )
put ( key , l ) ;
string value = tokenizer . nexttoken ( ) ;
if ( !validsubset )
new vector ( ) : defaultvalue ) ;
if ( i ! = null )
public extendedproperties ( string file )
return getbyte ( key , new byte ( defaultvalue ) ) . bytevalue ( ) ;
c . setproperty ( newkey , get ( key ) ) ;
byte b = new byte ( ( string ) value ) ;
if ( validsubset )
string token = tokens [ i ] ;
file file = null ;
return getstring ( key , null ) ;
key + " doesn't map to an existing object" ) ;
for ( iterator i = c . getkeys ( ) ; i . hasnext ( ) ; )
string newkey = null ;
private void addstringproperty ( string key , string token )
object value = get ( key ) ;
{
public byte getbyte ( string key ,
if ( value instanceof short )
currentoutput . append ( " = " ) ;
setproperty ( key , c . get ( key ) ) ;
while ( keys . hasnext ( ) )
iterator keys = getkeys ( ) ;
import java . util . vector ;
return l . longvalue ( ) ;
public boolean getboolean ( string key )
protected string fileseparator = system . getproperty ( "file . separator" ) ;
import java . util . arraylist ;
public string testboolean ( string value )
if ( defaultfile ! = null )
key . equalsignorecase ( getinclude ( ) ) )
if ( o = = null )
import java . util . iterator ;
public properties getproperties ( string key ,
public class extendedproperties extends hashtable
public float getfloat ( string key ,
if ( value instanceof boolean )
if ( token . endswith ( " \ \ " ) )
return this . include ;
put ( key , b ) ;
public void clearproperty ( string key )
string line = readline ( ) . trim ( ) ;
long defaultvalue )
return getboolean ( key , new boolean ( defaultvalue ) ) . booleanvalue ( ) ;
public propertiesreader ( reader reader )
put ( key , s ) ;
if ( l ! = null )
return getproperties ( key , new properties ( ) ) ;
if ( defaults ! = null )
public byte getbyte ( string key )
string line = reader . readproperty ( ) ;
public short getshort ( string key )
integer i = getinteger ( key , null ) ;
continue ;
private void init ( extendedproperties exp ) throws ioexception
new propertiestokenizer ( ( string ) token ) ;
object o = this . get ( key ) ;
if ( ( ( string ) keysaslisted . get ( i ) ) . equals ( key ) )
return i . intvalue ( ) ;
key + " doesn't map to a vector object" ) ;
return isinitialized ;
key + " doesn't map to a double object" ) ;
int equalsign = line . indexof ( ' = ' ) ;
public void combine ( extendedproperties c )
string s = ( ( string ) value ) . tolowercase ( ) ;
return l ;
for ( int i = 0 ; i < keysaslisted . size ( ) ; i + + )
return ( string ) value ;
float defaultvalue )
import java . io . printwriter ;
vector . addelement ( value ) ;
key + " doesn't map to a short object" ) ;
double d = getdouble ( key , null ) ;
int equalsign = token . indexof ( ' = ' ) ;
public extendedproperties ( )
return d ;
return defaultvalue ;
defaults = new extendedproperties ( defaultfile ) ;
this ( file , null ) ;
}
public double getdouble ( string key ,
protected string file ;
while ( thekeys . hasmoreelements ( ) )
extendedproperties c = new extendedproperties ( ) ;
public synchronized void save ( outputstream output ,
return defaults . getbyte ( key , defaultvalue ) ;
file = new file ( basepath + value ) ;
return keysaslisted . iterator ( ) ;
public integer getinteger ( string key ,
return s . shortvalue ( ) ;
protected static string include = "include" ;
import java . io . ioexception ;
return d . doublevalue ( ) ;
isinitialized = true ;
vector v = new vector ( 2 ) ;
if ( value . startswith ( " . " + fileseparator ) )
this . include = inc ;
arraylist matchingkeys = new arraylist ( ) ;
if ( value instanceof string )
clearproperty ( key ) ;
import java . util . properties ;
int def )
public extendedproperties subset ( string prefix )
short s = new short ( ( string ) value ) ;
thewrtr . println ( header ) ;
keysaslisted . remove ( i ) ;
system . out . println ( key + " = > " + value ) ;
currentoutput . append ( currentelement ) ;
return b . bytevalue ( ) ;
if ( value instanceof double )
v . addelement ( o ) ;
long defaultvalue )
enumeration thekeys = keys ( ) ;
integer defaultvalue )
thewrtr . println ( currentoutput . tostring ( ) ) ;
int defaultvalue )
thewrtr . println ( ) ;
if ( "" . equals ( value ) )
return getvector ( key , null ) ;
return defaults . getvector ( key , defaultvalue ) ;
public iterator getkeys ( )
public string getstring ( string key )
public int getinteger ( string key ,
string header )
if ( line . endswith ( " \ \ " ) )
import java . util . nosuchelementexception ;
break ;
key + " doesn't map to a long object" ) ;
remove ( key ) ;
float f = getfloat ( key , null ) ;
class propertiestokenizer extends stringtokenizer
( ( vector ) o ) . addelement ( token ) ;
else if ( s . equals ( "false" ) | | s . equals ( "off" ) | | s . equals ( "no" ) )
if ( value instanceof vector )
addstringproperty ( key , value ) ;
new propertiesreader ( new inputstreamreader ( input ) ) ;
return props ;
buffer . append ( token ) ;
thewrtr . flush ( ) ;
put ( key , token ) ;
return defaults . getboolean ( key , defaultvalue ) ;
if ( s . equals ( "true" ) | | s . equals ( "on" ) | | s . equals ( "yes" ) )
return new string [ 0 ] ;
public object getproperty ( string key )
newkey = ( ( string ) key ) . substring ( prefix . length ( ) + 1 ) ;
enumeration valuesenum = values . elements ( ) ;
key + " doesn't map to a float object" ) ;
return ( ( defaultvalue = = null ) ?
newkey = prefix ;
public int getinteger ( string key )
catch ( nullpointerexception e )
tokens [ i ] = ( string ) vector . elementat ( i ) ;
super ( reader ) ;
if ( ( line . length ( ) ! = 0 ) & & ( line . charat ( 0 ) ! = '#' ) )
long l = new long ( ( string ) value ) ;
key + " doesn't map to a boolean object" ) ;
for ( int i = 0 ; i < tokens . length ; i + + )
if ( output ! = null )
return ( integer ) value ;
return defaults . getstringarray ( key ) ;
currentoutput . append ( ( string ) value ) ;
import java . util . stringtokenizer ;
currentoutput . append ( key ) ;
else if ( o instanceof vector )
while ( hasmoretokens ( ) )
string value = line . substring ( equalsign + 1 ) . trim ( ) ;
key + " doesn't map to a byte object" ) ;
if ( f ! = null )
return buffer . tostring ( ) . trim ( ) ;
return ( short ) value ;
return ( vector ) value ;
throws ioexception
string [ ] tokens = getstringarray ( key ) ;
public long getlong ( string key ,
buffer . append ( token . substring ( 0 , token . length ( ) - 1 ) ) ;
iterator i = getkeys ( ) ;
public boolean getboolean ( string key , boolean defaultvalue )
propertiesreader reader =
key + " doesn't map to a string / vector object" ) ;
public void addproperty ( string key , object token )
public byte getbyte ( string key ,
public boolean isinitialized ( )
if ( !containskey ( key ) )
import java . io . linenumberreader ;
return ( double ) value ;
if ( value instanceof byte )
double defaultvalue )
props . put ( pkey , pvalue ) ;
return getshort ( key , new short ( defaultvalue ) ) . shortvalue ( ) ;
public void display ( )
token +
if ( value instanceof long )
throw new nosuchelementexception (
if ( equalsign > 0 )
return defaults . getshort ( key , defaultvalue ) ;
protected arraylist keysaslisted = new arraylist ( ) ;
import java . io . fileinputstream ;
printwriter thewrtr = new printwriter ( output ) ;
public string [ ] getstringarray ( string key )
return ( byte ) value ;
public int getint ( string name )
import java . io . reader ;
private extendedproperties defaults ;
while ( tokenizer . hasmoretokens ( ) )
return super . hasmoretokens ( ) ;
public iterator getkeys ( string prefix )
byte b = getbyte ( key , null ) ;
v . addelement ( token ) ;
protected boolean isinitialized = false ;
public static extendedproperties convertproperties ( properties p )
public string getstring ( string key ,
"an equals sign" ) ;
string key = ( string ) i . next ( ) ;
value = value . substring ( 2 ) ;
while ( i . hasnext ( ) )
byte defaultvalue )
return b . booleanvalue ( ) ;
import java . util . enumeration ;
integer i = new integer ( ( string ) value ) ;
stringbuffer currentoutput = new stringbuffer ( ) ;
public string getinclude ( )
if ( ( ( string ) key ) . length ( ) = = prefix . length ( ) )
if ( token instanceof string & & ( ( string ) token ) . indexof ( " , " ) > 0 )
try
if ( value instanceof integer )
return getdouble ( key , new double ( defaultvalue ) ) . doublevalue ( ) ;
super ( string , " , " ) ;
this . file = file ;
buffer . append ( " , " ) ;
while ( true )
else if ( value = = null )
return ( boolean ) value ;
return c ;
vector defaultvalue )
return "true" ;
put ( key , d ) ;
import java . io . inputstream ;
string pvalue = token . substring ( equalsign + 1 ) . trim ( ) ;
vector vector ;
load ( new fileinputstream ( file ) ) ;
if ( d ! = null )
v . addelement ( ( string ) value ) ;
key + " doesn't map to a integer object" ) ;
public float getfloat ( string key )
public short getshort ( string key ,
put ( key , i ) ;
properties props = new properties ( defaults ) ;
public long getlong ( string key ,
float f = new float ( ( string ) value ) ;
public double getdouble ( string key ,
class propertiesreader extends linenumberreader
this . load ( new fileinputstream ( file ) ) ;
double d = new double ( ( string ) value ) ;
return "false" ;
import java . io . outputstream ;
keysaslisted . add ( key ) ;
return defaults . getlong ( key , defaultvalue ) ;
return buffer . tostring ( ) ;
if ( header ! = null )
vector v = new vector ( 1 ) ;
public string readproperty ( ) throws ioexception
return ;
return defaults . getstring ( key , defaultvalue ) ;
for ( enumeration e = p . keys ( ) ; e . hasmoreelements ( ) ; )
stringbuffer buffer = new stringbuffer ( ) ;
( string ) valuesenum . nextelement ( ) ;
public void setinclude ( string inc )
string [ ] tokens = new string [ vector . size ( ) ] ;
public synchronized void load ( inputstream input )
basepath = basepath . substring ( 0 , basepath . lastindexof ( fileseparator ) + 1 ) ;
if ( key instanceof string & & ( ( string ) key ) . startswith ( prefix ) )
return f . floatvalue ( ) ;
}
string name = lowercase ? columnname . tolowercase ( ) : columnname ;
return ( string ) columnnamexref . get ( name ) ;
columnnamexref . put ( name , columnname ) ;
private map columnnamexref ;
return resultset . getobject ( columnname ) ;
return name ;
if ( !name . equals ( columnname ) ) {
protected object getobject ( resultset resultset , string name ) throws sqlexception {
string columnname = metadata . getcolumnname ( i ) ;
if ( columnnamexref = = null ) {
} else {
protected string getcolumnname ( string name ) {
columnnamexref = new hashmap ( ) ;
if ( columnnamexref ! = null & & columnnamexref . containskey ( name ) ) {
string columnname = getcolumnname ( name ) ;
public abstractconverter ( object defaultvalue ) {
protected abstract class getdefaulttype ( ) ;
public abstractconverter ( ) {
}
mappedwritemethod = methodutils . getaccessiblemethod ( bean . getclass ( ) , mappedwritemethod ) ;
writemethod = methodutils . getaccessiblemethod ( bean . getclass ( ) , writemethod ) ;
return ( methodutils . getaccessiblemethod ( clazz , descriptor . getreadmethod ( ) ) ) ;
method getwritemethod ( class clazz , propertydescriptor descriptor ) {
method readmethod = getreadmethod ( bean . getclass ( ) , descriptor ) ;
method readmethod = getreadmethod ( bean . getclass ( ) , desc ) ;
return ( methodutils . getaccessiblemethod ( clazz , descriptor . getwritemethod ( ) ) ) ;
readmethod = methodutils . getaccessiblemethod ( bean . getclass ( ) , readmethod ) ;
method writemethod = getwritemethod ( bean . getclass ( ) , desc ) ;
method getreadmethod ( class clazz , propertydescriptor descriptor ) {
method writemethod = getwritemethod ( bean . getclass ( ) , descriptor ) ;
method mappedwritemethod = null ;
return mappedwritemethodref . get ( ) ;
class clazz = ( class ) classref . get ( ) ;
if ( m ! = null ) {
if ( types . length = = 2 ) {
private reference methodref ;
mappedreadmethodref = new mappedmethodreference ( mappedgetter ) ;
private string methodname ;
method mappedreadmethod = null ;
if ( writeparamtyperef ! = null ) {
paramtypes = string class parameter ;
methodref = new softreference ( m ) ;
}
import java . lang . ref . weakreference ;
mappedwritemethodref = new mappedmethodreference ( mappedsetter ) ;
paramtypes = new class [ ] { string . class , ( class ) writeparamtyperef . get ( ) } ;
mappedwritemethodref = new mappedmethodreference ( mappedwritemethod ) ;
return ( class ) mappedpropertytyperef . get ( ) ;
method m = ( method ) methodref . get ( ) ;
writeparamtyperef = new weakreference ( types [ 1 ] ) ;
return m ;
method mappedreadmethod = getmappedreadmethod ( ) ;
private reference classref ;
method mappedwritemethod = getmappedwritemethod ( ) ;
} else {
private mappedmethodreference mappedwritemethodref ;
m = clazz . getmethod ( methodname , paramtypes ) ;
import java . lang . ref . reference ;
mappedmethodreference ( method m ) {
methodname = m . getname ( ) ;
classname + " could not be reconstructed - method not found" ) ;
if ( methodref = = null ) {
private method get ( ) {
private string classname ;
throw new runtimeexception ( "method " + methodname + " for " +
return mappedreadmethodref . get ( ) ;
} catch ( nosuchmethodexception e ) {
private reference writeparamtyperef ;
if ( clazz = = null ) {
mappedreadmethodref = new mappedmethodreference ( mappedreadmethod ) ;
classname = m . getdeclaringclass ( ) . getname ( ) ;
classref = new weakreference ( m . getdeclaringclass ( ) ) ;
import java . lang . ref . softreference ;
if ( m = = null ) {
mappedpropertytyperef = new softreference ( mappedpropertytype ) ;
class [ ] types = m . getparametertypes ( ) ;
return null ;
classname + " could not be reconstructed - class reference has gone" ) ;
class mappedpropertytype = null ;
private static class mappedmethodreference {
private mappedmethodreference mappedreadmethodref ;
class [ ] paramtypes = null ;
private reference mappedpropertytyperef ;
try {
}
clazz = reloadclass ( ) ;
classloader classloader = thread . currentthread ( ) . getcontextclassloader ( ) ;
private class reloadclass ( ) {
return null ;
if ( clazz = = null ) {
classref = new weakreference ( clazz ) ;
return classloader . loadclass ( classname ) ;
} catch ( throwable t ) {
if ( classloader ! = null ) {
if ( clazz ! = null ) {
try {
if ( posdouble ! = 0 & & posdouble < float . min value | | posdouble > float . max value ) {
} else if ( target . getclass ( ) . isarray ( ) & & index > = 0 ) {
type = array . get ( target , index ) . getclass ( ) ;
map . entry < ? , ? > entry = ( entry < ? , ? > ) entries . next ( ) ;
nestedbean = getpropertyofmapbean ( ( map < ? , ? > ) bean , next ) ;
private weakfasthashmap < class < ? > , propertydescriptor [ ] > descriptorscache = null ;
@ suppresswarnings ( "unchecked" )
protected void setpropertyofmapbean ( map < string , object > bean , string propertyname , object value )
class < ? > [ ] partypes = method . getparametertypes ( ) ;
list < object > list = ( list < object > ) obj ;
bean = getpropertyofmapbean ( ( map < ? , ? > ) bean , name ) ;
descriptorscache = new weakfasthashmap < class < ? > , propertydescriptor [ ] > ( ) ;
}
private static map < string , object > topropertymap ( object obj ) {
class < ? > type = descriptor . gettype ( ) ;
class < ? > [ ] parametertypes = methods [ j ] . getparametertypes ( ) ;
descriptorscache . get ( beanclass ) ;
>
result = ( ( java . util . map < ? , ? > ) invokeresult ) . get ( key ) ;
private static final class < ? > [ ] list class parameter = new class [ ] { java . util . list . class } ;
public class < ? > getpropertytype ( object bean , string name )
iterator < ? > entries = ( ( map < ? , ? > ) orig ) . entryset ( ) . iterator ( ) ;
protected object getpropertyofmapbean ( map < ? , ? > bean , string propertyname )
private weakfasthashmap < class < ? > , fasthashmap > mappeddescriptorscache = null ;
getpropertydescriptors ( class < ? > beanclass ) {
return mappeddescriptorscache . get ( beanclass ) ;
public class < ? > getpropertyeditorclass ( object bean , string name )
map . put ( key , value ) ;
return ( ( list < ? > ) bean ) . get ( index ) ;
java . util . map < string , object > map = topropertymap ( invokeresult ) ;
return map ;
import java . util . map . entry ;
map < string , object > map = ( map < string , object > ) obj ;
method getreadmethod ( class < ? > clazz , propertydescriptor descriptor ) {
mappeddescriptorscache = new weakfasthashmap < class < ? > , fasthashmap > ( ) ;
method getwritemethod ( class < ? > clazz , propertydescriptor descriptor ) {
public map < string , object > describe ( object bean )
private static list < object > toobjectlist ( object obj ) {
list . set ( index , value ) ;
list < object > list = toobjectlist ( array ) ;
public fasthashmap getmappedpropertydescriptors ( class < ? > beanclass ) {
list < object > list = toobjectlist ( bean ) ;
setpropertyofmapbean ( topropertymap ( bean ) , name , value ) ;
return list ;
return ( ( java . util . list < ? > ) value ) . get ( index ) ;
map < string , object > description = new hashmap < string , object > ( ) ;
private static final class < ? > [ ] empty class parameters = new class [ 0 ] ;
return doublemetaphone ( value1 , alternate ) . equals ( doublemetaphone ( value2 , alternate ) ) ;
if ( contains ( value , index + 1 , 4 , "heim" , "hoek" , "holm" , "holz" ) ) {
{ "l" , "t" , "k" , "s" , "n" , "m" , "b" , "z" } ;
private int handlep ( string value , doublemetaphoneresult result , int index ) {
private int handleh ( string value , doublemetaphoneresult result , int index ) {
contains ( value , 0 , 3 , "sch" ) | |
private int handlesc ( string value , doublemetaphoneresult result , int index ) {
( charat ( value , index + 1 ) = = 'a' | | charat ( value , index + 1 ) = = 'o' ) ) {
} else if ( ( index = = 0 & & contains ( value , index + 1 , 1 , "m" , "n" , "l" , "w" ) ) | |
if ( contains ( value , 0 , 4 , "van " , "von " ) | |
} else if ( contains ( value , index , 2 , "th" ) | | contains ( value , index , 3 , "tth" ) ) {
} else if ( !contains ( value , index + 1 , 1 , l t k s n m b z ) & &
( ( index + 1 ) = = value . length ( ) - 1 | | contains ( value , index + 2 , 2 , "er" ) ) ;
public boolean isdoublemetaphoneequal ( string value1 , string value2 , boolean alternate ) {
private int handlez ( string value , doublemetaphoneresult result , int index , boolean slavogermanic ) {
private int handlech ( string value , doublemetaphoneresult result , int index ) {
private int handled ( string value , doublemetaphoneresult result , int index ) {
this . alternate . length ( ) > = this . maxlength ;
} else if ( index = = 0 & &
contains ( value , index + 1 , 2 , "et" ) ) {
return contains ( value , start , length , new string [ ] { criteria } ) ;
return contains ( value , start , length , new string [ ] { criteria1 , criteria2 } ) ;
private int handlet ( string value , doublemetaphoneresult result , int index ) {
private int handler ( string value , doublemetaphoneresult result , int index , boolean slavogermanic ) {
( slavogermanic & & ( index > 0 & & charat ( value , index - 1 ) ! = 't' ) ) ) {
{ "es" , "ep" , "eb" , "el" , "ey" , "ib" , "il" , "in" , "ie" , "ei" , "er" } ;
( charat ( value , index + 1 ) = = 'y' | |
return ( ( contains ( value , 0 , 4 , "van " , "von " ) | | contains ( value , 0 , 3 , "sch" ) ) | |
private int handlecc ( string value , doublemetaphoneresult result , int index ) {
contains ( value , index - 1 , 5 , "ewski" , "ewsky" , "owski" , "owsky" ) | |
private int handlex ( string value , doublemetaphoneresult result , int index ) {
private int handlew ( string value , doublemetaphoneresult result , int index ) {
return contains ( value , start , length , new string [ ] { criteria1 , criteria2 , criteria3 } ) ;
if ( contains ( value , index + 3 , 2 , "oo" , "er" , "en" , "uy" , "ed" , "em" ) ) {
if ( index = = value . length ( ) - 1 & & contains ( value , index - 2 , 2 , "ai" , "oi" ) ) {
contains ( value , index + 1 , 1 , "z" ) ) {
string criteria1 , string criteria2 , string criteria3 ) {
contains ( value , index + 1 , 2 , es ep eb el ey ib il in ie ei er ) ) ) {
private int handleaeiouy ( doublemetaphoneresult result , int index ) {
private int handlel ( string value , doublemetaphoneresult result , int index ) {
private int handles ( string value , doublemetaphoneresult result , int index , boolean slavogermanic ) {
!contains ( value , index - 1 , 1 , "s" , "k" , "l" ) ) {
private int handlegh ( string value , doublemetaphoneresult result , int index ) {
private static boolean contains ( string value , int start , int length , string criteria ) {
{ "gn" , "kn" , "pn" , "wr" , "ps" } ;
private int handlec ( string value , doublemetaphoneresult result , int index ) {
{ "l" , "r" , "n" , "m" , "b" , "h" , "f" , "v" , "w" , " " } ;
private int handleg ( string value , doublemetaphoneresult result , int index , boolean slavogermanic ) {
if ( contains ( value , index + 1 , 2 , "zo" , "zi" , "za" ) | |
private int handlej ( string value , doublemetaphoneresult result , int index , boolean slavogermanic ) {
new string [ ] { criteria1 , criteria2 , criteria3 , criteria4 } ) ;
public soundex ( final char [ ] mapping ) {
public rulesapplication ( final list < rule > finalrules , final charsequence input ,
final stringbuilder sb = new stringbuilder ( ) ;
final int index = ch - 'a' ;
public string encode ( final string input ) {
expr : for ( final rule . phoneme left : this . phonemes ) {
public charsequence subsequence ( final int start , final int end ) {
final boolean found = rulesapplication . isfound ( ) ;
final set < rule . phoneme > phonemes = new treeset < rule . phoneme > ( rule . phoneme . comparator ) ;
final string combined = "d" + remainder ;
final string [ ] parts = aword . split ( "'" ) ;
final set < rule . phoneme > newphonemes = new linkedhashset < rule . phoneme > ( ) ;
for ( final rule rule : this . finalrules ) {
public phoneticengine ( final nametype nametype , final ruletype ruletype , final boolean concat , final int maxphonemes ) {
final char firstcode = this . map ( prehwchar ) ;
for ( final rule . phoneme ph : this . phonemes ) {
final string combined = l + remainder ;
final iterator < string > si = strings . iterator ( ) ;
private final int maxphonemes ;
public string encode ( final string str ) {
for ( final string aword : words ) {
final string pattern = rule . getpattern ( ) ;
private phonemebuilder ( final set < rule . phoneme > phonemes ) {
private phonemebuilder applyfinalrules ( final phonemebuilder phonemebuilder , final list < rule > finalrules ) {
final string remainder = input . substring ( 2 ) ;
final char out [ ] = { '0' , '0' , '0' , '0' } ;
private static string join ( final iterable < string > strings , final string sep ) {
public void setmaxlength ( final int maxlength ) {
final char mappedchar = this . map ( str . charat ( index ) ) ;
final rule . phoneme join = left . join ( right ) ;
final rulesapplication rulesapplication =
public phonemebuilder append ( final charsequence str ) {
private char getmappingcode ( final string str , final int index ) {
public object encode ( final object obj ) throws encoderexception {
final string remainder = input . substring ( l . length ( ) + 1 ) ;
public soundex ( final string mapping ) {
public phonemebuilder apply ( final rule . phonemeexpr phonemeexpr , final int maxphonemes ) {
for ( final rule . phoneme phoneme : phonemebuilder . getphonemes ( ) ) {
final char prehwchar = str . charat ( index - 2 ) ;
private char map ( final char ch ) {
for ( final string l : name prefixes . get ( this . nametype ) ) {
final phonemebuilder phonemebuilder , final int i , final int maxphonemes ) {
final charsequence inputcache = cachesubsequence ( input ) ;
public int difference ( final string s1 , final string s2 ) throws encoderexception {
final char hwchar = str . charat ( index - 1 ) ;
final string lastpart = parts [ parts . length - 1 ] ;
for ( final string word : words2 ) {
final stringbuilder result = new stringbuilder ( ) ;
final languages . languageset languageset = this . lang . guesslanguages ( input ) ;
public char charat ( final int index ) {
public phoneticengine ( final nametype nametype , final ruletype ruletype , final boolean concat ) {
for ( final rule . phoneme right : phonemeexpr . getphonemes ( ) ) {
public static phonemebuilder empty ( final languages . languageset languages ) {
final charsequence phonemetext = cachesubsequence ( phoneme . getphonemetext ( ) ) ;
}
return cs1 . equals ( cs2 ) ;
if ( cs1 = = cs2 ) {
return charsequenceutils . regionmatches ( cs1 , false , 0 , cs2 , 0 , math . max ( cs1 . length ( ) , cs2 . length ( ) ) ) ;
return false ;
if ( cs1 = = null | | cs2 = = null ) {
public static boolean equals ( final charsequence cs1 , final charsequence cs2 ) {
return true ;
if ( cs1 instanceof string & & cs2 instanceof string ) {
throw new illegalargumentexception ( "malformed folding statement - " +
rulemapping . put ( patternkey , rules ) ;
final branch branch = new branch ( ) ;
return false ;
final string rawline = scanner . nextline ( ) ;
final string [ ] result = new string [ currentbranches . size ( ) ] ;
if ( rule . matches ( inputcontext ) ) {
branch . lastreplacement = this . lastreplacement ;
this . replacementdefault = replacementdefault . split ( " \ \ | " ) ;
public boolean matches ( final string context ) {
public daitchmokotoffsoundex ( ) {
"problem parsing line '" + currentline + "' in " + location , e ) ;
int index = 0 ;
if ( inmultilinecomment ) {
cachedstring = null ;
final string leftcharacter = parts [ 0 ] ;
currentbranches . add ( new branch ( ) ) ;
final string replacementdefault ) {
return rule2 . getpatternlength ( ) - rule1 . getpatternlength ( ) ;
}
currentbranches . clear ( ) ;
private string lastreplacement ;
final string [ ] parts = line . split ( " \ \ s + " ) ;
private static final map < character , list < rule > > rules = new hashmap < character , list < rule > > ( ) ;
private static final class branch {
final boolean branchingrequired = replacements . length > 1 & & branching ;
inmultilinecomment = false ;
sb . append ( branch ) ;
currentbranches . addall ( nextbranches ) ;
} else {
str = str . substring ( 1 ) ;
final list < rule > rules = rules . get ( ch ) ;
final int nextindex = getpatternlength ( ) ;
public branch createbranch ( ) {
throw new illegalargumentexception ( "unable to load resource : " + resource file ) ;
private branch ( ) {
if ( folding & & foldings . containskey ( ch ) ) {
builder . append ( '0' ) ;
final inputstream rulesis = daitchmokotoffsoundex . class . getclassloader ( ) . getresourceasstream ( resource file ) ;
final boolean force = ( lastchar = = 'm' & & ch = = 'n' ) | | ( lastchar = = 'n' & & ch = = 'm' ) ;
return tostring ( ) . equals ( ( ( branch ) other ) . tostring ( ) ) ;
import java . util . collections ;
public object encode ( final object obj ) throws encoderexception {
inmultilinecomment = true ;
index + = rule . getpatternlength ( ) - 1 ;
this ( true ) ;
branch . builder . append ( tostring ( ) ) ;
this . replacementbeforevowel = replacementbeforevowel . split ( " \ \ | " ) ;
ch = foldings . get ( ch ) ;
final boolean append = lastreplacement = = null | | !lastreplacement . endswith ( replacement ) | | forceappend ;
if ( branching ) {
import java . util . comparator ;
private static final string comment = " / / " ;
import java . util . map ;
lastchar = ch ;
if ( line . contains ( " = " ) ) {
"patterns are not single characters : " + rawline + " in " + location ) ;
arrays . aslist ( replacementbeforevowel ) , arrays . aslist ( replacementdefault ) ) ;
static {
final boolean nextcharisvowel = nextindex < context . length ( ) ? isvowel ( context . charat ( nextindex ) ) : false ;
nextbranches . add ( nextbranch ) ;
protected rule ( final string pattern , final string replacementatstart , final string replacementbeforevowel ,
parserules ( scanner , resource file , rules , foldings ) ;
if ( append & & builder . length ( ) < max length ) {
final string replacement1 = stripquotes ( parts [ 1 ] ) ;
nextbranch . processnextreplacement ( nextreplacement , force ) ;
if ( builder . length ( ) > max length ) {
try {
public void processnextreplacement ( final string replacement , final boolean forceappend ) {
public int compare ( final rule rule1 , final rule rule2 ) {
private final string pattern ;
rules = new arraylist < rule > ( ) ;
final map < character , list < rule > > rulemapping , final map < character , character > asciifoldings ) {
final stringbuilder sb = new stringbuilder ( ) ;
private static final int max length = 6 ;
public string encode ( final string source ) {
@ suppresswarnings ( "unchecked" )
while ( builder . length ( ) < max length ) {
public boolean equals ( final object other ) {
import java . util . hashmap ;
for ( final string branch : branches ) {
if ( atstart ) {
import java . util . scanner ;
private static final string double quote = " \ "" ;
private string [ ] soundex ( final string source , final boolean branching ) {
import org . apache . commons . codec . encoderexception ;
rules . add ( r ) ;
char lastchar = ' \ 0' ;
cachedstring = builder . tostring ( ) ;
} ) ;
private final stringbuilder builder ;
final int cmti = line . indexof ( comment ) ;
import java . util . set ;
final char ch = input . charat ( index ) ;
this . folding = folding ;
return true ;
final char patternkey = r . pattern . charat ( 0 ) ;
private static final string multiline comment end = " * / " ;
return branch ;
return cachedstring ;
final string replacement3 = stripquotes ( parts [ 3 ] ) ;
if ( source = = null ) {
final string inputcontext = input . substring ( index ) ;
for ( int index = 0 ; index < input . length ( ) ; index + + ) {
public class daitchmokotoffsoundex implements stringencoder {
return ch = = 'a' | | ch = = 'e' | | ch = = 'i' | | ch = = 'o' | | ch = = 'u' ;
return replacementatstart ;
ch = character . tolowercase ( ch ) ;
str = str . substring ( 0 , str . length ( ) - 1 ) ;
throw new illegalargumentexception ( "malformed folding statement split into " + parts . length +
throw new illegalargumentexception ( "malformed rule statement split into " + parts . length +
package org . apache . commons . codec . language ;
string line = rawline ;
private static final map < character , character > foldings = new hashmap < character , character > ( ) ;
builder . delete ( max length , builder . length ( ) ) ;
final string rightcharacter = parts [ 1 ] ;
private static final string resource file = "org / apache / commons / codec / language / dmrules . txt" ;
public daitchmokotoffsoundex ( final boolean folding ) {
final string input = cleanup ( source ) ;
import java . util . arraylist ;
import java . util . linkedhashset ;
return pattern . length ( ) ;
final scanner scanner = new scanner ( rulesis , charencoding . utf 8 ) ;
if ( leftcharacter . length ( ) ! = 1 | | rightcharacter . length ( ) ! = 1 ) {
break ;
private boolean isvowel ( final char ch ) {
sb . append ( ' | ' ) ;
if ( nextcharisvowel ) {
if ( str . endswith ( double quote ) ) {
"parameter supplied to daitchmokotoffsoundex encode is not of type java . lang . string" ) ;
private final boolean folding ;
if ( ! ( other instanceof branch ) ) {
return sb . tostring ( ) ;
final set < branch > currentbranches = new linkedhashset < branch > ( ) ;
for ( final string nextreplacement : replacements ) {
public string [ ] getreplacements ( final string context , final boolean atstart ) {
import java . util . list ;
return string . format ( " % s = ( % s , % s , % s ) " , pattern , arrays . aslist ( replacementatstart ) ,
final string pattern = stripquotes ( parts [ 0 ] ) ;
result [ index + + ] = branch . tostring ( ) ;
" parts : " + rawline + " in " + location ) ;
public string tostring ( ) {
while ( scanner . hasnextline ( ) ) {
private final string [ ] replacementbeforevowel ;
private static final string multiline comment start = " / * " ;
final rule r = new rule ( pattern , replacement1 , replacement2 , replacement3 ) ;
if ( cmti > = 0 ) {
final string [ ] parts = line . split ( " = " ) ;
public void finish ( ) {
for ( final map . entry < character , list < rule > > rule : rules . entryset ( ) ) {
final string replacement2 = stripquotes ( parts [ 2 ] ) ;
lastreplacement = replacement ;
builder = new stringbuilder ( ) ;
this . pattern = pattern ;
if ( cachedstring = = null ) {
currentline + + ;
if ( parts . length ! = 4 ) {
import org . apache . commons . codec . charencoding ;
scanner . close ( ) ;
return replacementbeforevowel ;
if ( + + index < branches . length ) {
throw new illegalstateexception (
} catch ( final illegalargumentexception e ) {
@ override
if ( str . startswith ( double quote ) ) {
if ( ! ( obj instanceof string ) ) {
import java . io . inputstream ;
private string cleanup ( final string input ) {
sb . append ( ch ) ;
if ( parts . length ! = 2 ) {
return null ;
if ( line . startswith ( multiline comment start ) ) {
if ( this = = other ) {
final string [ ] branches = soundex ( source , true ) ;
return context . startswith ( pattern ) ;
return encode ( ( string ) obj ) ;
return replacementdefault ;
return result ;
this . replacementatstart = replacementatstart . split ( " \ \ | " ) ;
continue ;
final branch nextbranch = branchingrequired ? branch . createbranch ( ) : branch ;
if ( rulesis = = null ) {
import org . apache . commons . codec . stringencoder ;
collections . sort ( rulelist , new comparator < rule > ( ) {
private final string [ ] replacementdefault ;
for ( final rule rule : rules ) {
throw new encoderexception (
private static void parserules ( final scanner scanner , final string location ,
if ( line . endswith ( multiline comment end ) ) {
private final string [ ] replacementatstart ;
for ( final branch branch : currentbranches ) {
public string soundex ( final string source ) {
for ( char ch : input . tochararray ( ) ) {
/ /
if ( rules = = null ) {
private static final class rule {
builder . append ( replacement ) ;
boolean inmultilinecomment = false ;
public int getpatternlength ( ) {
import java . util . arrays ;
nextbranches . clear ( ) ;
list < rule > rules = rulemapping . get ( patternkey ) ;
branch . finish ( ) ;
final list < rule > rulelist = rule . getvalue ( ) ;
lastreplacement = null ;
line = line . trim ( ) ;
return soundex ( source , false ) [ 0 ] ;
public int hashcode ( ) {
int currentline = 0 ;
final string [ ] replacements = rule . getreplacements ( inputcontext , lastchar = = ' \ 0' ) ;
final list < branch > nextbranches = branching ? new arraylist < branch > ( ) : collections . empty list ;
line = line . substring ( 0 , cmti ) ;
asciifoldings . put ( leftcharacter . charat ( 0 ) , rightcharacter . charat ( 0 ) ) ;
if ( character . iswhitespace ( ch ) ) {
return str ;
private string cachedstring ;
private static string stripquotes ( string str ) {
if ( line . length ( ) = = 0 ) {
return tostring ( ) . hashcode ( ) ;
}
for ( int i = index - 1 ; i > = 0 ; i - - ) {
final char prevchar = str . charat ( i ) ;
if ( this . map ( prevchar ) = = mappedchar ) {
if ( 'h'! = prevchar & & 'w'! = prevchar ) {
break ;
} else if ( mapped ! = '#' & & mapped ! = last ) {
public static final string us english mapping string = "0123012#02245501262301#202" ;
if ( mapped = = '0' ) {
last = this . map ( str . charat ( 0 ) ) ;
last = mapped ;
mapped = this . map ( str . charat ( incount + + ) ) ;
out [ count + + ] = mapped ;
final stringbuilder sb = new stringbuilder ( ) ;
rules = new arraylist < rule > ( ) ;
lines . put ( patternkey , rules ) ;
list < rule > rules = lines . get ( patternkey ) ;
return sb . tostring ( ) ;
return parsephoneme ( ph ) ;
return input . equals ( content ) ;
}
final rule r = new rule ( pat , lcon , rcon , ph ) {
throw new illegalstateexception ( "problem parsing line '" + currentline + "' in " +
public string tostring ( ) {
final string rcon = stripquotes ( parts [ 2 ] ) ;
location , e ) ;
rules . add ( r ) ;
sb . append ( ' } ' ) ;
final phonemeexpr ph = parsephonemeexpr ( stripquotes ( parts [ 3 ] ) ) ;
if ( rules = = null ) {
sb . append ( "rule" ) ;
sb . append ( " { line = " ) . append ( myline ) ;
sb . append ( " , lcon = '" ) . append ( lcon ) . append ( ' \ '' ) ;
final string patternkey = r . pattern . substring ( 0 , 1 ) ;
sb . append ( " , rcon = '" ) . append ( rcon ) . append ( ' \ '' ) ;
private final int myline = cline ;
return new phoneme ( ph , languages . any language ) ;
final int cline = currentline ;
public boolean ismatch ( final charsequence input ) {
} catch ( final illegalargumentexception e ) {
@ override
lines . putall ( parserules ( createscanner ( incl ) , location + " - > " + incl ) ) ;
final string pat = stripquotes ( parts [ 0 ] ) ;
return new rpattern ( ) {
} ;
final string lcon = stripquotes ( parts [ 1 ] ) ;
sb . append ( " , pat = '" ) . append ( pat ) . append ( ' \ '' ) ;
private final string loc = location ;
sb . append ( " , loc = '" ) . append ( loc ) . append ( ' \ '' ) ;
try {
char lastchar = char ignore ;
char lastcode = char first pos ;
if ( lastcode = = char first pos ) {
if ( code ! = char ignore & & ( lastcode ! = code & & ( code ! = '0' | | lastcode = = char first pos ) | | code < '0' | | code > '8' ) ) {
continue ;
nextchar = char ignore ;
code = char ignore ;
final char char ignore = ' - ' ;
final char char first pos = ' / ' ;
throw new runtimeexception ( ) ;
putarchiveentry ( archiveentry , false ) ;
private void writelocalfileheader ( ziparchiveentry ze , boolean phased ) throws ioexception {
final zip64mode effectivemode = geteffectivezip64mode ( entry . entry ) ;
putshort ( versionneededtoextract ( zipmethod , haszip64extra ( ze ) ) , buf , lfh version needed offset ) ;
putarchiveentry ( ae , is2phasesource ) ;
closeentry ( actuallyneedszip64 , false ) ;
} else if ( phased ) {
closeentry ( actuallyneedszip64 , phased ) ;
putlong ( ze . getcrc ( ) , buf , lfh crc offset ) ;
boolean is2phasesource = ae . getcrc ( ) ! = - 1
private byte [ ] createlocalfileheader ( ziparchiveentry ze , bytebuffer name , boolean encodable ,
private void closeentry ( boolean actuallyneedszip64 , boolean phased ) throws ioexception {
}
closecopiedentry ( is2phasesource ) ;
& & ae . getcompressedsize ( ) ! = - 1 ;
private boolean istoolageforzip32 ( ziparchiveentry ziparchiveentry ) {
putlong ( ze . getsize ( ) , buf , lfh original size offset ) ;
size = new zipeightbyteinteger ( entry . entry . getsize ( ) ) ;
} else {
system . arraycopy ( lzero , 0 , buf , lfh compressed size offset , word ) ;
system . arraycopy ( lzero , 0 , buf , lfh original size offset , word ) ;
putshort ( initial version , buf , lfh version needed offset ) ;
if ( haszip64extra ( ae ) ) {
private boolean iszip64required ( ziparchiveentry entry1 , zip64mode requestedmode ) {
final boolean actuallyneedszip64 = iszip64required ( entry . entry , effectivemode ) ;
final boolean actuallyneedszip64 = handlesizesandcrc ( byteswritten , realcrc , effectivemode ) ;
ziplong . zip64 magic . putlong ( buf , lfh original size offset ) ;
if ( phased ) {
zipeightbyteinteger compressedsize = zipeightbyteinteger . zero ;
boolean phased ) {
ziplong . zip64 magic . putlong ( buf , lfh compressed size offset ) ;
compressedsize = new zipeightbyteinteger ( entry . entry . getcompressedsize ( ) ) ;
private void putarchiveentry ( archiveentry archiveentry , boolean phased ) throws ioexception {
compressedsize = size ;
writelocalfileheader ( ze , false ) ;
z64 . setcompressedsize ( compressedsize ) ;
& & ae . getsize ( ) ! = archiveentry . size unknown
putlong ( ze . getsize ( ) , buf , lfh compressed size offset ) ;
if ( phased & & !iszip64required ( entry . entry , zip64mode ) ) {
if ( haszip64extra ( entry . entry ) ) {
final byte [ ] localheader = createlocalfileheader ( ze , name , encodable , phased ) ;
ae . removeextrafield ( zip64extendedinformationextrafield . header id ) ;
} else if ( entry . entry . getmethod ( ) = = stored
} else if ( zipmethod = = deflated | | raf ! = null ) {
return ziparchiveentry . getsize ( ) > = zip64 magic | | ziparchiveentry . getcompressedsize ( ) > = zip64 magic ;
if ( !phased & & raf ! = null ) {
private void closecopiedentry ( boolean phased ) throws ioexception {
& & entry . entry . getsize ( ) ! = archiveentry . size unknown ) {
writelocalfileheader ( ( ziparchiveentry ) archiveentry , phased ) ;
putlong ( ze . getcompressedsize ( ) , buf , lfh compressed size offset ) ;
return requestedmode = = zip64mode . always | | istoolageforzip32 ( entry1 ) ;
private byte [ ] createcentralfileheader ( ziparchiveentry ze ) throws ioexception {
writecentraldirectoryinchunks ( ) ;
| | ze . getcompressedsize ( ) > = zip64 magic
writecounted ( centralfileheader ) ;
private void writecentraldirectoryinchunks ( ) throws ioexception {
bytearrayoutputstream bytearrayoutputstream = new bytearrayoutputstream ( 70 * num per write ) ;
return createcentralfileheader ( ze , getname ( ze ) , lfhoffset , needszip64extra ) ;
system . arraycopy ( zero , 0 , buf , cfh disk number offset , short ) ;
}
import java . util . iterator ;
bytearrayoutputstream . reset ( ) ;
. archive too big message ) ;
while ( iterator . hasnext ( ) ) {
import java . io . bytearrayoutputstream ;
bytearrayoutputstream . write ( createcentralfileheader ( ze ) ) ;
| | ze . getsize ( ) > = zip64 magic
iterator < ziparchiveentry > iterator = entries . iterator ( ) ;
int count = 0 ;
count = 0 ;
writecounted ( bytearrayoutputstream . tobytearray ( ) ) ;
ziparchiveentry ze ;
byte [ ] centralfileheader = createcentralfileheader ( ze ) ;
if ( count > num per write ) {
int num per write = 1000 ;
} ;
count + + ;
ze = iterator . next ( ) ;
final byte [ ] copybuffer = new byte [ 32768 ] ;
| | lfhoffset > = zip64 magic ;
e . setextrafields ( getallextrafieldsnocopy ( ) ) ;
if ( copy ! = null ) {
return unparseableextra ! = null ? getmergedfields ( ) : extrafields ;
private zipextrafield [ ] getunparseableonly ( ) {
extrafields = newfields . toarray ( new zipextrafield [ newfields . size ( ) ] ) ;
final zipextrafield [ ] allextrafieldsnocopy = getallextrafieldsnocopy ( ) ;
& & !name . contains ( " / " ) ) {
return extrafieldutils . mergecentraldirectorydata ( getallextrafieldsnocopy ( ) ) ;
return extrafield ;
system . arraycopy ( copy , 0 , extrafields , 1 , extrafields . length - 1 ) ;
extrafields = newresult . toarray ( new zipextrafield [ newresult . size ( ) ] ) ;
return extrafields ;
}
extrafields [ 0 ] = ze ;
private zipextrafield [ ] getallextrafieldsnocopy ( ) {
zipextrafields [ zipextrafields . length ] = unparseableextra ;
return includeunparseable ?
getallextrafields ( ) :
private static final zipextrafield [ ] noextrafields = new zipextrafield [ 0 ] ;
private zipextrafield [ ] getmergedfields ( ) {
} else {
return getunparseableonly ( ) ;
extrafields = new zipextrafield [ ] { ze } ;
list < zipextrafield > newresult = new arraylist < zipextrafield > ( ) ;
removeextrafield ( ze . getheaderid ( ) ) ;
if ( extrafields . length = = newresult . size ( ) ) {
return noextrafields ;
return ( allextrafieldsnocopy = = extrafields ) ? copyof ( allextrafieldsnocopy ) : allextrafieldsnocopy ;
private zipextrafield [ ] copyof ( zipextrafield [ ] src ) {
final zipextrafield [ ] parseableextrafields = getparseableextrafields ( ) ;
newresult . add ( extrafield ) ;
private zipextrafield [ ] extrafields ;
super . setextra ( extrafieldutils . mergelocalfiledatadata ( getallextrafieldsnocopy ( ) ) ) ;
extrafields = new zipextrafield [ newlen ] ;
final zipextrafield [ ] zipextrafields = arrays . copyof ( extrafields , extrafields . length + 1 ) ;
return arrays . copyof ( parseableextrafields , parseableextrafields . length ) ;
private zipextrafield [ ] getallextrafields ( ) {
if ( getextrafield ( ze . getheaderid ( ) ) ! = null ) {
extrafields = zipextrafields ;
if ( type . equals ( extrafield . getheaderid ( ) ) ) {
newfields . add ( field ) ;
setextrafields ( getallextrafieldsnocopy ( ) ) ;
getparseableextrafields ( ) ;
zipextrafield [ ] copy = extrafields ;
private zipextrafield [ ] getparseableextrafields ( ) {
int newlen = extrafields ! = null ? extrafields . length + 1 : 1 ;
list < zipextrafield > newfields = new arraylist < zipextrafield > ( ) ;
zipextrafields [ zipextrafields . length - 1 ] = ze ;
for ( zipextrafield extrafield : extrafields ) {
return arrays . copyof ( src , src . length ) ;
return unparseableextra = = null ? noextrafields : new zipextrafield [ ] { unparseableextra } ;
if ( extrafields = = null ) {
return zipextrafields ;
if ( !type . equals ( extrafield . getheaderid ( ) ) ) {
final long lfhoffset = offsets . get ( ze ) ;
}
+ "' is too big ( > "
failforbignumberwithposixmessage ( "group id" , entry . getgroupid ( ) , tarconstants . maxid ) ;
private void failforbignumber ( string field , long value , long maxvalue , string additionalmsg ) {
private void failforbignumberwithposixmessage ( string field , long value , long maxvalue ) {
failforbignumber ( field , value , maxvalue , "" ) ;
failforbignumber ( field , value , maxvalue , " use star or posix extensions to overcome this limit" ) ;
+ maxvalue + " ) . " + additionalmsg ) ;
private final zipencoding zipencoding ;
this . encoding = encoding ;
currentry . setlinkname ( zipencoding . decode ( longlinkdata ) ) ;
this . zipencoding = zipencodinghelper . getzipencoding ( encoding ) ;
currentry . setname ( zipencoding . decode ( longnamedata ) ) ;
final string encoding ;
currentry = new tararchiveentry ( headerbuf , zipencoding ) ;
}
if ( lzmautils . matches ( signature , signaturelength ) & &
lzmautils . islzmacompressionavailable ( ) ) {
return new lzmacompressorinputstream ( in ) ;
import org . apache . commons . compress . compressors . lzma . lzmautils ;
}
return new deflatecompressorinputstream ( in ) ;
if ( deflatecompressorinputstream . matches ( signature , signaturelength ) ) {
this . hashsize = zipshort . getvalue ( data , offset + 14 ) ;
int erdsize = zipshort . getvalue ( data , offset + ivsize + 14 ) ;
for ( int j = 0 ; j < this . hashsize ; j + + ) {
}
private int hashsize ;
return ziputil . copy ( localdata ) ;
} else {
private int format ;
system . arraycopy ( data , offset + ivsize + 22 + erdsize + resize , this . vdata , 0 , vsize - 4 ) ;
public hashalgorithm gethashalgorithm ( ) {
public encryptionalgorithm getencryptionalgorithm ( ) {
private byte ivdata [ ] ;
this . vcrc32 = new byte [ 4 ] ;
this . algid = encryptionalgorithm . getalgorithmbycode ( zipshort . getvalue ( data , offset + ivsize + 8 ) ) ;
system . out . println ( "rcount : " + rcount ) ;
this . bitlen = zipshort . getvalue ( data , offset + 4 ) ;
this . rcount = ziplong . getvalue ( data , offset + ivsize + 16 + erdsize ) ;
this . vdata = new byte [ vsize - 4 ] ;
return getlocalfiledatadata ( ) ;
this . erddata = new byte [ erdsize ] ;
public byte [ ] getlocalfiledatadata ( ) {
private int flags ;
private byte [ ] localdata ;
centraldata = ziputil . copy ( data ) ;
this . keyblob = new byte [ resize - this . hashsize ] ;
return hashalg ;
this . format = zipshort . getvalue ( data , offset ) ;
public void parsecentraldirectoryformat ( byte [ ] data , int offset , int length ) {
if ( centraldata ! = null ) {
private static final zipshort header id = new zipshort ( 0x0017 ) ;
system . arraycopy ( data , offset + ivsize + 22 + erdsize + vsize - 4 , vcrc32 , 0 , 4 ) ;
this . flags = zipshort . getvalue ( data , offset + 6 ) ;
system . arraycopy ( data , offset + 4 , this . ivdata , 0 , ivsize ) ;
public zipshort getcentraldirectorylength ( ) {
public void parsefromcentraldirectorydata ( byte [ ] data , int offset , int length ) {
for ( int i = 0 ; i < this . rcount ; i + + ) {
private byte [ ] centraldata ;
this . hashalg = hashalgorithm . getalgorithmbycode ( zipshort . getvalue ( data , offset + ivsize + 20 + erdsize ) ) ;
package org . apache . commons . compress . archivers . zip ;
public void setlocalfiledatadata ( byte [ ] data ) {
public zipshort getlocalfiledatalength ( ) {
private byte vcrc32 [ ] ;
public long getrecordcount ( ) {
system . arraycopy ( data , offset + ivsize + 22 + erdsize , this . vdata , 0 , vsize - 4 ) ;
private byte recipientkeyhash [ ] ;
int vsize = zipshort . getvalue ( data , offset + ivsize + 20 + erdsize ) ;
this . bitlen = zipshort . getvalue ( data , offset + ivsize + 10 ) ;
return ziputil . copy ( centraldata ) ;
this . ivdata = new byte [ ivsize ] ;
private int bitlen ;
system . arraycopy ( data , offset + ivsize + 16 , this . erddata , 0 , erdsize ) ;
parsecentraldirectoryformat ( data , offset , length ) ;
public class x0017 strongencryptionheader extends pkwareextraheader implements zipextrafield {
system . arraycopy ( data , offset + ivsize + 24 + erdsize , this . recipientkeyhash , 0 , this . hashsize ) ;
public zipshort getheaderid ( ) {
return new zipshort ( centraldata . length ) ;
return getlocalfiledatalength ( ) ;
if ( rcount = = 0 ) {
this . algid = encryptionalgorithm . getalgorithmbycode ( zipshort . getvalue ( data , offset + 2 ) ) ;
this . hashalg = hashalgorithm . getalgorithmbycode ( zipshort . getvalue ( data , offset + 12 ) ) ;
this . flags = zipshort . getvalue ( data , offset + ivsize + 12 ) ;
long size = ziplong . getvalue ( data , offset + ivsize + 2 ) ;
this . recipientkeyhash = new byte [ this . hashsize ] ;
setcentraldirectorydata ( tmp ) ;
int resize = zipshort . getvalue ( data , offset + ivsize + 24 + erdsize ) ;
byte [ ] tmp = new byte [ length ] ;
public void parsefromlocalfiledata ( byte [ ] data , int offset , int length ) {
public void parsefileformat ( byte [ ] data , int offset , int length ) {
system . arraycopy ( data , offset + ivsize + 24 + erdsize + this . hashsize , this . keyblob , 0 , resize - this . hashsize ) ;
private byte keyblob [ ] ;
private byte erddata [ ] ;
private hashalgorithm hashalg ;
return algid ;
parsefileformat ( data , offset , length ) ;
if ( rcount > 0 ) {
system . arraycopy ( data , offset + ivsize + 22 + erdsize + resize + vsize - 4 , vcrc32 , 0 , 4 ) ;
this . rcount = ziplong . getvalue ( data , offset + 8 ) ;
return rcount ;
int ivsize = zipshort . getvalue ( data , offset ) ;
this . format = zipshort . getvalue ( data , offset + ivsize + 6 ) ;
private static final long serialversionuid = 1l ;
localdata = ziputil . copy ( data ) ;
public byte [ ] getcentraldirectorydata ( ) {
public void setcentraldirectorydata ( byte [ ] data ) {
system . arraycopy ( data , offset , tmp , 0 , length ) ;
return header id ;
private long rcount ;
private byte vdata [ ] ;
this . hashsize = zipshort . getvalue ( data , offset + ivsize + 22 + erdsize ) ;
return new zipshort ( localdata ! = null ? localdata . length : 0 ) ;
int vsize = zipshort . getvalue ( data , offset + ivsize + 26 + erdsize + resize ) ;
private encryptionalgorithm algid ;
return createarchiveinputstream ( cpio , in ) ;
return createarchiveinputstream ( tar , in ) ;
return createarchiveinputstream ( dump , in ) ;
return createarchiveinputstream ( zip , in ) ;
return createarchiveinputstream ( arj , in ) ;
return createarchiveinputstream ( jar , in ) ;
return createarchiveinputstream ( ar , in ) ;
offset + = read > 0 ? read : 0 ;
}
if ( lfhoffset > = zip64 magic | | zip64mode = = zip64mode . always ) {
| | lfhoffset > = zip64 magic
| | zip64mode = = zip64mode . always ;
| | zip64mode = = zip64mode . always ) {
putlong ( zip64 magic , buf , cfh lfh offset ) ;
else {
| | ze . getsize ( ) > = zip64 magic
putlong ( math . min ( lfhoffset , zip64 magic ) , buf , cfh lfh offset ) ;
public int read ( byte [ ] b , int off , int len ) throws ioexception {
inflater . end ( ) ;
public void write ( int b ) throws ioexception {
public void close ( ) throws ioexception {
}
public int read ( ) throws ioexception {
return inflaterinputstream . read ( b , off , len ) ;
final deflater deflater = new deflater ( level , true ) ;
return inflaterinputstream . read ( ) ;
public int read ( byte [ ] b ) throws ioexception {
final inflater inflater = new inflater ( true ) ;
deflateroutputstream . close ( ) ;
deflateroutputstream . write ( b ) ;
inflater ) ;
public void write ( byte [ ] b , int off , int len ) throws ioexception {
return new inputstream ( ) {
final inflaterinputstream inflaterinputstream = new inflaterinputstream ( new dummybyteaddinginputstream ( in ) ,
deflater . end ( ) ;
return inflaterinputstream . read ( b ) ;
final deflateroutputstream deflateroutputstream = new deflateroutputstream ( out , deflater ) ;
@ override
public void write ( byte [ ] b ) throws ioexception {
deflateroutputstream . write ( b , off , len ) ;
return new outputstream ( ) {
} ;
inflaterinputstream . close ( ) ;
public synchronized void finish ( ) throws ioexception {
if ( out ! = null ) {
if ( this . out ! = null ) {
public synchronized void finish ( ) throws ioexception {
if ( this . out = = null ) {
}
this . currentchar = - 1 ;
this . blocksorter = null ;
this . out = null ;
endblock ( ) ;
public void finish ( ) throws ioexception {
private final object outlock = new object ( ) ;
endcompression ( ) ;
this . data = null ;
if ( out ! = null ) {
} finally {
writerun ( ) ;
synchronized ( outlock ) {
if ( this . runlength > 0 ) {
try {
this . channel = null ;
if ( !phased & & channel ! = null ) {
channel . position ( entry . localdatastart - 5 * short ) ;
final long save = channel . position ( ) ;
channel = files . newbytechannel ( file . topath ( ) ,
out = null ;
| | channel ! = null
streamcompressor streamcompressor = null ;
seekablebytechannel channel = null ;
} else if ( zipmethod = = deflated | | channel ! = null ) {
ioutils . closequietly ( channel ) ;
}
channel . position ( entry . localdatastart + 3 * word + 2 * short
private final seekablebytechannel channel ;
import java . nio . file . standardopenoption ;
} else if ( channel = = null ) {
channel = channel ;
if ( ze . getmethod ( ) ! = deflated | | channel ! = null ) {
return zipmethod = = deflated & & channel = = null ;
if ( entry . entry . getmethod ( ) = = stored & & channel = = null ) {
+ namelen + 2 * short ) ;
channel . position ( entry . localdatastart ) ;
return channel ! = null ;
import java . util . enumset ;
channel = null ;
standardopenoption . truncate existing ) ) ;
standardopenoption . read ,
enumset . of ( standardopenoption . create , standardopenoption . write ,
import java . nio . file . files ;
streamcompressor = streamcompressor . create ( o , def ) ;
this . channel = channel ;
streamcompressor = streamcompressor . create ( channel , def ) ;
streamcompressor = streamcompressor ;
if ( channel ! = null ) {
public ziparchiveoutputstream ( seekablebytechannel channel ) throws ioexception {
channel . position ( save ) ;
def = new deflater ( level , true ) ;
& & channel ! = null & & mode ! = zip64mode . never ) ;
import java . nio . channels . seekablebytechannel ;
channel . close ( ) ;
public compressorinputstream createcompressorinputstream ( final string name , final inputstream in )
throws compressorexception {
return new serviceloaderiterator < > ( compressorstreamprovider . class ) ;
if ( lzmautils . matches ( signature , signaturelength ) & & lzmautils . islzmacompressionavailable ( ) ) {
import java . util . locale ;
public static compressorstreamfactory getsingleton ( ) {
putall ( provider . getoutputstreamcompressornames ( ) , provider , map ) ;
}
putall ( singleton . getinputstreamcompressornames ( ) , singleton , map ) ;
public set < string > getinputstreamcompressornames ( ) {
return accesscontroller . doprivileged ( new privilegedaction < sortedmap < string , compressorstreamprovider > > ( ) {
private static string tokey ( final string name ) {
collections . addall ( set , gzip , bzip2 , xz , pack200 , deflate ) ;
public static sortedmap < string , compressorstreamprovider > findavailablecompressorinputstreamproviders ( ) {
public sortedmap < string , compressorstreamprovider > run ( ) {
import java . util . collections ;
return decompressuntileof ;
final compressorstreamprovider compressorstreamprovider = getcompressorinputstreamproviders ( ) . get ( tokey ( name ) ) ;
return deflate ;
import org . apache . commons . compress . compressors . xz . xzcompressoroutputstream ;
return bzip2 ;
return name . touppercase ( locale . root ) ;
throw new compressorexception ( "could not create compressoroutputstream" , e ) ;
private static arraylist < compressorstreamprovider > findcompressorstreamproviders ( ) {
throw new illegalargumentexception ( "compressor name and stream must not be null . " ) ;
compressorinputstreamproviders = collections
public set < string > getoutputstreamcompressornames ( ) {
public static string getsnappyraw ( ) {
public boolean getdecompressuntileof ( ) {
import org . apache . commons . compress . utils . serviceloaderiterator ;
if ( compressoroutputstreamproviders = = null ) {
treemap < string , compressorstreamprovider > map = new treemap < > ( ) ;
static void putall ( set < string > names , compressorstreamprovider provider ,
public compressorinputstream createcompressorinputstream ( final inputstream in ) throws compressorexception {
putall ( provider . getinputstreamcompressornames ( ) , provider , map ) ;
} ) ;
import java . util . set ;
return lists . newarraylist ( serviceloaderiterator ( ) ) ;
import java . util . treemap ;
throw new compressorexception ( "could not create compressorinputstream . " , e ) ;
import org . apache . commons . compress . utils . lists ;
private static iterator < compressorstreamprovider > serviceloaderiterator ( ) {
private static final compressorstreamfactory singleton = new compressorstreamfactory ( ) ;
public class compressorstreamfactory implements compressorstreamprovider {
public static string getz ( ) {
return compressorinputstreamproviders ;
public static string getsnappyframed ( ) {
public sortedmap < string , compressorstreamprovider > getcompressorinputstreamproviders ( ) {
import java . util . arraylist ;
public static sortedmap < string , compressorstreamprovider > findavailablecompressoroutputstreamproviders ( ) {
public static string getxz ( ) {
treemap < string , compressorstreamprovider > map ) {
import java . util . sortedmap ;
return compressorstreamprovider . createcompressoroutputstream ( name , out ) ;
compressoroutputstreamproviders = collections
import java . security . accesscontroller ;
return set ;
return gzip ;
import java . util . iterator ;
public compressoroutputstream createcompressoroutputstream ( final string name , final outputstream out )
import java . util . hashset ;
public sortedmap < string , compressorstreamprovider > getcompressoroutputstreamproviders ( ) {
return z ;
private sortedmap < string , compressorstreamprovider > compressorinputstreamproviders ;
return compressoroutputstreamproviders ;
return singleton ;
@ override
final compressorstreamprovider compressorstreamprovider = getcompressoroutputstreamproviders ( ) . get ( tokey ( name ) ) ;
hashset < string > set = new hashset < > ( ) ;
import org . apache . commons . compress . compressors . xz . xzcompressorinputstream ;
if ( compressorinputstreamproviders = = null ) {
return compressorstreamprovider . createcompressorinputstream ( name , in ) ;
public static string getbzip2 ( ) {
. unmodifiablesortedmap ( findavailablecompressoroutputstreamproviders ( ) ) ;
return lzma ;
this . decompressuntileof = null ;
public static string getgzip ( ) {
return pack200 ;
return snappy framed ;
collections . addall ( set , gzip , bzip2 , xz , lzma , pack200 , snappy raw , snappy framed , z , deflate ) ;
import org . apache . commons . compress . compressors . xz . xzutils ;
map . put ( tokey ( name ) , provider ) ;
private sortedmap < string , compressorstreamprovider > compressoroutputstreamproviders ;
for ( string name : names ) {
public static string getdeflate ( ) {
for ( compressorstreamprovider provider : findcompressorstreamproviders ( ) ) {
. unmodifiablesortedmap ( findavailablecompressorinputstreamproviders ( ) ) ;
return map ;
if ( xzutils . matches ( signature , signaturelength ) & & xzutils . isxzcompressionavailable ( ) ) {
public static string getlzma ( ) {
public static string getpack200 ( ) {
import java . security . privilegedaction ;
putall ( singleton . getoutputstreamcompressornames ( ) , singleton , map ) ;
if ( compressorstreamprovider ! = null ) {
return snappy raw ;
return xz ;
}
@ override
return createcompressorinputstream ( name , in , decompressconcatenated ) ;
return compressorstreamprovider . createcompressorinputstream ( name , in , actualdecompressconcatenated ) ;
return new gzipcompressorinputstream ( in , actualdecompressconcatenated ) ;
final treemap < string , compressorstreamprovider > map = new treemap < > ( ) ;
return new bzip2compressorinputstream ( in , actualdecompressconcatenated ) ;
final treemap < string , compressorstreamprovider > map ) {
static void putall ( final set < string > names , final compressorstreamprovider provider ,
for ( final string name : names ) {
for ( final compressorstreamprovider provider : findcompressorstreamproviders ( ) ) {
public compressorinputstream createcompressorinputstream ( final string name , final inputstream in ,
return new xzcompressorinputstream ( in , actualdecompressconcatenated ) ;
final boolean actualdecompressconcatenated ) throws compressorexception {
package org . apache . commons . compress . compressors . lzma ;
public void finish ( ) throws ioexception {
import org . apache . commons . compress . compressors . compressoroutputstream ;
public void flush ( ) throws ioexception {
public void close ( ) throws ioexception {
}
private final lzmaoutputstream out ;
throws ioexception {
out . close ( ) ;
import java . io . outputstream ;
public lzmacompressoroutputstream ( final outputstream outputstream )
out . write ( buf , off , len ) ;
out . write ( b ) ;
out = new lzmaoutputstream ( outputstream , new lzma2options ( ) , - 1 ) ;
import java . io . ioexception ;
import org . tukaani . xz . lzmaoutputstream ;
public class lzmacompressoroutputstream extends compressoroutputstream {
public void write ( final byte [ ] buf , final int off , final int len ) throws ioexception {
import org . tukaani . xz . lzma2options ;
@ override
out . flush ( ) ;
out . finish ( ) ;
public void write ( final int b ) throws ioexception {
private static final int dump signature size = 32 ;
final byte [ ] dumpsig = new byte [ dump signature size ] ;
if ( signaturelength > = tar header size ) {
private static final int tar header size = 512 ;
final byte [ ] tarheader = new byte [ tar header size ] ;
if ( tararchiveinputstream . matches ( tarheader , signaturelength ) ) {
final byte [ ] tarheader = new byte [ tar header size ] ;
signaturelength = ioutils . readfully ( in , tarheader ) ;
in . mark ( tarheader . length ) ;
tais = new tararchiveinputstream ( new bytearrayinputstream ( tarheader ) ) ;
return asint ( bytearray , base , false ) ;
private int asint ( final byte [ ] bytearray , final int base ) {
return asint ( bytearray , 10 , treatblankaszero ) ;
private long aslong ( final byte [ ] bytearray ) {
return long . parselong ( archiveutils . toasciistring ( bytearray ) . trim ( ) ) ;
private int asint ( final byte [ ] bytearray , final boolean treatblankaszero ) {
private int asint ( final byte [ ] bytearray ) {
return asint ( bytearray , 10 , false ) ;
private int asint ( final byte [ ] bytearray , final int base , final boolean treatblankaszero ) {
final string string = archiveutils . toasciistring ( bytearray ) . trim ( ) ;
p . pack ( ji , originaloutput ) ;
ji = new jarinputstream ( streambridge . getinput ( ) ) ;
ioutils . readfully ( this , lengthbuf ) ;
ioutils . readfully ( this , lastmodifiedbuf ) ;
private final byte [ ] filemodebuf = new byte [ 8 ] ;
ioutils . readfully ( this , idbuf ) ;
string temp = archiveutils . toasciistring ( namebuf ) . trim ( ) ;
private final byte [ ] lengthbuf = new byte [ 10 ] ;
ioutils . readfully ( this , filemodebuf ) ;
asint ( idbuf , true ) ,
private final byte [ ] namebuf = new byte [ 16 ] ;
final int userid = asint ( idbuf , true ) ;
private final byte [ ] idbuf = new byte [ 6 ] ;
ioutils . readfully ( this , namebuf ) ;
asint ( filemodebuf , 8 ) ,
long len = aslong ( lengthbuf ) ;
aslong ( lastmodifiedbuf ) ) ;
currententry = readgnustringtable ( lengthbuf ) ;
private final byte [ ] lastmodifiedbuf = new byte [ 12 ] ;
}
public static string getlz4framed ( ) {
lz4 framed ) ;
return sets . newhashset ( gzip , bzip2 , xz , lzma , pack200 , deflate , snappy raw , snappy framed , lz4 block ) ;
return lz4 framed ;
return lz4 block ;
import org . apache . commons . compress . compressors . lz4 . framedlz4compressorinputstream ;
public static string getlz4block ( ) {
return new framedlz4compressorinputstream ( in ) ;
public static final string lz4 framed = "lz4 - framed" ;
return sets . newhashset ( gzip , bzip2 , xz , lzma , pack200 , deflate , snappy raw , snappy framed , z , lz4 block ,
if ( lz4 framed . equalsignorecase ( name ) ) {
if ( framedlz4compressorinputstream . matches ( signature , signaturelength ) ) {
}
this . lzmamemorylimitkb = lzmamemorylimitkb ;
throws ioexception {
public lzmacompressorinputstream ( final inputstream inputstream )
in = new lzmainputstream ( inputstream , memorylimitkb ) ;
public lzmacompressorinputstream ( final inputstream inputstream , int memorylimitkb )
private volatile int lzmamemorylimitkb = - 1 ;
in = new lzmainputstream ( inputstream , - 1 ) ;
public void setlzmamemorylimitkb ( int lzmamemorylimitkb ) {
return new lzmacompressorinputstream ( in , lzmamemorylimitkb ) ;
this ( inputstream , - 1 ) ;
super ( message ) ;
public compressormemorylimitexception ( string message ) {
throw new compressormemorylimitexception ( e . getmessage ( ) ) ;
public zcompressorinputstream ( final inputstream inputstream , int memorylimitinkb )
throw new compressormemorylimitexception ( "tried to allocate " + maxtablesize +
}
throws ioexception {
public class compressormemorylimitexception extends compressorexception {
" but memorylimitinkb only allows " + ( memorylimitinkb * 1024 ) ) ;
final int maxtablesize = 1 < < maxcodesize ;
if ( memorylimitinkb > - 1 & & maxtablesize > memorylimitinkb * 1024 ) {
} catch ( compressormemorylimitexception e ) {
super ( message , e ) ;
public zcompressorinputstream ( final inputstream inputstream ) throws ioexception {
} catch ( memorylimitexception e ) {
import org . apache . commons . compress . compressors . compressormemorylimitexception ;
import org . tukaani . xz . memorylimitexception ;
this . memorylimitinkb = - 1 ;
initializetables ( maxcodesize ) ;
throw new compressormemorylimitexception ( "exceeded calculated memory limit" , e ) ;
public compressorstreamfactory ( final boolean decompressuntileof ) {
initializetables ( maxcodesize , memorylimitinkb ) ;
try {
} catch ( zcompressorinputstream . ioexceptionwrappingmemorylimitexception e ) {
public compressorstreamfactory ( final boolean decompressuntileof , final int memorylimitinkb ) {
throw new ioexceptionwrappingmemorylimitexception ( e . getmessage ( ) ) ;
public ioexceptionwrappingmemorylimitexception ( string message ) {
package org . apache . commons . compress . compressors ;
throws compressormemorylimitexception {
this ( decompressuntileof , - 1 ) ;
private final int memorylimitinkb ;
in = new lzmainputstream ( inputstream , memorylimitinkb ) ;
public lzmacompressorinputstream ( final inputstream inputstream , int memorylimitinkb )
return new lzmacompressorinputstream ( in , memorylimitinkb ) ;
this . memorylimitinkb = memorylimitinkb ;
public compressormemorylimitexception ( string message , exception e ) {
public static class ioexceptionwrappingmemorylimitexception extends ioexception {
return new zcompressorinputstream ( in , memorylimitinkb ) ;
protected void initializetables ( final int maxcodesize , final int memorylimitinkb )
+ + loc ;
if ( loc = = end & & adddummy ) {
int read = read ( loc + + , singlebytebuffer ) ;
singlebytebuffer = bytebuffer . allocate ( 1 ) ;
if ( singlebytebuffer = = null ) {
return ret ;
b [ off ] = 0 ;
buf = bytebuffer . wrap ( b , off , len ) ;
this . end = start + remaining ;
singlebytebuffer . rewind ( ) ;
if ( loc > = end ) {
}
private bytebuffer singlebytebuffer ;
private long loc ;
if ( this . end < start ) {
this . adddummy = true ;
if ( len > end - loc ) {
return 1 ;
return - 1 ;
else {
throw new illegalargumentexception ( "invalid length of stream at offset = " + start + " , length = " + remaining ) ;
public synchronized int read ( final byte [ ] b , final int off , int len ) throws ioexception {
return singlebytebuffer . get ( ) & 0xff ;
int ret = read ( loc , buf ) ;
len = ( int ) ( end - loc ) ;
private boolean adddummy = false ;
loc = start ;
private final long end ;
public synchronized int read ( ) throws ioexception {
return ret ;
in = new xzinputstream ( inputstream , memorylimitinkb ) ;
count ( ret = = - 1 ? - 1 : 1 ) ;
final int ret = in . read ( ) ;
public class memorylimitexception extends ioexception {
}
this ( inputstream , decompressconcatenated , - 1 ) ;
in = new singlexzinputstream ( inputstream , memorylimitinkb ) ;
throws ioexception {
return new xzcompressorinputstream ( in , actualdecompressconcatenated , memorylimitinkb ) ;
count ( ret ) ;
} catch ( org . tukaani . xz . memorylimitexception e ) {
public memorylimitexception ( string message ) {
throws memorylimitexception {
public memorylimitexception ( string message , exception e ) {
package org . apache . commons . compress ;
import java . io . ioexception ;
final int ret = in . read ( buf , off , len ) ;
public xzcompressorinputstream ( inputstream inputstream ,
boolean decompressconcatenated , int memorylimitinkb )
import org . apache . commons . compress . memorylimitexception ;
throw new memorylimitexception ( "tried to allocate " + maxtablesize +
in = new lzmainputstream ( inputstream , memorylimitinkb ) ;
return in . skip ( n ) ;
throw new memorylimitexception ( "exceeded calculated memory limit" , e ) ;
return new lzmacompressorinputstream ( in , memorylimitinkb ) ;
throw new memorylimitexception ( "excedded memory limit" , e ) ;
throw new memorylimitexception ( "exceeded memory limit" , e ) ;
return new zcompressorinputstream ( in , memorylimitinkb ) ;
try {
import org . apache . commons . compress . memorylimit ;
initializetables ( maxcodesize , memorylimit . getmemorylimitinkb ( ) ) ;
public zcompressorinputstream ( final inputstream inputstream ) throws ioexception {
}
? data descriptor min version
import static org . apache . commons . compress . archivers . zip . zipconstants . deflate min version ;
putshort ( versionneededtoextractmethod ( zipmethod ) , buf , lfh version needed offset ) ;
writeout ( zipshort . getbytes ( versionneededtoextractmethod ( entry . entry . getmethod ( ) ) ) ) ;
return isdeflatedtooutputstream ( zipmethod )
return zipmethod = = deflated ? deflate min version : initial version ;
: versionneededtoextractmethod ( zipmethod ) ;
private int versionneededtoextractmethod ( int zipmethod ) {
writeasciilong ( entry . getname ( ) . length ( ) + 1l , 8 , 16 ) ;
writebinarylong ( entry . getname ( ) . length ( ) + 1l , 2 , swaphalfword ) ;
writeasciilong ( entry . getname ( ) . length ( ) + 1l , 6 , 8 ) ;
}
import java . nio . charset . charset ;
@ deprecated
return getheaderpadcount ( name . length ( ) ) ;
public int getheaderpadcount ( charset charset ) {
return 0 ;
if ( name = = null ) {
return getheaderpadcount ( null ) ;
return getheaderpadcount ( name . getbytes ( charset ) . length ) ;
pad ( entry . getheaderpadcount ( charset . forname ( encoding ) ) ) ;
if ( charset = = null ) {
}
import java . io . ioexception ;
} catch ( ioexception ex ) {
pad ( entry . getheaderpadcount ( zipencoding ) ) ;
throw new runtimeexception ( "cannot encode " + name , ex ) ;
return getheaderpadcount ( buf . limit ( ) - buf . position ( ) ) ;
import java . nio . bytebuffer ;
import org . apache . commons . compress . archivers . zip . zipencoding ;
final bytebuffer buf = encoding . encode ( name ) ;
if ( encoding = = null ) {
public int getheaderpadcount ( zipencoding encoding ) {
try {
byte getbyte ( string key ) ;
string [ ] getstringarray ( string key ) ;
long getlong ( string key , long defaultvalue ) ;
iterator < string > getkeys ( ) ;
short getshort ( string key ) ;
package org . apache . commons . configuration ;
import java . util . list ;
int getint ( string key , int defaultvalue ) ;
double getdouble ( string key ) ;
properties getproperties ( string key ) ;
import java . util . iterator ;
}
float getfloat ( string key , float defaultvalue ) ;
{
byte getbyte ( string key , byte defaultvalue ) ;
int getint ( string key ) ;
boolean containskey ( string key ) ;
bigdecimal getbigdecimal ( string key ) ;
short getshort ( string key , short defaultvalue ) ;
double getdouble ( string key , double defaultvalue ) ;
string getstring ( string key , string defaultvalue ) ;
import java . math . biginteger ;
list < object > getlist ( string key , list < object > defaultvalue ) ;
long getlong ( string key ) ;
iterator < string > getkeys ( string prefix ) ;
long getlong ( string key , long defaultvalue ) ;
biginteger getbiginteger ( string key , biginteger defaultvalue ) ;
public interface immutableconfiguration
float getfloat ( string key ) ;
bigdecimal getbigdecimal ( string key , bigdecimal defaultvalue ) ;
import java . math . bigdecimal ;
boolean getboolean ( string key , boolean defaultvalue ) ;
double getdouble ( string key , double defaultvalue ) ;
biginteger getbiginteger ( string key ) ;
import java . util . properties ;
boolean getboolean ( string key , boolean defaultvalue ) ;
string getstring ( string key ) ;
byte getbyte ( string key , byte defaultvalue ) ;
short getshort ( string key , short defaultvalue ) ;
boolean getboolean ( string key ) ;
boolean isempty ( ) ;
list < object > getlist ( string key ) ;
float getfloat ( string key , float defaultvalue ) ;
integer getinteger ( string key , integer defaultvalue ) ;
object getproperty ( string key ) ;
new fileextensionconfigurationbuilderprovider (
config . childconfigurationsat ( null ) ;
public static final int event err load optional = 51 ;
configureentityresolver ( ) ;
private static final configurationbuilderprovider xml provider =
combinedconfiguration addconfig = createadditionalsconfiguration ( result ) ;
providers . put ( tagname , provider ) ;
static final string attr optional = defaultexpressionengine . default attribute start
configurationbuilderprovider provider =
static final string attr optional res = defaultexpressionengine . default attribute start
definitionconfiguration = getdefinitionbuilder ( ) . getconfiguration ( ) ;
+ attr atname
import java . util . map ;
new combinedconfiguration ( new unioncombiner ( ) ) ;
import java . util . linkedlist ;
+ " . combiner . additional . list - nodes . node" ;
"org . apache . commons . configuration . propertiesconfiguration" ,
import org . apache . commons . configuration . configurationexception ;
configurationsourcedata result = new configurationsourcedata ( ) ;
result . setnodecombiner ( new overridecombiner ( ) ) ;
system provider , builder provider ,
filebasedbuilderparameters . fromparameters ( params ) ;
return addconfig ;
} ) ;
protected void initfilesystem ( ) throws configurationexception
public static final string additional name = combinedconfigurationbuilder . class
provider . getconfiguration ( decl ) ;
arrays . aslist ( file params ) ) ;
public combinedconfigurationbuilder ( map < string , object > params )
super . resetparameters ( ) ;
protected configurationbuilder < ? extends hierarchicalconfiguration > setupdefinitionbuilder (
overridebuilders =
protected beandeclaration createresultdeclaration ( map < string , object > params )
private void adddefinitionbuilderchangelistener (
private configurationsourcedata getsourcedata ( )
+ xmlbeandeclaration . reserved prefix
public configurationsourcedata ( )
new configurationdeclaration (
configurationsourcedata data = getsourcedata ( ) ;
super ( combinedconfiguration . class , params , allowfailoninit ) ;
static final string key additional list = sec header
return definitionbuilder ;
createbuilders ( unionbuilders ,
sourcedata . cleanup ( ) ;
static final string [ ] config sections = {
"org . apache . commons . configuration . xmlconfiguration" ,
import org . apache . commons . configuration . abstractconfiguration ;
return null ;
public combinedconfigurationbuilder ( map < string , object > params , boolean allowfailoninit )
namedbuilders . put ( decl . getname ( ) , builder ) ;
if ( tagname = = null )
return new xmlbeandeclaration ( getdefinitionconfiguration ( ) , key result ,
public combinedconfiguration getconfiguration ( boolean load )
it . remove ( ) ;
public synchronized void resetparameters ( )
+ " . config bean factory name" ;
string nodename = it . next ( ) . getrootelementname ( ) ;
import org . apache . commons . configuration . builder . filebasedbuilderparameters ;
hierarchicalconfiguration config ) throws configurationexception
throw cex ;
combinedbuilderparameters cbparams =
public void initfromdefinitionconfiguration (
combinedconfigurationbuilder . this , src ) ;
private final map < configurationbuilder < ? extends configuration > , configurationdeclaration > declarations ;
throw new configurationexception ( "builder cannot be resolved : " + name ) ;
definitionbuilder = null ;
private static final baseconfigurationbuilderprovider env provider = null ; / *
private static final baseconfigurationbuilderprovider builder provider = null ;
return providers . remove ( tagname ) ;
public class combinedconfigurationbuilder extends basicconfigurationbuilder < combinedconfiguration >
hierarchicalconfiguration config = getdefinitionconfiguration ( ) ;
. isignorereloadexceptions ( ) ) ;
env provider * /
assert decl ! = null : "cannot resolve builder!" ;
import org . apache . commons . configuration . tree . unioncombiner ;
. getname ( )
private configurationsourcedata sourcedata ;
synchronized ( combinedconfigurationbuilder . this )
static final string attr name = defaultexpressionengine . default attribute start
initfilesystem ( ) ;
configurationbuilderprovider provider )
+ "reload"
return defbuilder ;
if ( sourcedata ! = null )
private list < subnodeconfiguration > fetchtopleveloverrideconfigs (
public set < string > buildernames ( )
import org . apache . commons . configuration . combinedconfiguration ;
private string configurationbasepath ;
static final string sec header = "header" ;
super ( combinedconfiguration . class ) ;
private builderlistener changelistener ;
providerfortag ( src . getrootelementname ( ) ) ;
public collection < configurationbuilder < ? extends configuration > > getunionbuilders ( )
private static final baseconfigurationbuilderprovider ini provider =
private static final configurationbuilderprovider [ ] default providers = {
"ini" / * , "configuration" , "env" , "jndi" , "system" * /
+ "filename" + defaultexpressionengine . default attribute end ;
config . childconfigurationsat ( key union ) ) ;
addconfig . setdelimiterparsingdisabled ( resultconfig
return providers . get ( tagname ) ;
import java . util . hashmap ;
private final collection < configurationbuilder < ? extends configuration > > unionbuilders ;
static final string attr forcecreate = defaultexpressionengine . default attribute start
configurationbuilder < ? > builder )
"org . apache . commons . configuration . xmlpropertiesconfiguration" ,
{
definitionconfiguration = null ;
resetresult ( ) ;
if ( config . getmaxindex ( key combiner ) < 0 )
if ( sourcedata = = null )
static final string key entity resolver = sec header + " . entity - resolver" ;
import org . apache . commons . configuration . xmlconfiguration ;
static final string key combiner = key result + " . nodecombiner" ;
static final string key override = "override" ;
namedbuilders =
"org . apache . commons . configuration . plist . xmlpropertylistconfiguration" ,
allbuilders =
if ( provider = = null )
file builder ,
+ " . lookups . lookup" ;
initialize ( ) ;
protected void initresultinstance ( combinedconfiguration result )
import java . util . iterator ;
import org . apache . commons . configuration . beanutils . beandeclaration ;
private void initialize ( )
protected void registerconfiguredlookups ( ) throws configurationexception
map < string , object > params ) throws configurationexception
cc . addconfiguration (
protected combinedconfiguration createadditionalsconfiguration (
public configurationbuilderprovider providerfortag ( string tagname )
definitionbuilder = setupdefinitionbuilder ( getparameters ( ) ) ;
allbuilders . addall ( overridebuilders ) ;
properties provider , xml provider , xml provider , plist provider , ini provider / * , jndi provider ,
@ override
import java . util . collection ;
static final string key configuration lookups = sec header
private final collection < configurationbuilder < ? extends configuration > > allbuilders ;
+ attr optionalname + defaultexpressionengine . default attribute end ;
if ( decl . getname ( ) ! = null )
private static final string reloading builder =
+ " / additional config" ;
configurationbuilder < ? extends hierarchicalconfiguration > defbuilder =
. hasnext ( ) ; )
/ /
public configurationbuilderprovider removeconfigurationprovider ( string tagname )
static final string attr atname = "at" ;
static final string attr at res = defaultexpressionengine . default attribute start
configurationbuilder < ? extends configuration > builder =
final configurationbuilder < ? extends hierarchicalconfiguration > defbuilder )
"additional" , "override" , sec header
private void registerchangelistener ( )
allbuilders . addall ( unionbuilders ) ;
throws configurationexception
combinedconfiguration resultconfig )
return builder ;
public void addconfigurationprovider ( string tagname ,
catch ( configurationexception cex )
addconfigurationprovider ( default tags [ i ] , default providers [ i ] ) ;
"unsupported configuration source : "
public configurationbuilder < ? extends configuration > getnamedbuilder (
return overridebuilders ;
public combinedconfigurationbuilder ( )
import org . apache . commons . configuration . builder . basicconfigurationbuilder ;
private hierarchicalconfiguration definitionconfiguration ;
}
return unionbuilders ;
static final string attr reload = defaultexpressionengine . default attribute start
cbparams . getdefinitionbuilder ( ) ;
return namedbuilders . get ( name ) ;
return namedbuilders . keyset ( ) ;
for ( configurationbuilder < ? > b : allbuilders )
return declarations . get ( builder ) ;
hierarchicalconfiguration config )
declarations =
static final string file system = sec header + " . filesystem" ;
addconfig . setforcereloadcheck ( resultconfig . isforcereloadcheck ( ) ) ;
true , combinedconfiguration . class . getname ( ) ) ;
( abstractconfiguration ) builder . getconfiguration ( ) ,
fetchtopleveloverrideconfigs ( config ) ) ;
"org . apache . commons . configuration . builder . reloadingfilebasedconfigurationbuilder" ;
+ " . providers . provider" ;
import org . apache . commons . configuration . beanutils . xmlbeandeclaration ;
new baseconfigurationbuilderprovider ( file builder , reloading builder ,
private static final baseconfigurationbuilderprovider system provider = null ; / * new baseconfigurationbuilderprovider (
new baseconfigurationbuilderprovider ( environmentconfiguration . class ) ; * /
+ attr atname + defaultexpressionengine . default attribute end ;
builders . add ( builder ) ;
private static void createandaddconfigurations ( combinedconfiguration cc ,
throw new illegalargumentexception ( "provider must not be null!" ) ;
return definitionconfiguration ;
createbuilders ( overridebuilders ,
protected hierarchicalconfiguration getdefinitionconfiguration ( )
private static final configurationbuilderprovider properties provider =
ext xml , arrays . aslist ( file params ) ) ;
config . childconfigurationsat ( key override ) ) ;
for ( int i = 0 ; i < default tags . length ; i + + )
import org . apache . commons . configuration . configuration ;
new hashmap < configurationbuilder < ? extends configuration > , configurationdeclaration > ( ) ;
break ;
new hashmap < string , configurationbuilderprovider > ( ) ;
string name )
static final string attr filename = defaultexpressionengine . default attribute start
protected void registerdefaultproviders ( )
import java . util . list ;
+ "forcecreate"
package org . apache . commons . configuration . builder . combined ;
xmlconfiguration . class ) . configure ( fileparams ) ;
static final string key union = "additional" ;
if ( !data . getunionbuilders ( ) . isempty ( ) )
for ( int i = 0 ; i < config sections . length ; i + + )
unionbuilders =
static final string key lookup key = xmlbeandeclaration . attr prefix + "prefix ] " ;
+ defaultexpressionengine . default attribute end ;
protected void registerconfiguredproviders ( ) throws configurationexception
for ( iterator < subnodeconfiguration > it = configs . iterator ( ) ; it
protected void initsystemproperties ( ) throws configurationexception
public synchronized configurationbuilder < ? extends hierarchicalconfiguration > getdefinitionbuilder ( )
for ( configurationbuilder < ? extends configuration > builder : builders )
registerconfiguredproviders ( ) ;
private class configurationsourcedata
static final string ext xml = "xml" ;
static final string attr at = defaultexpressionengine . default attribute start
static final string key provider key = xmlbeandeclaration . attr prefix + "tag ] " ;
private void createbuilders (
systemconfiguration . class ) ; * /
changelistener = new builderlistener ( )
+ src . getrootelementname ( ) ) ;
b . removebuilderlistener ( changelistener ) ;
configurationsourcedata srcdata ) throws configurationexception
collection < configurationbuilder < ? extends configuration > > builders ,
private final collection < configurationbuilder < ? extends configuration > > overridebuilders ;
static final string key override list = sec header
this . configurationbasepath = configurationbasepath ;
private configurationbuilder < ? extends hierarchicalconfiguration > definitionbuilder ;
"org . apache . commons . configuration . builder . filebasedconfigurationbuilder" ;
private final map < string , configurationbuilderprovider > providers =
static final string key configuration providers = sec header
reloading builder ,
decl . getname ( ) , decl . getat ( ) ) ;
declarations . put ( builder , decl ) ;
defbuilder . addbuilderlistener ( new builderlistener ( )
configurationbuilder < ? extends configuration > builder )
import java . util . collections ;
filebasedbuilderparameters fileparams =
result . addconfiguration ( addconfig , additional name ) ;
if ( !decl . isoptional ( ) )
private static final baseconfigurationbuilderprovider jndi provider = null ; / * new baseconfigurationbuilderprovider (
import org . apache . commons . configuration . tree . defaultexpressionengine ;
static final string attr optionalname = "optional" ;
return collections . unmodifiableset ( getsourcedata ( ) . buildernames ( ) ) ;
configurationdeclaration decl = srcdata . getdeclaration ( builder ) ;
throw new configurationexception (
registerconfiguredlookups ( ) ;
configurationdeclaration decl =
public configurationdeclaration getdeclaration (
new linkedlist < configurationbuilder < ? extends configuration > > ( ) ;
import org . apache . commons . configuration . builder . filebasedconfigurationbuilder ;
import org . apache . commons . configuration . hierarchicalconfiguration ;
combinedbuilderparameters . fromparameters ( params ) ;
registerdefaultproviders ( ) ;
if ( cbparams ! = null )
configurationbuilder < ? extends configuration > builder = getsourcedata ( ) . getnamedbuilder ( name ) ;
+ " . combiner . override . list - nodes . node" ;
sourcedata = null ;
if ( config sections [ i ] . equals ( nodename ) )
addconfig . setignorereloadexceptions ( resultconfig
"properties" , "xml" , "hierarchicalxml" , "plist" ,
static final string key result = sec header + " . result" ;
public void cleanup ( )
. isdelimiterparsingdisabled ( ) ) ;
import java . util . set ;
super ( combinedconfiguration . class , params ) ;
adddefinitionbuilderchangelistener ( definitionbuilder ) ;
if ( defbuilder ! = null )
if ( definitionbuilder = = null )
static final string config bean factory name = combinedconfigurationbuilder . class
for ( hierarchicalconfiguration src : sources )
private static final baseconfigurationbuilderprovider plist provider =
initsystemproperties ( ) ;
"org . apache . commons . configuration . builder . filebasedbuilderparameters" ;
static final string key system props = " [ @ systemproperties ] " ;
list < subnodeconfiguration > configs =
new hashmap < string , configurationbuilder < ? extends configuration > > ( ) ;
private final map < string , configurationbuilder < ? extends configuration > > namedbuilders ;
createandaddconfigurations ( result , data . getoverridebuilders ( ) , data ) ;
return new filebasedconfigurationbuilder < xmlconfiguration > (
collection < ? extends hierarchicalconfiguration > sources )
definitionbuilder = defbuilder ;
b . addbuilderlistener ( changelistener ) ;
createandaddconfigurations ( addconfig , data . getunionbuilders ( ) , data ) ;
import org . apache . commons . configuration . builder . builderlistener ;
protected void configureentityresolver ( ) throws configurationexception
try
result . initfromdefinitionconfiguration ( getdefinitionconfiguration ( ) ) ;
sourcedata = createsourcedata ( ) ;
jndiconfiguration . class ) ; * /
public void setconfigurationbasepath ( string configurationbasepath )
"no builder for configuration definition specified!" ) ;
public collection < configurationbuilder < ? extends configuration > > getoverridebuilders ( )
public synchronized set < string > buildernames ( ) throws configurationexception
public string getconfigurationbasepath ( )
reset ( ) ;
registerchangelistener ( ) ;
if ( definitionconfiguration = = null )
"org . apache . commons . configuration . hierarchicaliniconfiguration" ,
return sourcedata ;
public synchronized configurationbuilder < ? extends configuration > getnamedbuilder ( string name ) throws configurationexception
import org . apache . commons . configuration . subnodeconfiguration ;
if ( builder = = null )
private static final string file params =
+ attr optionalname
"org . apache . commons . configuration . plist . propertylistconfiguration" ,
public void builderreset (
+ "name"
return result ;
if ( fileparams ! = null )
private configurationsourcedata createsourcedata ( )
return configs ;
throw new illegalargumentexception ( "tag name must not be null!" ) ;
private static final string [ ] default tags = {
import org . apache . commons . configuration . tree . overridecombiner ;
import java . util . arrays ;
combinedconfiguration addconfig =
return configurationbasepath ;
} ;
import org . apache . commons . configuration . builder . configurationbuilder ;
private static final string file builder =
object value = l . lookup ( var ) ;
return substitutor . replace ( ( string ) value ) ;
return new strsubstitutor ( new strlookup < object > ( )
return collections . unmodifiableset ( prefixlookups . keyset ( ) ) ;
import org . apache . commons . lang3 . text . strsubstitutor ;
object result = resolve ( key ) ;
private volatile configurationinterpolator parentinterpolator ;
protected lookup fetchlookupforprefix ( string prefix )
public configurationinterpolator getparentinterpolator ( )
private final map < string , lookup > prefixlookups ;
public void setparentinterpolator (
public boolean removedefaultlookup ( lookup lookup )
substitutor . setenablesubstitutioninvariables ( f ) ;
private final list < lookup > defaultlookups ;
return new hashmap < string , lookup > ( prefixlookups ) ;
defaultlookups . add ( defaultlookup ) ;
return new arraylist < lookup > ( defaultlookups ) ;
import java . util . list ;
public object resolve ( string var )
return defaultlookups . remove ( lookup ) ;
lookup = dummylookup . instance ;
}
{
if ( parent ! = null )
defaultlookups . addall ( lookups ) ;
return value ;
substitutor = initsubstitutor ( ) ;
defaultlookups = new copyonwritearraylist < lookup > ( ) ;
for ( lookup l : defaultlookups )
public class configurationinterpolator
public boolean deregisterlookup ( string prefix )
} ) ;
configurationinterpolator parentinterpolator )
return getparentinterpolator ( ) . resolve ( var ) ;
return ( result ! = null ) ? result . tostring ( ) : null ;
lookup lookup = prefixlookups . get ( prefix ) ;
public map < string , lookup > getlookups ( )
public void adddefaultlookup ( lookup defaultlookup )
import java . util . concurrent . copyonwritearraylist ;
public void registerlookups ( map < string , ? extends lookup > lookups )
public string lookup ( string key )
return this . parentinterpolator ;
import java . util . collections ;
object value = fetchlookupforprefix ( prefix ) . lookup ( name ) ;
prefixlookups . putall ( lookups ) ;
public void registerlookup ( string prefix , lookup lookup )
prefixlookups . put ( prefix , lookup ) ;
private strsubstitutor initsubstitutor ( )
public object interpolate ( object value )
public list < lookup > getdefaultlookups ( )
if ( value instanceof string )
@ override
import java . util . collection ;
public void adddefaultlookups ( collection < ? extends lookup > lookups )
return substitutor . isenablesubstitutioninvariables ( ) ;
this . parentinterpolator = parentinterpolator ;
return prefixlookups . remove ( prefix ) ! = null ;
import java . util . arraylist ;
prefixlookups = new concurrenthashmap < string , lookup > ( ) ;
return null ;
public set < string > prefixset ( )
import java . util . concurrent . concurrenthashmap ;
public boolean isenablesubstitutioninvariables ( )
public void setenablesubstitutioninvariables ( boolean f )
configurationinterpolator parent = getparentinterpolator ( ) ;
if ( value ! = null )
import org . apache . commons . lang3 . text . strlookup ;
if ( lookups ! = null )
private final strsubstitutor substitutor ;
private final atomicreference < configurationinterpolator > interpolator ;
( ciold ! = null ) ? ciold : new configurationinterpolator ( ) ;
} while ( !success ) ;
cinew . removedefaultlookup ( conflookup ) ;
success = interpolator . compareandset ( ciold , cinew ) ;
interpolator . set ( ci ) ;
if ( l instanceof configurationlookup )
}
{
conflookup = new configurationlookup ( this ) ;
cinew . adddefaultlookups ( lookups ) ;
else
for ( lookup l : ci . getdefaultlookups ( ) )
do
if ( this = = ( ( configurationlookup ) l ) . getconfiguration ( ) )
interpolator = new atomicreference < configurationinterpolator > ( ) ;
public final void setprefixlookups ( map < string , ? extends lookup > lookups )
cinew . registerlookups ( lookups ) ;
public final void setdefaultlookups ( collection < ? extends lookup > lookups )
configurationinterpolator ciold = getinterpolator ( ) ;
configurationinterpolator cinew =
return l ;
cinew . adddefaultlookup ( conflookup ) ;
private lookup findconfigurationlookup ( configurationinterpolator ci )
import java . util . concurrent . atomic . atomicreference ;
return null ;
return interpolator . get ( ) ;
lookup conflookup = findconfigurationlookup ( cinew ) ;
if ( conflookup = = null )
boolean success ;
if ( decl . getname ( ) ! = null )
namedbuilders . put ( decl . getname ( ) , builder ) ;
"unsupported configuration source : "
builder . addbuilderlistener ( changelistener ) ;
public collection < subnodeconfiguration > getunionsources ( )
throw new configurationexception ( "information about child builders"
if ( !data . getunionsources ( ) . isempty ( ) )
new configurationdeclaration (
decl . getname ( ) , decl . getat ( ) ) ;
}
{
unionbuilders . addall ( config . childconfigurationsat ( key union ) ) ;
private configurationbuilder < ? extends configuration > createconfigurationbuilder (
createconfigurationbuilder ( src , decl ) ;
createbuilderchangelistener ( ) ;
addchildconfiguration ( ccresult , decl , builder ) ;
collection < subnodeconfiguration > srcdecl )
else
private void addchildconfiguration ( combinedconfiguration ccresult ,
overridebuilders . addall ( fetchtopleveloverrideconfigs ( config ) ) ;
public void createandaddconfigurations ( combinedconfiguration ccresult ,
try
if ( provider = = null )
configurationbuilderprovider provider =
public collection < subnodeconfiguration > getoverridesources ( )
+ src . getrootelementname ( ) ) ;
configurationdeclaration decl ,
hierarchicalconfiguration src , configurationdeclaration decl )
provider . getconfigurationbuilder ( decl ) ;
configurationbuilder < ? extends configuration > builder )
if ( sourcedata = = null )
namedbuilders . clear ( ) ;
private final collection < subnodeconfiguration > unionbuilders ;
configurationbuilder < ? extends configuration > builder =
new linkedlist < subnodeconfiguration > ( ) ;
+ " has not been setup yet! call getconfiguration ( ) first . " ) ;
private final collection < subnodeconfiguration > overridebuilders ;
( abstractconfiguration ) builder . getconfiguration ( ) ,
return collections . unmodifiableset ( sourcedata . buildernames ( ) ) ;
throw cex ;
if ( !decl . isoptional ( ) )
sourcedata . getnamedbuilder ( name ) ;
private void createbuilderchangelistener ( )
combinedconfigurationbuilder . this , src ) ;
for ( configurationbuilder < ? > b : namedbuilders . values ( ) )
return collections . emptyset ( ) ;
overridebuilders . addall ( config . childconfigurationsat ( key override ) ) ;
throws configurationexception
return builder ;
data . createandaddconfigurations ( addconfig , data . getunionsources ( ) ) ;
catch ( configurationexception cex )
providerfortag ( src . getrootelementname ( ) ) ;
throw new configurationexception (
ccresult . addconfiguration (
data . createandaddconfigurations ( result , data . getoverridesources ( ) ) ;
configurationdeclaration decl =
for ( hierarchicalconfiguration src : srcdecl )
new arraylist < reloadingcontroller > ( subctrls ) ;
public void reloadingperformed ( )
rc . resetreloadingstate ( ) ;
return false ;
private static reloadingdetector createdetector (
for ( reloadingcontroller rc : controllers )
}
{
throw new illegalargumentexception (
public multireloadingcontrollerdetector (
"collection with sub controllers must not be null!" ) ;
collection < ? extends reloadingcontroller > subctrls )
controllers = ctrls ;
for ( reloadingcontroller rc : ctrls )
"collection with sub controllers contains a null entry!" ) ;
return true ;
if ( rc . checkforreloading ( null ) | | rc . isinreloadingstate ( ) )
public boolean isreloadingrequired ( )
super ( createdetector ( subctrls ) ) ;
return new multireloadingcontrollerdetector ( ctrls ) ;
package org . apache . commons . configuration . reloading ;
collection < reloadingcontroller > ctrls )
public class combinedreloadingcontroller extends reloadingcontroller
public combinedreloadingcontroller (
if ( subctrls = = null )
private final collection < reloadingcontroller > controllers ;
if ( rc = = null )
import java . util . collection ;
import java . util . arraylist ;
private static class multireloadingcontrollerdetector implements
collection < reloadingcontroller > ctrls =
reloadingdetector
}
{
new linkedlist < configurationbuilder < ? extends configuration > > ( ) ;
for ( configurationbuilder < ? > b : getchildbuilders ( ) )
private final collection < configurationbuilder < ? extends configuration > > allbuilders ;
public collection < configurationbuilder < ? extends configuration > > getchildbuilders ( )
builderparameters builderparams )
allbuilders . add ( builder ) ;
protected configurationbuilder < ? extends hierarchicalconfiguration > createxmldefinitionbuilder (
return new filebasedconfigurationbuilder < xmlconfiguration > (
allbuilders =
return allbuilders ;
return sourcedata . getchildbuilders ( ) ;
protected collection < configurationbuilder < ? extends configuration > > getchildbuilders ( )
xmlconfiguration . class ) . configure ( builderparams ) ;
import org . apache . commons . configuration . hierarchicalconfiguration ;
import org . apache . commons . configuration . reloading . combinedreloadingcontroller ;
super ( ) ;
combinedconfigurationbuilder implements reloadingcontrollersupport
. getreloadingcontroller ( ) ) ;
public synchronized reloadingcontroller getreloadingcontroller ( )
builderparameters builderparams )
super . initresultinstance ( result ) ;
import org . apache . commons . configuration . builder . reloadingfilebasedconfigurationbuilder ;
protected void initresultinstance ( combinedconfiguration result )
}
{
package org . apache . commons . configuration . builder . combined ;
private reloadingcontroller reloadingcontroller ;
collection < reloadingcontroller > subcontrollers =
configurationbuilder < ? extends hierarchicalconfiguration > defbuilder =
public reloadingcombinedconfigurationbuilder ( map < string , object > params ,
reloadingcontroller = createreloadingcontroller ( ) ;
super ( params ) ;
getdefinitionbuilder ( ) ;
obtainreloadingcontroller ( subcontrollers , b ) ;
obtainreloadingcontroller ( subcontrollers , defbuilder ) ;
if ( builder instanceof reloadingcontrollersupport )
return new combinedreloadingcontroller ( subcontrollers ) ;
return new reloadingfilebasedconfigurationbuilder < xmlconfiguration > (
import org . apache . commons . configuration . builder . builderparameters ;
public class reloadingcombinedconfigurationbuilder extends
import org . apache . commons . configuration . combinedconfiguration ;
import org . apache . commons . configuration . configuration ;
new linkedlist < reloadingcontroller > ( ) ;
collection < reloadingcontroller > subcontrollers , object builder )
boolean allowfailoninit )
import org . apache . commons . configuration . xmlconfiguration ;
import org . apache . commons . configuration . reloading . reloadingcontroller ;
public reloadingcombinedconfigurationbuilder ( )
xmlconfiguration . class ) . configure ( builderparams ) ;
@ override
throws configurationexception
import org . apache . commons . configuration . reloading . reloadingcontrollersupport ;
public reloadingcombinedconfigurationbuilder ( map < string , object > params )
import java . util . collection ;
import java . util . map ;
subcontrollers . add ( ( ( reloadingcontrollersupport ) builder )
import java . util . linkedlist ;
import org . apache . commons . configuration . configurationexception ;
public static void obtainreloadingcontroller (
protected configurationbuilder < ? extends hierarchicalconfiguration > createxmldefinitionbuilder (
import org . apache . commons . configuration . builder . configurationbuilder ;
super ( params , allowfailoninit ) ;
protected reloadingcontroller createreloadingcontroller ( )
return reloadingcontroller ;
for ( configurationbuilder < ? extends configuration > b : getchildbuilders ( ) )
import org . apache . commons . configuration . io . filehandler ;
filehandler fh = new filehandler ( this ) ;
configurationutils . locate ( locator . getfilesystem ( ) ,
url url = null ;
}
if ( baseurl ! = null )
{
baseurl . tostring ( ) , filename ) ;
this . locator = locator ;
import org . apache . commons . configuration . io . filelocator ;
fh . load ( url ) ;
import org . apache . commons . configuration . io . filelocatoraware ;
throw new configurationexception ( "cannot resolve include file "
private filelocator locator ;
if ( url = = null )
locator . getbasepath ( ) , filename ) ;
url =
+ filename ) ;
implements filebasedconfiguration , filelocatoraware
url baseurl = locator . getsourceurl ( ) ;
public void initfilelocator ( filelocator locator )
if ( locator ! = null )
handler , refreshdelay ) : new filehandlerreloadingdetector (
long refreshdelay = params . getreloadingrefreshdelay ( ) ;
return ( refreshdelay ! = null ) ? new filehandlerreloadingdetector (
handler ) ;
if ( !event . isbeforeupdate ( ) )
}
if ( baseurl ! = null )
assert locator ! = null : "locator has not been set!" ;
{
baseurl . tostring ( ) , filename ) ;
throw new configurationexception ( "cannot resolve include file "
filehandler fh = new filehandler ( this ) ;
url =
+ filename ) ;
if ( url = = null )
configurationutils . locate ( locator . getfilesystem ( ) ,
url baseurl = locator . getsourceurl ( ) ;
url url =
fh . load ( url ) ;
locator . getbasepath ( ) , filename ) ;
protected void performoperation ( ) throws sqlexception
stringbuilder query = new stringbuilder ( "insert into " ) ;
if ( namecol & & namecolumn ! = null )
resultset rs = openresultset ( string . format (
return keys ;
private final object errorpropertyvalue ;
protected integer performoperation ( ) throws sqlexception
}
final collection < string > keys = new arraylist < string > ( ) ;
conn . commit ( ) ;
else
sql is empty , table ) , true ) ;
new jdbcoperation < void > ( event clear property , key , null )
false , key , string . valueof ( obj ) ) ;
sql get keys , keycolumn , table ) , true ) ;
protected object performoperation ( ) throws sqlexception
private final string errorpropertyname ;
object errpropval )
errorpropertyname = errpropname ;
preparedstatement ps = createstatement ( sql , namecol ) ;
public t execute ( )
if ( !results . isempty ( ) )
preparedstatement pstmt = initstatement ( query . tostring ( ) ,
new jdbcoperation < void > ( event clear , null , null )
protected void clearpropertydirect ( final string key )
private preparedstatement pstmt ;
preparedstatement pstmt = initstatement ( string . format (
protected collection < string > performoperation ( ) throws sqlexception
return result ! = null & & result . booleanvalue ( ) ;
private resultset resultset ;
return ( results . size ( ) > 1 ) ? results : results . get ( 0 ) ;
errorpropertyvalue , e ) ;
protected abstract t performoperation ( ) throws sqlexception ;
resultset rs = pstmt . executequery ( ) ;
protected boolean performoperation ( ) throws sqlexception
buf . append ( " and " ) . append ( namecolumn ) . append ( " = ? " ) ;
query . append ( " , " ) . append ( namecolumn ) ;
private static final string sql get keys = "select distinct % s from % s where 1 = 1" ;
return count = = null | | count . intvalue ( ) = = 0 ;
{
throws sqlexception
if ( isdelimiterparsingdisabled ( ) )
keys . add ( rs . getstring ( 1 ) ) ;
new jdbcoperation < void > ( event add property , key , obj )
conn = getdatasource ( ) . getconnection ( ) ;
query . append ( " , ? " ) ;
boolean result = op . execute ( ) ;
ps . executeupdate ( ) ;
private abstract class jdbcoperation < t >
int idx = 1 ;
initstatement ( string . format ( sql clear ,
preparedstatement ps = initstatement ( string . format (
private static final string sql clear = "delete from % s where 1 = 1" ;
string statement ;
return pstmt ;
errorpropertyvalue = errpropval ;
jdbcoperation < object > op = new jdbcoperation < object > ( event read property , key , null )
fireerror ( erroreventtype , errorpropertyname ,
while ( rs . next ( ) )
ps . setobject ( idx + + , param ) ;
catch ( sqlexception e )
query . append ( valuecolumn ) ;
object value = rs . getobject ( valuecolumn ) ;
return ps ;
result = performoperation ( ) ;
return rs . next ( ) ? integer . valueof ( rs . getint ( 1 ) ) : null ;
object . . . params ) throws sqlexception
results . add ( value ) ;
return conn ;
jdbcoperation < integer > op = new jdbcoperation < integer > ( event read property , null , null )
private static final string sql get property = "select * from % s where % s = ? " ;
finally
query . append ( " ) values ( ? , ? " ) ;
try
iterator < ? > it = propertyconverter . toiterator ( value , getlistdelimiter ( ) ) ;
sql clear property , table , keycolumn ) , true , key ) ;
private connection conn ;
protected resultset openresultset ( string sql , boolean namecol ,
protected void addpropertydirect ( final string key , final object obj )
return op . execute ( ) ;
query . append ( table ) . append ( " ( " ) ;
return initstatement ( sql , namecol , params ) . executequery ( ) ;
if ( isdocommits ( ) )
statement = sql ;
@ override
while ( it . hasnext ( ) )
stringbuilder buf = new stringbuilder ( sql ) ;
return null ;
for ( object param : params )
jdbcoperation < boolean > op = new jdbcoperation < boolean > ( event read property , key , null )
statement = buf . tostring ( ) ;
return result ;
pstmt = getconnection ( ) . preparestatement ( statement ) ;
protected preparedstatement initstatement ( string sql , boolean namecol ,
results . add ( it . next ( ) ) ;
new jdbcoperation < collection < string > > ( event read property , null , null )
pstmt . setstring ( 3 , name ) ;
protected connection getconnection ( )
private static final string sql clear property = "delete from % s where % s = ? " ;
return rs . next ( ) ;
erroreventtype = errevtype ;
private final int erroreventtype ;
query . append ( keycolumn ) . append ( " , " ) ;
table ) , true ) . executeupdate ( ) ;
public object getproperty ( final string key )
list < object > results = new arraylist < object > ( ) ;
protected jdbcoperation ( int errevtype , string errpropname ,
sql get property , table , keycolumn ) , true , key ) ;
ps . setstring ( idx , name ) ;
protected preparedstatement createstatement ( string sql , boolean namecol )
query . append ( " ) " ) ;
public boolean containskey ( final string key )
pstmt . executeupdate ( ) ;
} ;
close ( conn , pstmt , resultset ) ;
if ( namecolumn ! = null )
} . execute ( ) ;
private static final string sql is empty = "select count ( * ) from % s where 1 = 1" ;
integer count = op . execute ( ) ;
t result = null ;
this . configurationnamecolumn = configurationnamecolumn ;
return valuecolumn ;
public databaseconfiguration ( )
private string configurationname ;
private datasource datasource ;
private boolean autocommit ;
public string getkeycolumn ( )
return datasource ;
public void setdatasource ( datasource datasource )
}
{
public void setkeycolumn ( string keycolumn )
return configurationname ;
if ( configurationnamecolumn ! = null )
private string keycolumn ;
return keycolumn ;
private string valuecolumn ;
protected void close ( connection conn , statement stmt , resultset rs )
public void setconfigurationnamecolumn ( string configurationnamecolumn )
ps . setstring ( idx , configurationname ) ;
setlogger ( logfactory . getlog ( getclass ( ) ) ) ;
this . autocommit = autocommit ;
private string configurationnamecolumn ;
buf . append ( " and " ) . append ( configurationnamecolumn ) . append ( " = ? " ) ;
public string getconfigurationname ( )
public string getconfigurationnamecolumn ( )
public void setvaluecolumn ( string valuecolumn )
pstmt . setstring ( 3 , configurationname ) ;
public datasource getdatasource ( )
public string gettable ( )
return table ;
public void setconfigurationname ( string configurationname )
this . configurationname = configurationname ;
if ( namecol & & configurationnamecolumn ! = null )
public string getvaluecolumn ( )
return configurationnamecolumn ;
return autocommit ;
public boolean isautocommit ( )
if ( isautocommit ( ) )
private string table ;
public void setautocommit ( boolean autocommit )
query . append ( " , " ) . append ( configurationnamecolumn ) ;
this . datasource = datasource ;
adderrorloglistener ( ) ;
public void settable ( string table )
string . valueof ( listdelimiterhandler . escape (
collection < string > values ;
listdelimiterhandler = handler ;
public xmlbuildervisitor ( document doc , listdelimiterhandler handler )
listdelimiterhandler . noop transformer ) ) ;
string . valueof ( getlistdelimiterhandler ( ) . escape ( value ,
string newvalue =
values = getlistdelimiterhandler ( ) . split (
newnode . getvalue ( ) ,
string txt =
child . getvalue ( ) . tostring ( ) , trim ) ;
private final listdelimiterhandler listdelimiterhandler ;
child . setvalue ( values . iterator ( ) . next ( ) ) ;
xmlbuildervisitor builder = new xmlbuildervisitor ( document , getlistdelimiterhandler ( ) ) ;
}
{
private static string escapecomments ( string value )
private string escapevalue ( string value )
return string . valueof ( getlistdelimiterhandler ( ) . escape (
out . print ( escapevalue ( value . tostring ( ) ) ) ;
values = getlistdelimiterhandler ( ) . split ( value , false ) ;
escapecomments ( value ) , listdelimiterhandler . noop transformer ) ) ;
setlistdelimiterhandler ( disabledlistdelimiterhandler . instance ) ;
while ( it . hasnext ( ) )
openresultset ( string . format ( sql get property ,
iterator < ? > it = getlistdelimiterhandler ( ) . parse ( value ) ;
resultset rs =
table , keycolumn ) , true , key ) ;
results . add ( it . next ( ) ) ;
return ( results . size ( ) > 1 ) ? results : results
. get ( 0 ) ;
listdelimiterhandler oldhandler = getlistdelimiterhandler ( ) ;
setlistdelimiterhandler ( oldhandler ) ;
private static filelocator createfullyinitializedlocator ( filelocator src ,
return createfullyinitializedlocator ( locator , url ) ;
return fullyinitializedlocatorfromurl ( locator ) ;
return locator ;
locator . getfilename ( ) ) ;
url url =
}
return createfullyinitializedlocator ( locator , locator . getsourceurl ( ) ) ;
{
if ( !islocationdefined ( locator ) )
return fullyinitializedlocatorfrompathandname ( locator ) ;
public static filelocator fullyinitializedlocator ( filelocator locator )
filelocator locator )
private static filelocator fullyinitializedlocatorfromurl (
return filelocator ( src ) . sourceurl ( url ) . filename ( getfilename ( url ) )
. basepath ( getbasepath ( url ) ) . create ( ) ;
if ( url = = null )
if ( locator . getbasepath ( ) ! = null & & locator . getfilename ( ) ! = null
return null ;
url url )
& & locator . getsourceurl ( ) ! = null )
if ( locator . getsourceurl ( ) ! = null )
private static filelocator fullyinitializedlocatorfrompathandname (
locate ( obtainfilesystem ( locator ) , locator . getbasepath ( ) ,
attrmap = new hashmap < string , collection < string > > ( ) ;
element element , boolean elemrefs , boolean trim )
child . setvalue ( value ) ;
private void appendattributes ( node node , element element , boolean elemrefs ,
}
{
for ( string value : values )
element element , boolean elemrefs )
processattributes ( node , element , elemrefs ) ;
else
private map < string , collection < string > > processattributes ( node node ,
attrmap . put ( attr . getname ( ) , values ) ;
map < string , collection < string > > attrmap ;
string attr , collection < string > values )
map < string , collection < string > > attributes =
private map < string , collection < string > > constructhierarchy ( node node ,
handledelimiters ( node , childnode , trimflag , attrmap ) ;
for ( map . entry < string , collection < string > > e : attrmap
private void handledelimiters ( node parent , node child , boolean trim ,
constructhierarchy ( childnode , child , elemrefs , trimflag ) ;
node . addattribute ( child ) ;
appendattributes ( c , null , false , e . getkey ( ) ,
appendattributes ( node , element , elemrefs , attr . getname ( ) , values ) ;
return attributes ;
map < string , collection < string > > attrmap =
attrmap = collections . emptymap ( ) ;
. entryset ( ) )
node child = new xmlnode ( attr , elemrefs ? element : null ) ;
e . getvalue ( ) ) ;
map < string , collection < string > > attrmap )
return attrmap ;
if ( attributes . getlength ( ) > 0 )
parent . getconfigurationnode ( ) , name ) ! = null )
configurationnodeiteratorbase < t >
private list < string > createattributedatalist (
configurationnodepointer < t > parent , qname name )
result . add ( name ) ;
}
{
if ( parent . getnodehandler ( ) . getattributevalue (
import java . util . set ;
protected int size ( )
. getattributes ( parent . getconfigurationnode ( ) ) ) ;
new linkedhashset < string > ( parent . getnodehandler ( )
return attributenames . size ( ) ;
attributenames = createattributedatalist ( parent , name ) ;
for ( string n : names )
private configurationnodepointer < t > parentpointer ;
private list < string > attributenames ;
parentpointer = parent ;
public configurationnodeiteratorattribute (
list < string > result , string name )
private void addattributedata ( configurationnodepointer < t > parent ,
@ override
return new configurationattributepointer < t > ( parentpointer ,
set < string > names =
attributenames . get ( position ) ) ;
import java . util . linkedhashset ;
class configurationnodeiteratorattribute < t > extends
list < string > result = new arraylist < string > ( ) ;
addattributedata ( parent , result , name . getname ( ) ) ;
protected nodepointer createnodepointer ( int position )
addattributedata ( parent , result , n ) ;
results . add ( o ) ;
for ( object o : getlistdelimiterhandler ( ) . parse ( value ) )
import javax . sql . datasource ;
private static boolean issectionnode ( immutablenode node )
value = parsevalue ( line . substring ( index + 1 ) , in ) ;
immutablenode . builder rootbuilder ,
if ( sectionbuilder = = null )
. value ( v ) . create ( ) ) ;
sectionbuilder . addchild ( new immutablenode . builder ( ) . name ( key )
immutablenode rootnode = createnewrootnode ( rootbuilder , sectionbuilders ) ;
import java . util . linkedhashmap ;
configuration subset = getsection ( section ) ;
}
createnodebuilders ( bufferedreader , rootbuilder , sectionbuilders ) ;
{
private void createnodebuilders ( bufferedreader in ,
map < string , immutablenode . builder > sectionbuilders = new linkedhashmap < string , immutablenode . builder > ( ) ;
sectionbuilder = sectionbuilders . get ( section ) ;
inmemorynodemodel parentmodel = getsubconfigurationparentmodel ( ) ;
for ( map . entry < string , immutablenode . builder > e : sectionbuilders
string key , string value )
for ( string section : getsections ( ) )
rootbuilder . addchild ( e . getvalue ( ) . name ( e . getkey ( ) ) . create ( ) ) ;
public hierarchicalconfiguration < immutablenode > getsection ( string name )
import org . apache . commons . configuration . tree . nodeselector ;
line = in . readline ( ) ;
/ /
return createsubconfigurationfortrackednode ( selector , parentmodel ) ;
createvaluenodes ( sectionbuilder , key , value ) ;
import org . apache . commons . configuration . tree . inmemorynodemodel ;
map < string , immutablenode . builder > sectionbuilders )
sectionbuilders . put ( section , sectionbuilder ) ;
import org . apache . commons . configuration . ex . configurationruntimeexception ;
for ( immutablenode node : getrootnode ( ) . getchildren ( ) )
import org . apache . commons . configuration . tree . immutablenode ;
private static immutablenode createnewrootnode (
immutablenode . builder sectionbuilder = rootbuilder ;
throws ioexception
catch ( configurationruntimeexception iex )
string key ;
addnodes ( null , rootnode . getchildren ( ) ) ;
immutablenode . builder rootbuilder = new immutablenode . builder ( ) ;
string line = in . readline ( ) ;
sectionbuilder = new immutablenode . builder ( ) ;
getlistdelimiterhandler ( ) . split ( value , false ) ;
nodeselector selector = parentmodel . trackchildnodewithcreation ( null , name , this ) ;
return !node . getchildren ( ) . isempty ( ) ;
. entryset ( ) )
import java . util . map ;
return rootbuilder . create ( ) ;
public iniconfiguration ( hierarchicalconfiguration < immutablenode > c )
return null ;
collection < string > values =
sections . add ( node . getnodename ( ) ) ;
private void createvaluenodes ( immutablenode . builder sectionbuilder ,
if ( sibling2 = = null )
referencenodehandler refhandler )
map < string , string > attributes = processattributes ( element ) ;
if ( e . getvalue ( ) ! = null )
return ( dochelper ! = null ) ? dochelper . getdocument ( ) : null ;
import org . w3c . dom . node ;
private map < string , string > constructhierarchy ( immutablenode . builder node ,
for ( int i = 0 ; i < children . getlength ( ) ; i + + )
elemrefmap . put ( getrootnode ( ) , dochelper ) ;
getmodel ( ) . addnodes ( null , top . getchildren ( ) , this ) ;
if ( text . length ( ) > 0 | | ( !childrenflag & & level ! = 0 ) )
mutableobject < string > refvalue , element element ,
immutablenode . builder childnode = new immutablenode . builder ( ) ;
}
elem . removeattribute ( attributes . item ( i ) . getnodename ( ) ) ;
updateelement ( element , refhandler . getvalue ( node ) ) ;
map < immutablenode , object > elemrefs , boolean trim , int level )
if ( values . size ( ) > 1 )
elem . removechild ( tn ) ;
listdelimiterhandler . noop transformer ) ) ;
if ( newnode . getvalue ( ) ! = null )
namednodemap attributes = elem . getattributes ( ) ;
else
new mutableobject < string > ( ) ;
protected void insert ( immutablenode newnode , immutablenode parent ,
private final map < node , node > elementmapping ;
referencenodehandler handler = getreferencehandler ( ) ;
return newhelper . getdocument ( ) ;
import javax . xml . transform . result ;
public void handleremovednodes ( referencenodehandler refhandler )
document doc = getdocument ( ) ;
( xmldocumenthelper ) handler . getreference ( handler . getrootnode ( ) ) ;
import org . apache . commons . configuration . tree . immutablenode ;
import java . util . collections ;
refvalue . setvalue ( text ) ;
( dochelper = = null ) ? xmldocumenthelper
string txt =
node removedelem = ( node ) ref ;
immutablenode . builder rootbuilder = new immutablenode . builder ( ) ;
c . addattributes ( attrmap ) ;
constructhierarchy ( rootbuilder , rootvalue ,
initrootelementtext ( newhelper . getdocument ( ) , getrootnode ( ) . getvalue ( ) ) ;
import javax . xml . transform . dom . domsource ;
immutablenode . builder c = new immutablenode . builder ( ) ;
import javax . xml . transform . source ;
getelement ( sibling1 , refhandler ) . getnextsibling ( ) ) ;
document = dochelper . getdocument ( ) ;
private void initproperties ( xmldocumenthelper dochelper , boolean elemrefs )
getelement ( parent , refhandler ) . appendchild ( elem ) ;
org . w3c . dom . node parentelem = element . getparentnode ( ) ;
private static boolean shouldtrim ( element element , boolean currenttrim )
c . value ( it . next ( ) ) ;
result = null ;
parent . addchild ( addedchildnode ) ;
if ( result = = null )
builder . processdocument ( handler ) ;
parentelem . removechild ( element ) ;
element . removechild ( txtnode ) ;
removereference ( ( element ) elementmapping . get ( removedelem ) ) ;
string text = determinevalue ( buffer . tostring ( ) , childrenflag , trimflag ) ;
public xmlbuildervisitor ( xmldocumenthelper dochelper ,
{
elemrefs ? new hashmap < immutablenode , object > ( ) : null ;
for ( int i = 0 ; i < attributes . getlength ( ) ; i + + )
trimflag | | ( stringutils . isblank ( content ) & & haschildren ) ;
new xmlbuildervisitor ( newhelper , getlistdelimiterhandler ( ) ) ;
this . getlogger ( ) . debug ( "unable to load the configuration" , e ) ;
document document = dochelper . getdocument ( ) ;
refhandler ) ;
constructhierarchy ( childnode , refchildvalue , child ,
. fornewdocument ( getrootelementname ( ) ) : dochelper
static class xmlbuildervisitor extends buildervisitor
immutablenode addedchildnode ;
if ( elemrefs ! = null )
text result = null ;
elementmapping = dochelper . getelementmapping ( ) ;
childtrim . booleanvalue ( ) , attrmap ) ;
xmldocumenthelper newhelper =
listdelimiterhandler = handler ;
nodetreewalker . instance . walkdfs ( refhandler . getrootnode ( ) , this ,
if ( element . getfirstchild ( ) ! = null )
newnode . getvalue ( ) ,
private static void updateattributes ( immutablenode node , element elem )
return getsubconfigurationparentmodel ( ) . getreferencenodehandler ( ) ;
private xmldocumenthelper getdocumenthelper ( )
childnode . name ( child . gettagname ( ) ) ;
private immutablenode createchildnodewithvalue (
rootelementname = getrootnode ( ) . getnodename ( ) ;
element elem = document . createelement ( newnode . getnodename ( ) ) ;
else if ( values . size ( ) = = 1 )
private element getelement ( immutablenode node ,
immutablenode newchild =
private static void clearattributes ( element elem )
collection < string > values ;
. entryset ( ) )
immutablenode sibling1 , immutablenode sibling2 ,
xmldocumenthelper dochelper = getdocumenthelper ( ) ;
beginread ( true ) ;
string value , boolean trim , map < string , string > attrmap )
import javax . xml . parsers . documentbuilderfactory ;
. createcopy ( ) ;
endread ( ) ;
immutablenode . builder parent , immutablenode . builder child ,
element = ( node ) reference ;
document olddocument = getdocument ( ) ;
element . insertbefore ( txtnode , element . getfirstchild ( ) ) ;
newelements = new hashmap < immutablenode , element > ( ) ;
getsubconfigurationparentmodel ( ) . addreferences ( elemrefmap ) ;
rootelementname = name ;
result result = new streamresult ( writer ) ;
elemrefs , trimflag , level + 1 ) ;
immutablenode top = rootbuilder . value ( rootvalue . getvalue ( ) ) . create ( ) ;
public xmlconfiguration ( hierarchicalconfiguration < immutablenode > c )
if ( result instanceof cdatasection )
createchildnodewithvalue ( node , childnode ,
element element = getelement ( node , refhandler ) ;
private static text findtextnodeforupdate ( element elem )
string . valueof ( listdelimiterhandler . escape ( value ,
else if ( sibling1 ! = null )
textnodes . add ( result ) ;
if ( doc = = null )
import javax . xml . parsers . documentbuilder ;
for ( org . w3c . dom . node tn : textnodes )
if ( elementnew ! = null )
if ( reference instanceof xmldocumenthelper )
olddocument = = null ) ;
return ( xmldocumenthelper ) handler . getreference ( handler . getrootnode ( ) ) ;
child . value ( it . next ( ) ) ;
return elementnew ;
c . name ( addedchildnode . getnodename ( ) ) ;
mutableobject < string > rootvalue = new mutableobject < string > ( ) ;
private referencenodehandler getreferencehandler ( )
boolean haschildren = false ;
element =
xmldocumenthelper . transform ( transformer , source , result ) ;
return addedchildnode ;
import org . apache . commons . configuration . tree . referencenodehandler ;
finally
getelement ( parent , refhandler ) . insertbefore ( elem ,
try
values = getlistdelimiterhandler ( ) . split ( value , trim ) ;
transformer transformer = xmldocumenthelper . createtransformer ( ) ;
org . w3c . dom . node nd = children . item ( i ) ;
text txtnode = findtextnodeforupdate ( element ) ;
for ( object ref : refhandler . removedreferences ( ) )
return doc . getdocumentelement ( ) . getnodename ( ) ;
elem . appendchild ( document . createtextnode ( txt ) ) ;
textnodes . add ( nd ) ;
public void processdocument ( referencenodehandler refhandler )
collection < org . w3c . dom . node > textnodes =
updateattributes ( newnode , elem ) ;
updateattributes ( node , element ) ;
haschildren = true ;
private final listdelimiterhandler listdelimiterhandler ;
object reference = refhandler . getreference ( node ) ;
private final map < immutablenode , element > newelements ;
. getdocumentelement ( ) ;
clearattributes ( elem ) ;
( ( xmldocumenthelper ) reference ) . getdocument ( )
private final document document ;
@ override
import org . apache . commons . configuration . tree . nodetreewalker ;
while ( it . hasnext ( ) )
mutableobject < string > refchildvalue =
source source = new domsource ( createdocument ( ) ) ;
iterator < string > it = values . iterator ( ) ;
element elementnew = newelements . get ( node ) ;
document . getdocumentelement ( ) , elemrefmap , true , 0 ) ;
xmldocumenthelper dochelper =
if ( value ! = null )
"the name of the root element "
getelement ( parent , refhandler ) . getfirstchild ( ) ) ;
import javax . xml . parsers . parserconfigurationexception ;
parent . addchild ( c . create ( ) ) ;
xmlbuildervisitor builder =
return result ;
result = ( text ) nd ;
+ "cannot be changed when loaded from an xml document!" ) ;
private void removereference ( element element )
import javax . xml . transform . stream . streamresult ;
childnode . addattributes ( attrmap ) ;
map < immutablenode , object > elemrefmap =
boolean trimflag )
if ( getdocument ( ) ! = null )
listdelimiterhandler handler )
throw new unsupportedoperationexception (
elemrefs . put ( newchild , child ) ;
elem . setattribute ( e . getkey ( ) , e . getvalue ( ) . tostring ( ) ) ;
return ( element ! = null ) ? ( element ) elementmapping . get ( element )
nodelist children = elem . getchildnodes ( ) ;
newelements . put ( newnode , elem ) ;
builder . handleremovednodes ( handler ) ;
private static string determinevalue ( string content , boolean haschildren ,
boolean childrenflag = haschildren | | attributes . size ( ) > 1 ;
values = collections . emptylist ( ) ;
if ( nd instanceof text )
private static map < string , string > processattributes ( element element )
addedchildnode = child . create ( ) ;
for ( map . entry < string , object > e : node . getattributes ( )
initproperties ( xmldocumenthelper . forsourcedocument ( newdocument ) ,
private void updateelement ( element element , object value )
string . valueof ( listdelimiterhandler . escape (
private document createdocument ( ) throws configurationexception
import org . apache . commons . lang3 . mutable . mutableobject ;
: document . getdocumentelement ( ) ;
if ( parentelem ! = null )
node element ;
refchildvalue . getvalue ( ) ,
new arraylist < org . w3c . dom . node > ( ) ;
child . value ( values . iterator ( ) . next ( ) ) ;
import javax . xml . transform . transformer ;
element . appendchild ( txtnode ) ;
protected void update ( immutablenode node , object reference ,
}
@ override
{
public inmemorynodemodel getnodemodel ( )
return new trackednodemodel ( getparent ( ) , getrootselector ( ) , true ) ;
return getparent ( ) . getnodemodel ( ) ;
hierarchicalconfiguration < ? > tmpconfiguration ;
return result ;
props . put ( key , interpolate ( getnode ( ) . getattribute ( key ) ) ) ;
return config . getnodemodel ( ) . getnodehandler ( ) . getrootnode ( )
return new nodedata < t > ( handler . getrootnode ( ) , handler ) ;
return ( value = = null ) ? null : string . valueof ( interpolate ( value ) ) ;
public < t > xmlbeandeclaration ( hierarchicalconfiguration < t > config )
object value = nd . getattribute ( attr ) ;
return wrapinnodedata ( handler . getchildren ( node ) ) ;
object obj = nested . get ( child . nodename ( ) ) ;
public hierarchicalconfiguration < ? > getconfiguration ( )
public boolean matchesconfigrootnode ( hierarchicalconfiguration < ? > config )
private final t node ;
private final hierarchicalconfiguration < ? > configuration ;
}
return !nd . getattributes ( ) . contains ( attr bean class name ) ;
{
if ( !isreservedname ( child . nodename ( ) ) )
. equals ( node ) ;
return name = = null | | name . startswith ( reserved prefix ) ;
import org . apache . commons . configuration . tree . nodehandler ;
throw new configurationruntimeexception ( "unable to match node for "
private constructorarg createconstructorarg ( nodedata < ? > child )
result . add ( new nodedata < t > ( node , handler ) ) ;
getconfiguration ( ) . getinterpolator ( ) ;
import java . util . set ;
private string getattribute ( nodedata < ? > nd , string attr )
. configurationsat ( node . nodename ( ) ) )
this . node = createnodedatafromconfiguration ( tmpconfiguration ) ;
return getconfiguration ( ) . getstring ( attr bean factory , null ) ;
public < t > xmlbeandeclaration ( hierarchicalconfiguration < t > config , string key )
protected boolean isreservedname ( string name )
if ( node . matchesconfigrootnode ( config ) )
return node ;
private final nodedata < ? > node ;
tmpconfiguration = new basehierarchicalconfiguration ( ) ;
if ( nested . containskey ( child . nodename ( ) ) )
+ node . nodename ( ) ) ;
nested . put ( child . nodename ( ) , createbeandeclaration ( child ) ) ;
return wrapinnodedata ( handler . getchildren ( node , name ) ) ;
public object getattribute ( string key )
public set < string > getattributes ( )
for ( hierarchicalconfiguration < ? > config : getconfiguration ( )
nodehandler < t > handler = config . getnodemodel ( ) . getnodehandler ( ) ;
public string nodename ( )
xmlbeandeclaration ( hierarchicalconfiguration < ? > config ,
catch ( configurationruntimeexception iex )
public list < nodedata < t > > getchildren ( )
return handler . getattributevalue ( node , key ) ;
list < nodedata < t > > result = new arraylist < nodedata < t > > ( nodes . size ( ) ) ;
hierarchicalconfiguration < t > config )
for ( string key : getnode ( ) . getattributes ( ) )
private list < nodedata < t > > wrapinnodedata ( list < t > nodes )
private final nodehandler < t > handler ;
return new xmlbeandeclaration ( config , node ) ;
beandeclaration createbeandeclaration ( nodedata < ? > node )
static class nodedata < t >
public list < nodedata < t > > getchildren ( string name )
handler = hndlr ;
return handler . getattributes ( node ) ;
nested . put ( child . nodename ( ) , list ) ;
for ( nodedata < ? > child : getnode ( ) . getchildren ( ) )
private static < t > nodedata < t > createnodedatafromconfiguration (
private static boolean isbeandeclarationargument ( nodedata < ? > nd )
import org . apache . commons . configuration . basehierarchicalconfiguration ;
for ( nodedata < ? > child : getnode ( ) . getchildren ( elem ctor arg ) )
public nodedata ( t nd , nodehandler < t > hndlr )
nodedata < ? > getnode ( )
nodedata < ? > node )
return handler . nodename ( node ) ;
public < t > xmlbeandeclaration ( hierarchicalconfiguration < t > config , string key ,
for ( t node : nodes )
if ( !isreservedname ( key ) )
private void initsubnodeconfiguration ( hierarchicalconfiguration < ? > conf )
node = nd ;
hierarchicalconfiguration < ? > config ) throws configurationexception
for ( string element : config sections )
for ( hierarchicalconfiguration < ? > src : srcdecl )
list < ? extends hierarchicalconfiguration < ? > > nodes =
protected configurationbuilder < ? extends hierarchicalconfiguration < ? > > setupdefinitionbuilder (
hierarchicalconfiguration < ? > config = getdefinitionconfiguration ( ) ;
hierarchicalconfiguration < ? > defconfig , configuration resultconfig )
configs . iterator ( ) ; it . hasnext ( ) ; )
protected hierarchicalconfiguration < ? > getdefinitionconfiguration ( )
protected filesystem initfilesystem ( hierarchicalconfiguration < ? > config )
{
for ( iterator < ? extends hierarchicalconfiguration < ? > > it =
private final collection < hierarchicalconfiguration < ? > > unionbuilders ;
hierarchicalconfiguration < ? > defconfig , string key )
hierarchicalconfiguration < ? > src , configurationdeclaration decl )
public collection < hierarchicalconfiguration < ? > > getunionsources ( )
configurationbuilder < ? extends hierarchicalconfiguration < ? > > defbuilder =
for ( hierarchicalconfiguration < ? > config : nodes )
protected configurationbuilder < ? extends hierarchicalconfiguration < ? > > createxmldefinitionbuilder (
protected void configureentityresolver ( hierarchicalconfiguration < ? > config ,
protected void initsystemproperties ( hierarchicalconfiguration < ? > config ,
final configurationbuilder < ? extends hierarchicalconfiguration < ? > > defbuilder )
private final collection < hierarchicalconfiguration < ? > > overridebuilders ;
public synchronized configurationbuilder < ? extends hierarchicalconfiguration < ? > > getdefinitionbuilder ( )
private configurationbuilder < ? extends hierarchicalconfiguration < ? > > definitionbuilder ;
private void registerconfiguredproviders ( hierarchicalconfiguration < ? > defconfig )
public collection < hierarchicalconfiguration < ? > > getoverridesources ( )
list < ? extends hierarchicalconfiguration < ? > > configs =
hierarchicalconfiguration < ? > config )
private list < ? extends hierarchicalconfiguration < ? > > fetchtopleveloverrideconfigs (
collection < hierarchicalconfiguration < ? > > srcdecl )
new linkedlist < hierarchicalconfiguration < ? > > ( ) ;
private hierarchicalconfiguration < ? > definitionconfiguration ;
@ override
defbuilder . addeventlistener ( configurationbuilderevent . reset ,
b . removeeventlistener ( configurationbuilderevent . reset ,
import org . apache . commons . configuration . builder . configurationbuilderevent ;
private eventlistener < configurationbuilderevent > changelistener ;
import org . apache . commons . configuration . event . eventlistener ;
changelistener = new eventlistener < configurationbuilderevent > ( )
new eventlistener < configurationbuilderevent > ( )
changelistener ) ;
synchronized ( combinedconfigurationbuilder . this ) {
public void onevent ( configurationbuilderevent event ) {
builder . addeventlistener ( configurationbuilderevent . reset ,
}
{
fetchlayoutdata ( event . getpropertyname ( ) ) ;
propertylayoutdata data =
. geteventtype ( ) ) )
if ( configurationevent . add property . equals ( event . geteventtype ( ) ) )
else if ( configurationevent . clear . equals ( event . geteventtype ( ) ) )
boolean contained =
public void onevent ( configurationevent event )
import org . apache . commons . configuration . event . eventlistener ;
else if ( configurationevent . clear property . equals ( event
layoutdata . containskey ( event . getpropertyname ( ) ) ;
public class propertiesconfigurationlayout implements eventlistener < configurationevent >
config . removeeventlistener ( configurationevent . any , this ) ;
else if ( configurationevent . set property . equals ( event
config . addeventlistener ( configurationevent . any , this ) ;
string . format ( sql get property , table , keycolumn ) , true , key ) ;
resultset rs = openresultset (
if ( getheadercomment ( ) = = null )
{
}
setheadercomment ( extractcomment ( commentlines , 0 , index - 1 ) ) ;
if ( loadcounter = = 1 & & layoutdata . isempty ( ) )
return new eventlistener < configurationbuilderevent > ( )
for ( configurationdeclaration cd : overridedeclarations )
addchildconfiguration ( ccresult , srcdecl . get ( i ) , builders . get ( i ) ) ;
for ( int i = 0 ; i < srcdecl . size ( ) ; i + + )
configurationdeclaration decl ) throws configurationexception
return declarations ;
private final list < configurationbuilder < ? extends configuration > > overridebuilders ;
uniondeclarations . addall ( createdeclarations ( config . childconfigurationsat ( key union ) ) ) ;
public list < configurationdeclaration > getoverridesources ( )
for ( configurationdeclaration cd : uniondeclarations )
private final list < configurationdeclaration > uniondeclarations ;
new arraylist < > ( configs . size ( ) ) ;
}
allbuilders = new linkedlist < > ( ) ;
private collection < configurationdeclaration > createdeclarations (
{
data . createandaddconfigurations ( addconfig , data . uniondeclarations ,
overridedeclarations . addall ( createdeclarations ( fetchtopleveloverrideconfigs ( config ) ) ) ;
private final list < configurationdeclaration > overridedeclarations ;
unionbuilders . add ( createconfigurationbuilder ( cd ) ) ;
overridebuilders = new arraylist < > ( ) ;
unionbuilders = new arraylist < > ( ) ;
data . createandaddconfigurations ( result , data . getoverridesources ( ) ,
for ( hierarchicalconfiguration < ? > c : configs )
data . unionbuilders ) ;
overridedeclarations = new arraylist < > ( ) ;
namedbuilders = new hashmap < > ( ) ;
+ decl . getconfiguration ( ) . getrootelementname ( ) ) ;
declarations . add ( new configurationdeclaration ( this , c ) ) ;
overridebuilders . add ( createconfigurationbuilder ( cd ) ) ;
changelistener = createbuilderchangelistener ( ) ;
providerfortag ( decl . getconfiguration ( ) . getrootelementname ( ) ) ;
public list < configurationdeclaration > getunionsources ( )
return overridedeclarations ;
data . overridebuilders ) ;
collection < configurationdeclaration > declarations =
uniondeclarations = new arraylist < > ( ) ;
private final list < configurationbuilder < ? extends configuration > > unionbuilders ;
return uniondeclarations ;
import java . util . arraylist ;
private final eventlistener < configurationbuilderevent > changelistener ;
collection < ? extends hierarchicalconfiguration < ? > > configs )
private eventlistener < configurationbuilderevent > createbuilderchangelistener ( )
list < configurationbuilder < ? extends configuration > > builders )
list < configurationdeclaration > srcdecl ,
overridedeclarations . addall ( createdeclarations ( config . childconfigurationsat ( key override ) ) ) ;
if ( primitive types . containskey ( paramtypenames [ i ] ) )
primitivetypes . put ( "short" , short . class ) ;
format ( "class ' % s' cannot be loaded" , paramtypenames [ i ] ) ) ;
if ( defaultconstructorarguments ! = null )
continue ;
arrays . tostring ( paramtypenames ) ) ,
return this ;
private static final map < string , class < ? > > primitive types ;
paramtypes [ i ] = classloader . loadclass ( paramtypenames [ i ] ) ;
import java . util . hashmap ;
primitivetypes . put ( "double" , double . class ) ;
}
{
primitivetypes . put ( "float" , float . class ) ;
primitive types = collections . unmodifiablemap ( primitivetypes ) ;
primitivetypes . put ( "byte" , byte . class ) ;
primitivetypes . put ( "char" , char . class ) ;
try
public objectcreatebuilder usingconstructor ( class < ? > . . . constructorargumenttypes )
primitivetypes . put ( "boolean" , boolean . class ) ;
primitivetypes . put ( "int" , int . class ) ;
this . reporterror ( format ( "createobject ( ) . usingconstructor ( % s ) " ,
if ( defaultconstructorarguments = = null )
import java . util . collections ;
this . constructorargumentstype = constructorargumenttypes ;
class < ? > [ ] paramtypes = new class < ? > [ paramtypenames . length ] ;
objectcreaterule . setdefaultconstructorarguments ( defaultconstructorarguments ) ;
public objectcreatebuilder usingdefaultconstructorarguments ( object . . . defaultconstructorarguments )
for ( int i = 0 ; i < paramtypenames . length ; i + + )
if ( constructorargumenttypes = = null )
objectcreaterule . setconstructorargumenttypes ( constructorargumentstype ) ;
hashmap < string , class < ? > > primitivetypes = new hashmap < string , class < ? > > ( ) ;
reporterror ( "createobject ( ) . usingdefaultconstructorarguments ( object [ ] ) " , "null defaultconstructorarguments not allowed" ) ;
catch ( classnotfoundexception e )
primitivetypes . put ( "long" , long . class ) ;
import java . util . map ;
static
reporterror ( "createobject ( ) . usingconstructor ( class < ? > [ ] ) " , "null constructorargumenttypes not allowed" ) ;
this . defaultconstructorarguments = defaultconstructorarguments ;
private object [ ] defaultconstructorarguments ;
paramtypes [ i ] = primitive types . get ( paramtypenames [ i ] ) ;
return adaptedclassloader . loadclass ( name ) ;
primitivetypes . put ( "short" , short . class ) ;
this . adaptedclassloader = adaptedclassloader ;
final class binderclassloader
private static final map < string , class < ? > > primitive types ;
import java . util . hashmap ;
primitivetypes . put ( "double" , double . class ) ;
}
{
return primitive types . get ( name ) ;
protected synchronized class < ? > loadclass ( string name , boolean resolve )
throws classnotfoundexception
primitivetypes . put ( "float" , float . class ) ;
if ( primitive types . containskey ( name ) )
primitive types = collections . unmodifiablemap ( primitivetypes ) ;
primitivetypes . put ( "byte" , byte . class ) ;
primitivetypes . put ( "char" , char . class ) ;
primitivetypes . put ( "boolean" , boolean . class ) ;
primitivetypes . put ( "int" , int . class ) ;
public classloader getadaptedclassloader ( )
return adaptedclassloader ;
import java . util . collections ;
package org . apache . commons . digester3 . binder ;
private final classloader adaptedclassloader ;
@ override
hashmap < string , class < ? > > primitivetypes = new hashmap < string , class < ? > > ( ) ;
primitivetypes . put ( "long" , long . class ) ;
import java . util . map ;
static
extends classloader
public binderclassloader ( classloader adaptedclassloader )
}
{
if ( stackaction ! = null )
t popped = this . < t > npesafecast ( namedstack . pop ( ) ) ;
return popped ;
return null ;
return ( null ) ;
popped = stackaction . onpop ( this , stackname , popped ) ;
try
log . warn ( "empty stack ( returning null ) " ) ;
catch ( emptystackexception e )
classloader ctxloader =
public void begin ( attributelist attributes ) throws exception {
string realclassname = classname ;
protected string attributename = null ;
if ( digester . getdebug ( ) > = 1 )
clazz = ctxloader . loadclass ( realclassname ) ;
this . attributename = attributename ;
digester . log ( "pop " + top . getclass ( ) . getname ( ) ) ;
}
digester . push ( instance ) ;
import org . xml . sax . attributelist ;
attributename = null ;
public objectcreaterule ( digester digester , string classname ) {
} else {
this ( digester , classname , null ) ;
string value = attributes . getvalue ( attributename ) ;
package org . apache . commons . digester ;
classname = null ;
public void end ( ) throws exception {
clazz = class . forname ( realclassname ) ;
this . classname = classname ;
if ( attributename ! = null ) {
public class objectcreaterule extends rule {
protected string classname = null ;
digester . log ( "new " + realclassname ) ;
object instance = clazz . newinstance ( ) ;
super ( digester ) ;
realclassname = value ;
class clazz = null ;
object top = digester . pop ( ) ;
thread . currentthread ( ) . getcontextclassloader ( ) ;
if ( ctxloader = = null ) {
if ( value ! = null )
public objectcreaterule ( digester digester , string classname ,
public void finish ( ) throws exception {
import java . lang . classloader ;
string attributename ) {
remove ( k ) ;
copy . setiseternal ( false ) ;
private final compositecache < k , v > delegate ;
dogetcontrollingexpiry ( k , true , true , false , completionlistener ! = null ) ;
v val = elt ! = null ? elt . getval ( ) : null ;
lastkey = keys . next ( ) ;
if ( expiryforaccess ! = null & & ( !elt . getelementattributes ( ) . getiseternal ( ) | | !expiryforaccess . iseternal ( ) ) )
final icacheelement < k , v > oldelt = delegate . get ( key ) ;
final duration duration = created ? expirypolicy . getexpiryforcreation ( ) : expirypolicy . getexpiryforupdate ( ) ;
import javax . cache . cacheexception ;
final icacheelement < k , v > v = delegate . get ( cachekey ) ;
element . setelementattributes ( copy ) ;
final k cachekey = key ;
delegate . update ( new cacheelement < k , v > ( name , copy , element . getval ( ) , element . getelementattributes ( ) ) ) ;
final duration expiryforaccess = expirypolicy . getexpiryforaccess ( ) ;
final boolean eternal = duration . iseternal ( ) ;
}
{
final v v = dogetcontrollingexpiry ( key , false , false , true , false ) ;
final v value = v ! = null & & v . getval ( ) ! = null ? v . getval ( ) : null ;
import org . apache . commons . jcs . engine . behavior . ielementattributes ;
return element ;
delegate . update ( element ) ;
final icacheelement < k , v > element = createelement ( key , v , duration ) ;
else
if ( isnotzero ( duration ) )
if ( duration ! = null )
final iterator < k > keys = new hashset < k > ( delegate . getkeyset ( ) ) . iterator ( ) ;
else if ( expiryforaccess ! = null & & ( !elt . getelementattributes ( ) . getiseternal ( ) | | !expiryforaccess . iseternal ( ) ) )
copy . setmaxlife ( duration . gettimeunit ( ) . tomillis ( duration . getdurationamount ( ) ) ) ;
v = doload ( key , false , propagateloadexception ) ;
result . put ( key , val ) ;
try
delegate . update ( createelement ( key , elt . getval ( ) , expiryforaccess ) ) ;
private void expires ( final k cachekey )
return dogetcontrollingexpiry ( key , true , false , false , true ) ;
return duration = = null | | !duration . iszero ( ) ;
final v old = oldelt ! = null ? oldelt . getval ( ) : null ;
copy . setiseternal ( true ) ;
if ( expiryforaccess ! = null )
touch ( key , elt ) ;
import java . io . ioexception ;
final icacheelement < k , v > element = new cacheelement < k , v > ( name , key , v ) ;
final v v = dogetcontrollingexpiry ( key , false , false , false , false ) ;
delegate . update ( createelement ( key , v , expiryforaccess ) ) ;
if ( !created )
final icacheelement < k , v > elt = delegate . get ( cachekey ) ;
statistics . addgettime ( times . now ( false ) - getstart ) ;
delegate = cache ;
final properties properties , final compositecache < k , v > cache )
final icacheelement < k , v > element = createelement ( jcskey , value , duration ) ;
private icacheelement < k , v > createelement ( final k key , final v v , final duration duration )
final k jcskey = storebyvalue ? copy ( serializer , manager . getclassloader ( ) , key ) : key ;
import org . apache . commons . jcs . engine . behavior . icacheelement ;
copy . settimefactorformilliseconds ( 1 ) ;
eventtype . removed , null , cachekey , elt . getval ( ) ) ) ) ;
private void touch ( final k key , final icacheelement < k , v > element )
private v dogetcontrollingexpiry ( final k key , final boolean updateacess , final boolean forcedoload , final boolean skipload ,
delegate . removeall ( ) ;
private static boolean isnotzero ( final duration duration )
v value = elt . getval ( ) ;
if ( isnotzero ( expiryforaccess ) )
catch ( final ioexception e )
v v = elt ! = null ? elt . getval ( ) : null ;
if ( !isnotzero ( expiryforaccess ) )
final icacheelement < k , v > elt = delegate . get ( key ) ;
v oldvalue = elt . getval ( ) ;
throw new cacheexception ( e ) ;
expires ( key ) ;
writer . write ( new jcsentry < k , v > ( jcskey , value ) ) ;
if ( eternal )
final k copy = copy ( serializer , manager . getclassloader ( ) , key ) ;
import org . apache . commons . jcs . engine . cacheelement ;
for ( final k k : delegate . getkeyset ( ) )
return delegate . get ( key ) ! = null ;
final ielementattributes copy = delegate . getelementattributes ( ) . copy ( ) ;
log . info ( "in dispose , [ " + this . cacheattr . getcachename ( ) + " ] fromremote [ " + fromremote + " ] " ) ;
parametersasarray [ i ] = newcacheinvocationparameterimpl ( parametertypes [ i ] , args [ i ] , parameterannotations [ i ] , i ) ;
final cacheinvocationparameter [ ] parametersasarray = new cacheinvocationparameter [ indexes = = null ? args . length : indexes . length ] ;
final class < ? > [ ] parametertypes = getmethod ( ) . getparametertypes ( ) ;
final int i = indexes [ idx ] ;
if ( indexes = = null )
}
{
for ( int idx = 0 ; idx < indexes . length ; idx + + )
import java . util . hashset ;
else
protected cacheinvocationparameter [ ] dogetallparameters ( final integer [ ] indexes )
parameters = dogetallparameters ( null ) ;
private cacheinvocationparameterimpl newcacheinvocationparameterimpl ( final class < ? > type , final object arg ,
import java . lang . annotation . annotation ;
final annotation [ ] annotations , final int i ) {
final annotation [ ] [ ] parameterannotations = getmethod ( ) . getparameterannotations ( ) ;
return parametersasarray ;
final object [ ] args = delegate . getparameters ( ) ;
return new cacheinvocationparameterimpl ( type , arg , new hashset < annotation > ( aslist ( annotations ) ) , i ) ;
for ( int i = 0 ; i < args . length ; i + + )
public void registryfacput ( auxiliarycachefactory auxfac )
public auxiliarycachefactory registryfacget ( string name )
public auxiliarycacheattributes registryattrget ( string name )
log . error ( "could not instantiate auxattr named '" + attrname + "'" ) ;
auxiliarycaches . put ( key , cache ) ;
auxfac = optionconverter . instantiatebykey ( props , prefix , null ) ;
public < k , v > blockdiskcache < k , v > createcache ( auxiliarycacheattributes iaca , icompositecachemanager cachemgr ,
string key = string . format ( "aux . % s . region . % s" , auxname , cachename ) ;
public void addcache ( string cachename , icache < ? , ? > cache )
@ suppresswarnings ( "unchecked" )
return ( auxiliarycache < k , v > ) auxiliarycaches . get ( key ) ;
cache . setcacheeventlogger ( cacheeventlogger ) ;
log . debug ( "parsing options for '" + attrname + "'" ) ;
if ( auxfac = = null )
auxiliarycacheattributes auxattr = compositecachemanager . registryattrget ( auxname ) ;
string attrname = auxiliary prefix + auxname + attribute prefix ;
auxattr = optionconverter . instantiatebykey ( props , prefix , null ) ;
}
{
public void addauxiliarycache ( string auxname , string cachename , auxiliarycache < ? , ? > cache )
auxattr . setname ( auxname ) ;
if ( auxattr = = null )
factory . dispose ( ) ;
auxiliaryfactoryregistry . clear ( ) ;
public void setdefaultauxvalues ( string defaultauxvalues )
auxiliarycachefactory auxfac = compositecachemanager . registryfacget ( auxname ) ;
catch ( exception e )
try
cache . setelementserializer ( elementserializer ) ;
propertysetter . setproperties ( auxattr , props , attrname + " . " ) ;
import org . apache . commons . jcs . auxiliary . auxiliarycache ;
compositecachemanager . addauxiliarycache ( auxname , regname , auxcache ) ;
auxattr . setcachename ( regname ) ;
new concurrenthashmap < string , auxiliarycache < ? , ? > > ( 11 ) ;
import org . apache . commons . jcs . auxiliary . abstractauxiliarycachefactory ;
public < k , v > indexeddiskcache < k , v > createcache ( auxiliarycacheattributes iaca , icompositecachemanager cachemgr ,
indexeddiskcache < k , v > cache = new indexeddiskcache < k , v > ( idca ) ;
public < k , v > auxiliarycache < k , v > getauxiliarycache ( string auxname , string cachename )
auxfac . initialize ( ) ;
ielementserializer elementserializer = auxiliarycacheconfigurator . parseelementserializer ( props , auxprefix ) ;
log . debug ( "end of parsing for '" + attrname + "'" ) ;
auxcache = auxfac . createcache ( auxattr , compositecachemanager , cacheeventlogger , elementserializer ) ;
blockdiskcache < k , v > cache = new blockdiskcache < k , v > ( idca ) ;
auxiliarycache < k , v > auxcache = ( auxiliarycache < k , v > ) compositecachemanager . getauxiliarycache ( auxname , regname ) ;
extends abstractauxiliarycachefactory
auxfac . setname ( auxname ) ;
compositecachemanager . registryfacput ( auxfac ) ;
log . error ( "could not instantiate auxiliary cache named \ "" + regname + " \ " . " ) ;
compositecachemanager . registryattrput ( auxattr ) ;
private final map < string , auxiliarycache < ? , ? > > auxiliarycaches =
@ override
string prefix = auxiliary prefix + auxname ;
icacheeventlogger cacheeventlogger = auxiliarycacheconfigurator . parsecacheeventlogger ( props , auxprefix ) ;
auxiliaryattributeregistry . clear ( ) ;
auxattr = auxattr . copy ( ) ;
string auxprefix = auxiliary prefix + auxname ;
string prefix = auxiliary prefix + auxname + attribute prefix ;
return cache ;
return null ;
public void registryattrput ( auxiliarycacheattributes auxattr )
for ( auxiliarycachefactory factory : auxiliaryfactoryregistry . values ( ) )
log . error ( "could not instantiate auxfactory named \ "" + auxname + " \ " . " ) ;
if ( auxcache = = null )
if ( log . isdebugenabled ( ) )
import java . util . concurrent . atomic . atomicinteger ;
extends lrumap < k , int [ ] >
super ( ) ;
log . debug ( logcachename + "removing key : [ " + key + " ] from key store . " ) ;
import org . apache . commons . jcs . utils . struct . abstractlrumap ;
public lrumapcountlimited ( int maxkeysize ) {
protected void processremovedlru ( k key , int [ ] value )
if ( value ! = null ) {
protected void processremovedlru ( k key , indexeddiskelementdescriptor value )
extends lrumap < k , indexeddiskelementdescriptor >
log . debug ( logcachename + "key store size : [ " + this . size ( ) + " ] . " ) ;
this . maxsize = maxkeysize ;
value = super . remove ( key ) ;
import org . apache . commons . jcs . auxiliary . disk . indexed . indexeddiskelementdescriptor ;
keyhash = new lrumapcountlimited ( maxkeysize ) ;
}
this . disklimittype = cacheattributes . getdisklimittype ( ) ;
keyhash = new lrumapsizelimited ( maxkeysize ) ;
{
dooptimizerealtime ( ) ;
return value ;
public string tag = "orig - lru - count" ;
public int [ ] remove ( object key ) {
private int maxsize = - 1 ;
public lrumapsizelimited ( int maxkeysize )
contentsize . addandget ( ( int ) ( ( math . ceil ( value . len / 1024 . 0 ) ) * - 1 ) ) ;
public string tag = "orig - lru - size" ;
contentsize . addandget ( ( int ) ( ( math . ceil ( ( value . len + indexeddisk . header size bytes ) / 1024 . 0 ) ) * - 1 ) ) ;
contentsize . addandget ( ( int ) math . ceil ( ( value . len + indexeddisk . header size bytes ) / 1024 . 0 ) ) ;
} else {
public indexeddiskelementdescriptor put ( k key , indexeddiskelementdescriptor value ) {
if ( this . disklimittype . equals ( disklimittype . size ) ) {
private disklimittype disklimittype = disklimittype . count ;
public int [ ] put ( k key , int [ ] value ) {
int [ ] value = null ;
import org . apache . commons . jcs . utils . struct . lrumap ;
addtorecyclebin ( value ) ;
this . blocksize = cacheattributes . getblocksizebytes ( ) ;
log . debug ( logcachename + "key store size : [ " + super . size ( ) + " ] . " ) ;
private int blocksize ;
public class lrumapsizelimited
return maxsize > 0 & & contentsize . intvalue ( ) > maxsize & & this . size ( ) > 0 ;
public lrumapsizelimited ( )
elems . add ( new statelement < integer > ( "max key size" , this . maxkeysize ) ) ;
extends abstractlrumap < k , int [ ] >
public class lrumapcountlimited
if ( log . isdebugenabled ( ) )
return super . put ( key , value ) ;
indexeddiskelementdescriptor value = null ;
extends abstractlrumap < k , indexeddiskelementdescriptor >
@ override
contentsize . addandget ( ( int ) math . ceil ( value . length * blocksize / 1024 . 0 ) ) ;
contentsize . addandget ( ( int ) ( ( math . ceil ( value . length * blocksize / 1024 . 0 ) ) * - 1 ) ) ;
if ( this . disklimittype . equals ( disklimittype . count ) ) {
return maxsize > 0 & & contentsize . intvalue ( ) > maxsize & & this . size ( ) > 1 ;
blockdiskcache . freeblocks ( value ) ;
import org . apache . commons . jcs . auxiliary . disk . behavior . idiskcacheattributes . disklimittype ;
} finally {
private atomicinteger contentsize = new atomicinteger ( ) ;
this . disklimittype = cattr . getdisklimittype ( ) ;
protected boolean shouldremove ( ) {
public indexeddiskelementdescriptor remove ( object key ) {
super ( maxkeysize ) ;
try {
import java . util . iterator ;
import java . util . properties ;
import java . io . ioexception ;
import java . util . concurrent . scheduledexecutorservice ;
import java . io . inputstream ;
import java . util . map ;
threadpoolmanager . dispose ( ) ;
import java . util . arraylist ;
import java . util . linkedhashset ;
import javax . management . mbeanserver ;
import java . util . concurrent . concurrenthashmap ;
import java . util . set ;
import javax . management . objectname ;
import java . security . accesscontrolexception ;
import java . lang . management . managementfactory ;
import java . util . concurrent . executors ;
}
( ( irequirescheduler ) auxfac ) . setscheduledexecutorservice ( compositecachemanager . getscheduledexecutorservice ( ) ) ;
{
import java . util . properties ;
import java . io . ioexception ;
import java . util . stringtokenizer ;
import java . util . arraylist ;
if ( auxfac instanceof irequirescheduler )
import java . util . enumeration ;
import java . util . list ;
import java . io . fileinputstream ;
if ( aux = = null )
cachetype cachetype = aux . getcachetype ( ) ;
import java . util . concurrent . atomic . atomicboolean ;
log . info ( "in dispose , [ " + this . cacheattr . getcachename ( ) + " ] skipping auxiliary [ " + aux . getcachename ( ) + " ] fromremote [ "
+ cacheelement . getelementattributes ( ) . getisremote ( ) ) ;
if ( aux ! = null & & ( aux . getcachetype ( ) = = cachetype . disk cache | | !localonly ) )
memcache . freeelements ( numtofree ) ;
}
log . debug ( "removing " + key + " from cachetype" + cachetype ) ;
return misscountnotfound . get ( ) ;
return hitcountram . get ( ) ;
private atomicinteger hitcountaux ;
return misscountexpired . get ( ) ;
log . debug ( cacheattr . getcachename ( ) + " - aux cache [ " + aux . getcachename ( ) + " ] hit" ) ;
if ( cacheattr . isusedisk ( )
this . alive = new atomicboolean ( true ) ;
misscountnotfound . incrementandget ( ) ;
hitcountram . incrementandget ( ) ;
synchronized ( this )
log . error ( "trouble updating memory cache . " , ex ) ;
log . error ( "failure removing all from aux" , ex ) ;
aux . update ( cacheelement ) ;
public void dispose ( boolean fromremote )
protected void removeall ( boolean localonly )
aux . dispose ( ) ;
protected boolean remove ( k key , boolean localonly )
case remote cache :
this . updatecount = new atomicinteger ( 0 ) ;
log . debug ( "updated remote store for " + cacheelement . getkey ( ) + cacheelement ) ;
private atomicinteger updatecount ;
removed = b ;
log . info ( "in dispose , [ " + this . cacheattr . getcachename ( ) + " ] put " + numtofree + " into auxiliary " + aux . getcachename ( ) ) ;
& & cacheelement . getelementattributes ( ) . getisspool ( ) )
for ( icache < k , v > aux : auxcaches )
public int getupdatecount ( )
{
log . debug ( "diskcache in aux list : cattr " + cacheattr . isusedisk ( ) ) ;
if ( aux . getcachetype ( ) = = cachetype . disk cache )
if ( alive . compareandset ( true , false ) = = false )
private void processretrievedelements ( auxiliarycache < k , v > aux , map < k , icacheelement < k , v > > elementsfromauxiliary )
return hitcountaux . get ( ) ;
hitcountaux . incrementandget ( ) ;
log . error ( e ) ;
log . debug ( cacheattr . getcachename ( ) + " - aux cache [ " + aux . getcachename ( ) + " ] hit , but element expired . " ) ;
return alive . get ( ) ? cachestatus . alive : cachestatus . disposed ;
log . error ( "failure disposing of memcache" , ex ) ;
log . debug ( "auxiliary cache type : " + aux . getcachetype ( ) ) ;
& & cacheattr . getdiskusagepattern ( ) = = diskusagepattern . update
case lateral cache :
private atomicinteger hitcountram ;
aux . removeall ( ) ;
this . misscountnotfound = new atomicinteger ( 0 ) ;
break ;
log . debug ( "ce . getelementattributes ( ) . getisremote ( ) = "
public cachestatus getstatus ( )
+ fromremote + " ] " ) ;
log . debug ( "diskcache available , but jcs is not configured to use the diskcache as a swap . " ) ;
if ( localonly & & ( cachetype = = cachetype . remote cache | | cachetype = = cachetype . lateral cache ) )
log . debug ( "updated lateral cache for " + cacheelement . getkey ( ) ) ;
if ( log . isinfoenabled ( ) )
elementeventq . dispose ( ) ;
log . debug ( "lateralcache in aux list : cattr " + cacheattr . isuselateral ( ) ) ;
log . error ( "failure in updateexclude" , ex ) ;
cacheelement . getelementattributes ( ) . setlastaccesstimenow ( ) ;
removecount . incrementandget ( ) ;
for ( auxiliarycache < k , v > aux : auxcaches )
if ( elementeventq ! = null )
log . error ( "failure removing from aux" , ex ) ;
try
case disk cache :
log . info ( "in dispose , [ " + this . cacheattr . getcachename ( ) + " ] auxiliary [ " + aux . getcachename ( ) + " ] " ) ;
log . debug ( "removed all keys from the memory cache . " ) ;
this . misscountexpired = new atomicinteger ( 0 ) ;
log . debug ( "removing all keys from cachetype" + aux . getcachetype ( ) ) ;
return updatecount . get ( ) ;
if ( cacheattr . isuselateral ( ) & & cacheelement . getelementattributes ( ) . getislateral ( ) & & !localonly )
private atomicinteger removecount ;
misscountexpired . incrementandget ( ) ;
if ( log . isdebugenabled ( ) )
import java . util . concurrent . atomic . atomicinteger ;
continue ;
this . removecount = new atomicinteger ( 0 ) ;
updatecount . incrementandget ( ) ;
removed = memcache . remove ( key ) ;
private atomicinteger misscountexpired ;
default :
memcache . dispose ( ) ;
catch ( ioexception ex )
catch ( ioexception e )
switch ( aux . getcachetype ( ) )
if ( aux = = null | | aux . getstatus ( ) ! = cachestatus . alive
private atomicinteger misscountnotfound ;
this . hitcountaux = new atomicinteger ( 0 ) ;
private atomicboolean alive ;
| | ( fromremote & & aux . getcachetype ( ) = = cachetype . remote cache ) )
processretrievedelements ( aux , elementsfromauxiliary ) ;
log . info ( "in dispose , [ " + this . cacheattr . getcachename ( ) + " ] disposing of memory cache . " ) ;
misscountnotfound . addandget ( keys . size ( ) - elements . size ( ) ) ;
log . debug ( "spooltodisk done for : " + ce . getkey ( ) + " on disk cache [ " + aux . getcachename ( ) + " ] " ) ;
memcache . removeall ( ) ;
this . hitcountram = new atomicinteger ( 0 ) ;
log . debug ( "updated disk cache for " + cacheelement . getkey ( ) ) ;
elementeventq = null ;
return ;
if ( !removed & & cachetype ! = cachetype . remote cache )
if ( cacheelement . getelementattributes ( ) . getisremote ( ) & & !localonly )
int numtofree = memcache . getsize ( ) ;
boolean b = aux . remove ( key ) ;
log . error ( "failure disposing of aux . " , ex ) ;
import java . util . concurrent . concurrentmap ;
compositecacheconfigurator configurator = new compositecacheconfigurator ( this ) ;
private final concurrentmap < string , auxiliarycache < ? , ? > > auxiliarycaches =
props . setproperty ( key , sysprops . getproperty ( key ) ) ;
}
{
cache = configurator . parseregion ( this . getconfigurationproperties ( ) , cattr . getcachename ( ) ,
private final concurrentmap < string , auxiliarycacheattributes > auxiliaryattributeregistry =
cache = ( compositecache < k , v > ) caches . get ( cattr . getcachename ( ) ) ;
import java . util . concurrent . locks . reentrantlock ;
finally
try
protected void parseregions ( properties props )
protected void parsesystemregions ( properties props )
cachelock . unlock ( ) ;
cattr . setcachename ( cattr . getcachename ( ) ) ;
caches . put ( cattr . getcachename ( ) , cache ) ;
cachelock . lock ( ) ;
private final concurrentmap < string , icache < ? , ? > > caches =
private final concurrentmap < string , auxiliarycachefactory > auxiliaryfactoryregistry =
if ( cache = = null )
this . defaultauxvalues , cattr ) ;
icache < ? , ? > cache ;
private reentrantlock cachelock = new reentrantlock ( ) ;
ccattr = ccattr2 . clone ( ) ;
auxattr = auxattr . clone ( ) ;
eattr = eattr2 . clone ( ) ;
protected icompositecacheattributes parsecompositecacheattributes ( properties props ,
return parseregion ( props , ccm , regname , auxiliaries , null , region prefix ) ;
( ( irequirescheduler ) auxcache ) . setscheduledexecutorservice (
public compositecacheconfigurator ( )
( ( irequirescheduler ) auxfac ) . setscheduledexecutorservice ( ccm . getscheduledexecutorservice ( ) ) ;
ccm . getscheduledexecutorservice ( ) ) ;
properties sysprops = system . getproperties ( ) ;
if ( ! ( auxiliaries . startswith ( " , " ) | | auxiliaries . equals ( "" ) ) )
log . debug ( "parsing region name '" + regname + "' , value '" + auxiliaries + "'" ) ;
props . setproperty ( key , sysprops . getproperty ( key ) ) ;
protected < k > ikeymatcher < k > parsekeymatcher ( properties props , string auxprefix )
cache . setscheduledexecutorservice ( ccm . getscheduledexecutorservice ( ) ) ;
properties props , compositecachemanager ccm , string regname , string auxiliaries )
cache . setelementeventqueue ( ccm . getelementeventqueue ( ) ) ;
protected static final string system region prefix = "jcs . system . " ;
if ( log . isinfoenabled ( ) )
protected static final string attribute prefix = " . attributes" ;
ccm . addauxiliarycache ( auxname , regname , auxcache ) ;
}
{
auxiliarycacheconfigurator . parseelementserializer ( props , auxprefix ) ;
ielementattributes defaulteattr , string regionprefix )
icompositecacheattributes cca )
log . info ( "using system property [ [ " + key + " ] [ " + sysprops . getproperty ( key ) + " ] ] " ) ;
if ( auxiliaries ! = null )
properties props , compositecachemanager ccm , string regname , string auxiliaries ,
auxiliarycacheattributes auxattr = ccm . registryattrget ( auxname ) ;
cache = parseregion ( props , ccm , regionname , auxiliaries ) ;
auxcache = auxfac . createcache ( auxattr , ccm , cacheeventlogger , elementserializer ) ;
ccm . addcache ( regionname , cache ) ;
if ( key . startswith ( system property key prefix ) )
ccm . registryfacput ( auxfac ) ;
stringtokenizer st = new stringtokenizer ( auxiliaries , " , " ) ;
? new compositecache < k , v > ( parsecompositecacheattributes ( props , regname ,
protected static void overridewithsystemproperties ( properties props )
ccm . getdefaultelementattributes ( ) , regionprefix ) ;
string regname , icompositecacheattributes defaultccattr , string regionprefix )
string auxiliaries = optionconverter . findandsubst ( key , props ) ;
protected static final string auxiliary prefix = "jcs . auxiliary . " ;
icacheeventlogger cacheeventlogger =
auxiliarycache < k , v > auxcache = ( auxiliarycache < k , v > ) ccm . getauxiliarycache ( auxname , regname ) ;
auxiliarycachefactory auxfac = ccm . registryfacget ( auxname ) ;
for ( string key : props . stringpropertynames ( ) )
protected void parseregions ( properties props , compositecachemanager ccm )
protected static final string region prefix = "jcs . region . " ;
string regname , icompositecacheattributes defaultccattr )
ccm . getdefaultcacheattributes ( ) , regionprefix ) , ea )
cache = parseregion ( props , ccm , regionname , auxiliaries , null , system region prefix ) ;
protected static final string system property key prefix = "jcs" ;
auxcache = parseauxiliary ( props , ccm , auxname , regname ) ;
ielementserializer elementserializer =
auxiliarycacheconfigurator . parsecacheeventlogger ( props , auxprefix ) ;
protected static final string element attribute prefix = " . elementattributes" ;
ielementattributes ea = parseelementattributes ( props , regname ,
return parseregion ( props , ccm , regname , auxiliaries , cca , region prefix ) ;
protected void parsesystemregions ( properties props , compositecachemanager ccm )
protected static final string cache attribute prefix = " . cacheattributes" ;
string auxname , string regname )
protected < k , v > auxiliarycache < k , v > parseauxiliary ( properties props , compositecachemanager ccm ,
for ( string key : sysprops . stringpropertynames ( ) )
return parsecompositecacheattributes ( props , regname , defaultccattr , region prefix ) ;
ccattr = defaultccattr ;
protected ielementattributes parseelementattributes ( properties props , string regname ,
ccm . registryattrput ( auxattr ) ;
eattr = defaulteattr ;
properties props = loadpropertiesfromfile ( ) ;
private string propsfilename = default props file name ;
log . error ( "problem loading props . " , e ) ;
log . warn ( "the local address [ " + registryhost
log . info ( stats ) ;
throws servletexception , ioexception
remotecacheserverfactory . shutdownimpl ( registryhost , registryport ) ;
import org . apache . commons . jcs . auxiliary . remote . remoteutils ;
log . debug ( "registryhost = [ " + registryhost + " ] " ) ;
private void setregistryport ( string ports )
loadinitparams ( ) ;
catch ( unknownhostexception e )
remotecacheserverfactory . startup ( registryhost , registryport , propsfilename ) ;
if ( null ! = regportstring )
private static final log log = logfactory . getlog ( remotecachestartupservlet . class ) ;
if ( log . isinfoenabled ( ) )
log . error ( "problem starting remote cache server . " , e ) ;
protected void service ( httpservletrequest request , httpservletresponse response )
if ( "localhost" . equals ( registryhost ) | | "127 . 0 . 0 . 1" . equals ( registryhost ) )
return props ;
log . error ( "problem writing response . " , e ) ;
this . registryport = integer . parseint ( ports ) ;
}
string regportstring = config . getinitparameter ( "registryport" ) ;
{
registryhost = hostnameutil . getlocalhostaddress ( ) ;
private string registryhost = null ;
catch ( numberformatexception e )
os . write ( stats . getbytes ( characterencoding ) ) ;
this . registryhost = registryhost ;
import java . io . outputstream ;
catch ( ioexception e )
registryhost = props . getproperty ( "registry . host" , registryhost ) ;
else
private void loadinitparams ( )
import org . apache . commons . jcs . engine . control . compositecachemanager ;
import org . apache . commons . logging . log ;
if ( null ! = registryhost )
setregistryport ( ports ) ;
properties props = null ;
try
import org . apache . commons . jcs . access . exception . cacheexception ;
import org . apache . commons . jcs . utils . net . hostnameutil ;
log . error ( "could not get local address to use for the registry!" , e ) ;
this . propsfilename = propsfilename ;
import java . io . ioexception ;
log . info ( "shutting down remote cache " ) ;
private properties loadpropertiesfromfile ( )
string propsfilename = config . getinitparameter ( "propsfilename" ) ;
remotecacheserverfactory . startup ( registryhost , registryport , props , propsfilename ) ;
import org . apache . commons . logging . logfactory ;
this . registryport = default registry port ;
setregistryport ( regportstring ) ;
+ " ] is invalid . other machines must be able to use the address to reach this server . " ) ;
props = remoteutils . loadprops ( propsfilename ) ;
log . error ( "problem converting port to an int . " , e ) ;
throws servletexception
if ( props ! = null )
import java . util . properties ;
string ports = props . getproperty ( "registry . port" , string . valueof ( registryport ) ) ;
import javax . servlet . servletconfig ;
log . info ( "remote jcs server started with properties from " + propsfilename ) ;
private static final string default props file name = " / cache . ccf" ;
private int registryport = default registry port ;
log . error ( "problem shutting down . " , e ) ;
import java . net . unknownhostexception ;
if ( null ! = propsfilename )
extends httpservlet
servletconfig config = getservletconfig ( ) ;
string registryhost = config . getinitparameter ( "registryhost" ) ;
if ( registryhost = = null )
if ( props = = null )
if ( log . isdebugenabled ( ) )
private remotecachemonitor monitor ;
( ( remotecache < k , v > ) rcnw . getremotecache ( ) ) . setfacade ( this ) ;
private lock managerlock ;
this . nowaits . add ( rcnw ) ;
remotecachemanager ins = getmanager ( cattr ) ;
managers . put ( cattr . getremotelocation ( ) , ins ) ;
public remotecachemanager getmanager ( iremotecacheattributes cattr )
private concurrentmap < remotelocation , remotecachemanager > managers ;
remotecachenowait < k , v > rcnw = ( remotecachenowait < k , v > ) nw ;
ins = managers . get ( cattr . getremotelocation ( ) ) ;
public remotecachemanager getmanager ( iremotecacheattributes cattr , icompositecachemanager cachemgr ,
new remotecachenowaitfacade < k , v > ( nowaits , rca , cachemgr , cacheeventlogger , elementserializer , this ) ;
}
{
( ( remotecache < k , v > ) rcnw . getremotecache ( ) ) . setfacade ( this ) ;
this . nowaits . add ( rcnw ) ;
for ( remotecachenowait < k , v > rcnw : this . nowaits )
import java . util . concurrent . executorservice ;
element . getelementattributes ( ) . setmaxlife ( duration . gettimeunit ( ) . tomillis ( duration . getdurationamount ( ) ) ) ;
oldelt ! = null ? oldelt . getelementattributes ( ) : delegate . getelementattributes ( ) . clone ( ) ) ;
delegate . update ( updateelement ( key , elt . getval ( ) , expiryforaccess , elt . getelementattributes ( ) ) ) ;
import java . util . concurrent . concurrentmap ;
import org . apache . commons . jcs . jcache . jmx . jmxs ;
element . setelementattributes ( attrs ) ;
import java . io . closeable ;
import org . apache . commons . jcs . jcache . jmx . jcscachemxbean ;
if ( elementattributes . class . isinstance ( copy ) ) {
import java . util . concurrent . executors ;
import java . util . hashmap ;
elementattributes . class . cast ( clone ) . setcreatetime ( ) ;
}
import java . util . iterator ;
final icacheelement < k , v > element = updateelement (
import org . apache . commons . jcs . jcache . jmx . jcscachestatisticsmxbean ;
import org . apache . commons . jcs . engine . behavior . ielementattributes ;
{
import java . util . hashset ;
if ( elementattributes . class . isinstance ( clone ) )
else
import java . util . set ;
final ielementattributes clone = delegate . getelementattributes ( ) . clone ( ) ;
expirypolicy . getexpiryforaccess ( ) ;
if ( oldvalue . equals ( v ) )
else if ( v ! = null )
return true ;
final icacheelement < k , v > element = updateelement ( key , v , duration , clone ) ;
import org . apache . commons . jcs . engine . control . compositecache ;
import org . apache . commons . jcs . engine . elementattributes ;
import java . io . ioexception ;
import java . util . arrays ;
import static org . apache . commons . jcs . jcache . serialization . serializations . copy ;
import org . apache . commons . jcs . engine . behavior . icacheelement ;
elementattributes . class . cast ( copy ) . setcreatetime ( ) ;
import org . apache . commons . jcs . jcache . proxy . exceptionwrapperhandler ;
import static org . apache . commons . jcs . jcache . asserts . assertnotnull ;
import java . util . properties ;
import org . apache . commons . jcs . engine . behavior . ielementserializer ;
jcskey , value , created ? null : duration ,
import java . util . map ;
import java . util . concurrent . concurrenthashmap ;
import org . apache . commons . jcs . jcache . thread . daemonthreadfactory ;
element . getelementattributes ( ) . setidletime ( duration . gettimeunit ( ) . tomillis ( duration . getdurationamount ( ) ) ) ;
attrs . setlastaccesstimenow ( ) ;
if ( duration = = expirypolicy . getexpiryforaccess ( ) )
import org . apache . commons . jcs . engine . cacheelement ;
attrs . setiseternal ( eternal ) ;
import org . apache . commons . jcs . utils . serialization . standardserializer ;
private icacheelement < k , v > updateelement ( final k key , final v v , final duration duration , final ielementattributes attrs )
remove ( key ) ;
attrs . settimefactorformilliseconds ( 1 ) ;
initialize ( listener , listenerid , cachename , maxfailure , waitbeforeretry ) ;
log . debug ( "event entering queue for " + getcachename ( ) + " : " + event ) ;
if ( !isalive ( ) )
import java . util . concurrent . linkedblockingqueue ;
elems . add ( new statelement < integer > ( "size" , integer . valueof ( this . size ( ) ) ) ) ;
processorthread = new qprocessor ( ) ;
log . info ( "destroy was called after queue was destroyed . doing nothing . stats = " + getstatistics ( ) ) ;
if ( log . isinfoenabled ( ) )
protected void stopprocessing ( )
super ( "cacheeventqueue . qprocessor - " + getcachename ( ) ) ;
log . info ( "cache event queue destroyed : " + this ) ;
}
processorthread . start ( ) ;
elems . add ( new statelement < boolean > ( "working" , boolean . valueof ( this . isworking ( ) ) ) ) ;
{
qprocessor ( )
else
import java . util . concurrent . timeunit ;
return queue . isempty ( ) ;
try
log . info ( "destroying queue , stats = " + getstatistics ( ) ) ;
if ( event ! = null & & isworking ( ) & & isalive ( ) )
while ( isalive ( ) )
stopprocessing ( ) ;
catch ( interruptedexception e )
private linkedblockingqueue < abstractcacheevent > queue = new linkedblockingqueue < abstractcacheevent > ( ) ;
setalive ( true ) ;
log . info ( "cache event queue created : " + this ) ;
processorthread = null ;
processorthread . interrupt ( ) ;
setalive ( false ) ;
protected class qprocessor
return queue . size ( ) ;
if ( isworking ( ) )
if ( processorthread ! = null )
abstractcacheevent event = null ;
import java . util . arraylist ;
queue . offer ( event ) ;
event = queue . poll ( getwaittodiemillis ( ) , timeunit . milliseconds ) ;
if ( isalive ( ) )
log . debug ( "qprocessor exiting for " + getcachename ( ) ) ;
for ( remotecachenowait < k , v > rcnw : new arraylist < remotecachenowait < k , v > > ( this . nowaits ) )
import java . util . arraylist ;
for ( remotecachenowait < k , v > rcnw : this . nowaits )
return ioutils . tobytearray ( in , file . length ( ) ) ;
return sb . tostring ( ) ;
}
default :
case ' \ n' :
private string readline ( randomaccessfile reader ) throws ioexception {
if ( seencr ) {
stringbuffer sb = new stringbuffer ( ) ;
return pos ;
line = readline ( reader ) ;
seencr = false ;
int ch ;
switch ( ch ) {
boolean seencr = false ;
pos = reader . getfilepointer ( ) ;
while ( ( ch = reader . read ( ) ) ! = - 1 ) {
seencr = true ;
string line = readline ( reader ) ;
return null ;
sb . append ( ( char ) ch ) ;
break ;
long pos = reader . getfilepointer ( ) ;
reader . seek ( pos ) ;
import java . math . biginteger ;
public static final long one eb = one kb * one pb ;
public static final long one tb = one kb * one gb ;
public static final biginteger one zb = biginteger . valueof ( one kb ) . multiply ( biginteger . valueof ( one eb ) ) ;
public static final biginteger one yb = one zb . multiply ( biginteger . valueof ( one eb ) ) ;
public static final long one pb = one kb * one tb ;
}
throw e ;
super . close ( ) ;
this . branch . close ( ) ;
public teeoutputstream ( outputstream out , outputstream branch ) {
} catch ( ioexception e ) {
public void close ( ) throws ioexception {
try {
n - - ;
throw new nullpointerexception ( "byte array is null" ) ;
this . mark = this . cbuf . position ( ) ;
coderresult result = this . encoder . encode ( this . cbuf , this . bbuf , true ) ;
public int available ( ) throws ioexception {
throw new indexoutofboundsexception ( "array size = " + b . length +
for ( ; ; ) {
}
this . mark = - 1 ;
} else {
public charsequenceinputstream ( final charsequence s , final charset charset ) {
import java . io . ioexception ;
while ( n > 0 & & this . cbuf . hasremaining ( ) ) {
import java . nio . charset . codingerroraction ;
import java . nio . charset . coderresult ;
" , offset = " + off + " , length = " + len ) ;
import java . nio . bytebuffer ;
private final charbuffer cbuf ;
public int read ( byte [ ] b , int off , int len ) throws ioexception {
public void close ( ) throws ioexception {
public int read ( ) throws ioexception {
this . cbuf = charbuffer . wrap ( s ) ;
while ( len > 0 ) {
this . cbuf . get ( ) ;
return true ;
if ( this . bbuf . hasremaining ( ) ) {
public long skip ( long n ) throws ioexception {
int chunk = math . min ( this . bbuf . remaining ( ) , len ) ;
return this . bbuf . get ( ) & 0xff ;
public charsequenceinputstream ( final charsequence s , final string charset ) {
skipped + + ;
private final charsetencoder encoder ;
private final bytebuffer bbuf ;
if ( this . mark ! = - 1 ) {
if ( result . iserror ( ) ) {
break ;
this . bbuf . flip ( ) ;
super ( ) ;
if ( !this . bbuf . hasremaining ( ) & & !this . cbuf . hasremaining ( ) ) {
public charsequenceinputstream ( final charsequence s , final charset charset , int buffersize ) {
fillbuffer ( ) ;
int skipped = 0 ;
private int mark ;
return read ( b , 0 , b . length ) ;
len - = chunk ;
return bytesread = = 0 & & !this . cbuf . hasremaining ( ) ? - 1 : bytesread ;
int bytesread = 0 ;
this ( s , charset . forname ( charset ) , buffersize ) ;
if ( len < 0 | | ( off + len ) > b . length ) {
off + = chunk ;
return skipped ;
@ override
import java . io . inputstream ;
this . cbuf . position ( this . mark ) ;
package org . apache . commons . io . input ;
this . bbuf = bytebuffer . allocate ( 124 ) ;
public boolean marksupported ( ) {
public charsequenceinputstream ( final charsequence s , final string charset , int buffersize ) {
this ( s , charset , 2048 ) ;
this . bbuf . compact ( ) ;
import java . nio . charbuffer ;
. onunmappablecharacter ( codingerroraction . replace ) ;
public int read ( byte [ ] b ) throws ioexception {
this . bbuf . get ( b , off , chunk ) ;
import java . nio . charset . charsetencoder ;
bytesread + = chunk ;
this . encoder = charset . newencoder ( )
return - 1 ;
result . throwexception ( ) ;
public void mark ( int readlimit ) {
import java . nio . charset . charset ;
public class charsequenceinputstream extends inputstream {
. onmalformedinput ( codingerroraction . replace )
public void reset ( ) throws ioexception {
private void fillbuffer ( ) throws ioexception {
if ( b = = null ) {
return this . cbuf . remaining ( ) ;
. equals ( "application / xml - external - parsed - entity" ) | | mime
while ( len > 0 & & b > = 0 ) {
mime = ( i = = - 1 ? httpcontenttype : httpcontenttype . substring ( 0 ,
encoding = defaultencoding = = null ? utf 8 : defaultencoding ;
return byteordermark ! = null & & getbom ( ) . equals ( bom ) ;
return defaultencoding = = null ? us ascii : defaultencoding ;
. startswith ( "application / " ) & & mime . endswith ( " + xml" ) ) ;
mime . startswith ( "text / " ) & & mime . endswith ( " + xml" ) ) ;
. startswith ( "text / " ) & & mime . endswith ( " + xml" ) ) ;
encoding = defaultencoding = = null ? us ascii
| | mime . equals ( "text / xml - external - parsed - entity" ) | | mime
return byteordermark = = null ? null : byteordermark . getcharsetname ( ) ;
return getbom ( ) ! = null ;
while ( n > 0 & & readfirstbytes ( ) > = 0 ) {
markedatstart = firstbytes = = null ;
return secondcount < 0 ? firstcount > 0 ? firstcount : - 1 : firstcount + secondcount ;
return fbindex < fblength ? firstbytes [ fbindex + + ] : - 1 ;
encoding = encoding ! = null ? encoding . touppercase ( locale . us ) : null ;
return b > = 0 ? b : in . read ( ) ;
return defaultencoding = = null ? utf 8 : defaultencoding ;
encoding = encoding ! = null ? encoding . touppercase ( ) : null ;
this . defaultencoding = defaultencoding = = null ? staticdefaultencoding
encoding = m . find ( ) ? m . group ( 1 ) : null ;
mime . startswith ( "application / " ) & & mime . endswith ( " + xml" ) ) ;
public static final byteordermark utf 32be = new byteordermark ( "utf - 32be" , 0x00 , 0x00 , 0xfe , 0xff ) ;
public static final byteordermark utf 32le = new byteordermark ( "utf - 32le" , 0xfe , 0xff , 0x00 , 0x00 ) ;
final file [ ] files = directory . listfiles ( ) ;
public static biginteger sizeofasbiginteger ( file file ) {
biginteger size = biginteger . zero ;
return biginteger . zero ;
public static biginteger sizeofdirectoryasbiginteger ( file directory ) {
}
return size ;
if ( file . isdirectory ( ) ) {
size . add ( biginteger . valueof ( sizeof ( file ) ) ) ;
throw new illegalargumentexception ( message ) ;
} else {
return biginteger . valueof ( file . length ( ) ) ;
for ( final file file : files ) {
string message = file + " does not exist" ;
if ( !directory . exists ( ) ) {
} catch ( ioexception ioe ) {
checkdirectory ( directory ) ;
long size = 0 ;
return sizeofdirectoryasbiginteger ( file ) ;
if ( files = = null ) {
if ( !issymlink ( file ) ) {
throw new illegalargumentexception ( directory + " does not exist" ) ;
private static void checkdirectory ( file directory ) {
if ( !directory . isdirectory ( ) ) {
if ( !file . exists ( ) ) {
throw new illegalargumentexception ( directory + " is not a directory" ) ;
try {
private static final int default bufsize = 4096 ;
while ( run & & ( ( num = reader . read ( inbuf ) ) ! = - 1 ) ) {
return create ( file , listener , delay , end , default bufsize ) ;
reader . seek ( position ) ;
for ( int i = 0 ; i < num ; i + + ) {
}
private static final string raf mode = "r" ;
int num ;
this . inbuf = new byte [ bufsize ] ;
tailer tailer = new tailer ( file , listener , delay , end , bufsize ) ;
public static tailer create ( file file , tailerlistener listener , long delay , boolean end , int bufsize ) {
public tailer ( file file , tailerlistener listener , long delay , boolean end , int bufsize ) {
private byte inbuf [ ] ;
sb = new stringbuilder ( ) ;
return repos ;
reader . seek ( repos ) ;
switch ( ch ) {
pos = reader . getfilepointer ( ) ;
reader = new randomaccessfile ( file , raf mode ) ;
repos = pos + i + 1 ;
byte ch = inbuf [ i ] ;
sb . append ( ( char ) ch ) ;
public static tailer create ( file file , tailerlistener listener , long delay , boolean end ) {
this ( file , listener , delay , end , default bufsize ) ;
break ;
listener . handle ( sb . tostring ( ) ) ;
stringbuilder sb = new stringbuilder ( ) ;
long repos = pos ;
}
sb . setlength ( 0 ) ;
repos = pos + i + 1 ;
seencr = false ;
listener . handle ( sb . tostring ( ) ) ;
if ( seencr ) {
} else if ( newer ) {
boolean newer = fileutils . isfilenewer ( file , last ) ;
last = system . currenttimemillis ( ) ;
private static final int no mark = - 1 ;
if ( this . mark ! = no mark ) {
this . mark = no mark ;
bytearrayoutputstream linebuf = new bytearrayoutputstream ( 64 ) ;
import java . nio . charset . charset ;
listener . handle ( new string ( linebuf . tobytearray ( ) , cset ) ) ;
linebuf . write ( ch ) ;
import java . io . bytearrayoutputstream ;
ioutils . closequietly ( linebuf ) ;
linebuf . reset ( ) ;
charset cset = charset . defaultcharset ( ) ;
this . encoder . reset ( ) ;
this . bbuf . limit ( 0 ) ;
try ( inputstream in = new checkedinputstream ( new fileinputstream ( file ) , checksum ) ) {
try ( reader input1 = charsetname = = null
try ( inputstream input1 = new fileinputstream ( file1 ) ;
: new inputstreamreader ( new fileinputstream ( file1 ) , charsetname ) ;
? new inputstreamreader ( new fileinputstream ( file1 ) , charset . defaultcharset ( ) )
filechannel input = fis . getchannel ( ) ;
}
copytofile ( in , destination ) ;
outputstream out = openoutputstream ( destination ) ) {
try ( inputstream in = source ) {
try ( inputstream in = source ;
ioutils . copy ( in , out ) ;
: new inputstreamreader ( new fileinputstream ( file2 ) , charsetname ) ) {
try ( fileinputstream fis = new fileinputstream ( srcfile ) ;
openoutputstream ( file ) . close ( ) ;
catch ( final ioexception e ) {
try ( outputstream out = openoutputstream ( file , append ) ) {
inputstream input2 = new fileinputstream ( file2 ) ) {
reader input2 = charsetname = = null
in . close ( ) ;
ex . addsuppressed ( e ) ;
filechannel output = fos . getchannel ( ) ) {
? new inputstreamreader ( new fileinputstream ( file2 ) , charset . defaultcharset ( ) )
ioutils . writelines ( lines , lineending , out , encoding ) ;
} catch ( final ioexception | runtimeexception ex ) {
try ( inputstream in = openinputstream ( file ) ) {
final boolean append ) throws ioexception {
if ( in ! = null ) {
public static void writestringtofile ( final file file , final string data , final charset encoding ,
try ( outputstream out = new bufferedoutputstream ( openoutputstream ( file , append ) ) ) {
fileoutputstream fos = new fileoutputstream ( destfile ) ;
try {
super ( ) ;
import org . apache . commons . scxml2 . scxmlexpressionexception ;
object arrayobject = eval . eval ( ctx , array ) ;
this . actions . add ( action ) ;
currentindex + + ;
evaluator eval = scinstance . getevaluator ( ) ;
public void execute ( final eventdispatcher evtdispatcher ,
import java . lang . reflect . array ;
for ( object value : arraylist ) {
import org . apache . commons . scxml2 . evaluator ;
int currentindex = 0 ;
final errorreporter errrep , final scinstance scinstance ,
return elem foreach ;
public final list < action > getactions ( ) {
public foreach ( ) {
if ( action ! = null ) {
import java . util . list ;
return index ;
import org . apache . commons . scxml2 . triggerevent ;
private list < action > actions ;
}
import org . apache . commons . scxml2 . context ;
if ( index ! = null ) {
private string array ;
import org . apache . commons . logging . log ;
public final string getcontainerelementname ( ) {
package org . apache . commons . scxml2 . model ;
for ( action aa : actions ) {
import org . apache . commons . scxml2 . errorreporter ;
return actions ;
public string getitem ( ) {
for ( int currentindex = 0 , size = array . getlength ( arrayobject ) ; currentindex < size ; currentindex + + ) {
ctx . setlocal ( item , value ) ;
private string index ;
if ( arrayobject . getclass ( ) . isarray ( ) ) {
public string getindex ( ) {
this . actions = new arraylist < action > ( ) ;
finally {
ctx . setlocal ( getnamespaceskey ( ) , getnamespaces ( ) ) ;
ctx . setlocal ( item , array . get ( arrayobject , currentindex ) ) ;
this . index = index ;
final log applog , final collection < triggerevent > derivedevents )
public void setarray ( final string array ) {
public void setitem ( final string item ) {
throws modelexception , scxmlexpressionexception {
aa . execute ( evtdispatcher , errrep , scinstance , applog , derivedevents ) ;
import org . apache . commons . scxml2 . eventdispatcher ;
private static final long serialversionuid = 1l ;
else {
this . item = item ;
public final void addaction ( final action action ) {
for ( object value : ( iterable ) arrayobject ) {
public string getarray ( ) {
arraylist < object > arraylist = new arraylist < object > ( ) ;
@ override
arraylist . add ( value ) ;
ctx . setlocal ( getnamespaceskey ( ) , null ) ;
if ( arrayobject ! = null & & arrayobject instanceof iterable | | arrayobject . getclass ( ) . isarray ( ) ) {
import java . util . collection ;
this . array = array ;
ctx . setlocal ( index , currentindex ) ;
public class foreach extends action implements actionscontainer {
import java . util . arraylist ;
return array ;
return item ;
import org . apache . commons . scxml2 . scinstance ;
private string item ;
context ctx = scinstance . getcontext ( getparenttransitiontarget ( ) ) ;
public void setindex ( final string index ) {
try {
}
public list < simpletransition > gettransitlist ( ) {
public map < string , simpletransition > getdefaulthistorytransitionentrymap ( ) {
return defaulthistorytransitionentrymap ;
import java . util . map ;
private list < simpletransition > transitlist ;
this . defaulthistorytransitionentrymap = new hashmap < string , simpletransition > ( ) ;
this . transitlist = new arraylist < simpletransition > ( ) ;
private map < string , simpletransition > defaulthistorytransitionentrymap ;
import org . apache . commons . scxml2 . model . simpletransition ;
import java . util . hashmap ;
rootctx . setlocal ( event variable , eventvar ) ;
import org . apache . commons . scxml2 . invoke . invokerexception ;
rootctx . setlocal ( event data , eventdata ) ;
if ( invokerclass = = null ) {
externaleventqueue . clear ( ) ;
public scxml getstatemachine ( ) {
seteventdata ( event ) ;
public eventdispatcher geteventdispatcher ( ) {
return scinstance . getcurrentstatus ( ) ;
} catch ( invokerexception ie ) {
context rootcontext = scinstance . getrootcontext ( ) ;
return invokerclass . newinstance ( ) ;
map < string , object > payloadmap = new hashmap < string , object > ( ) ;
scxmlhelper . clonedatamodel ( dm , context , getevaluator ( ) , log ) ;
return executor . geteventdispatcher ( ) ;
}
public notificationregistry getnotificationregistry ( ) {
getcurrentstatus ( ) . getstates ( ) . clear ( ) ;
notificationregistry . addlistener ( observable , listener ) ;
@ suppresswarnings ( "unused deprecation" )
handleevent ( evt ) ;
public actionexecutioncontext getactionexecutioncontext ( ) {
semantics . enterstates ( exctx , step ) ;
public boolean haspendingevents ( ) {
} catch ( instantiationexception ie ) {
return invokeids ;
triggerevents ( ) ;
semantics . determineinitialstates ( exctx , step ) ;
string invokeid = invokeids . get ( invoke ) ;
semantics . executeglobalscript ( exctx , step ) ;
semantics . processinvokes ( exctx , evt ) ;
eventdata = evt . getpayload ( ) ;
event = exctx . nextinternalevent ( ) ;
removeinvoker ( invoke ) ;
scinstance instance = scinstance ;
import java . util . concurrent . concurrentlinkedqueue ;
return invokeid ;
internaleventqueue . add ( event ) ;
datamodel rootdm = statemachine . getdatamodel ( ) ;
return executor . getnotificationregistry ( ) ;
if ( getstatemachine ( ) = = null ) {
import java . util . map ;
import java . util . linkedlist ;
this . actionexecutioncontext = new actionexecutioncontext ( this ) ;
scinstance . getrootcontext ( ) . setlocal ( " all states" , getcurrentstatus ( ) . getallstates ( ) ) ;
for ( triggerevent evt : evts ) {
if ( !invokeids . isempty ( ) ) {
notificationregistry . removelistener ( observable , listener ) ;
throws invokerexception {
eventtype = eventvariable . type platform ;
throw new invokerexception ( "no invoker registered for type \ ""
try {
public void execute ( actionexecutioncontext exctx ) throws modelexception , scxmlexpressionexception {
private final queue < triggerevent > internaleventqueue = new linkedlist < triggerevent > ( ) ;
object eventdata = null ;
return invokers . get ( invokeids . get ( invoke ) ) ;
private log applog = logfactory . getlog ( scxmlexecutioncontext . class ) ;
public scinstance detachinstance ( ) {
public void triggerevents ( ) throws modelexception {
triggerevent event = evt ;
import java . util . hashmap ;
return evaluator ;
this . executor = executor ;
protected void handleevent ( final triggerevent evt )
return notificationregistry ;
return actionexecutioncontext ;
this . notificationregistry = notifregistry ;
import org . apache . commons . scxml2 . actionexecutioncontext ;
this . notificationregistry = new notificationregistry ( ) ;
this . exctx = new scxmlexecutioncontext ( this ) ;
public invoker getinvoker ( final invoke invoke ) {
public void removeinvoker ( final invoke invoke ) {
import org . apache . commons . scxml2 . model . scxml ;
semantics . enumeratereachabletransitions ( exctx , step ) ;
public class scxmlexecutor {
triggerevent event = exctx . nextinternalevent ( ) ;
if ( evt ! = null ) {
private final map < string , class < ? extends invoker > > invokerclasses ;
scinstance . setstatemachine ( semantics . normalizestatemachine ( statemachine , errorreporter ) ) ;
exctx . reset ( ) ;
void reset ( ) {
internaleventqueue . clear ( ) ;
handleevent ( event ) ;
import org . apache . commons . scxml2 . invoke . invoker ;
getcurrentstatus ( ) . getstates ( ) . addall ( step . getafterstatus ( ) . getstates ( ) ) ;
string eventtype = eventvariable . type external ;
package org . apache . commons . scxml2 ;
while ( ( evt = externaleventqueue . poll ( ) ) ! = null ) {
semantics . executeactions ( exctx , step ) ;
public class scxmlexecutioncontext {
import java . util . arraylist ;
return executor . getevaluator ( ) ;
public void cancelinvoker ( invoke invoke ) {
rootctx . setlocal ( event data map , payloadmap ) ;
eventvariable eventvar = null ;
private final map < string , invoker > invokers = new hashmap < string , invoker > ( ) ;
class < ? extends invoker > invokerclass = invokerclasses . get ( type ) ;
if ( invokeid ! = null ) {
triggerevent nextinternalevent ( ) {
triggerevent evt ;
if ( scxmlhelper . isstringempty ( invokeid ) ) {
return running ;
private final map < invoke , string > invokeids = new hashmap < invoke , string > ( ) ;
void setnotificationregistry ( final notificationregistry notifregistry ) {
} catch ( illegalaccessexception iae ) {
public void stoprunning ( ) {
scinstance = null ;
scinstance . setexecutor ( this ) ;
public log getapplog ( ) {
private void seteventdata ( final triggerevent evt ) {
import org . apache . commons . logging . log ;
private notificationregistry notificationregistry ;
return internaleventqueue . poll ( ) ;
cancelinvoker ( invoke ) ;
@ suppresswarnings ( "unused" )
invoker newinvoker ( final string type )
if ( evts ! = null ) {
public errorreporter geterrorreporter ( ) {
this . running = false ;
this . invokerclasses = new hashmap < string , class < ? extends invoker > > ( ) ;
import org . apache . commons . scxml2 . model . invoke ;
return scinstance . getstatemachine ( ) ;
return instance ;
import java . util . queue ;
private scxmlexecutioncontext exctx ;
invokers . put ( invokeid , invoker ) ;
string invokeid = invoke . getid ( ) ;
throw new invokerexception ( ie . getmessage ( ) , ie . getcause ( ) ) ;
semantics . filtertransitionsset ( exctx , step ) ;
for ( enterablestate es : getcurrentstatus ( ) . getstates ( ) ) {
throw new invokerexception ( iae . getmessage ( ) , iae . getcause ( ) ) ;
invokers . remove ( invokeids . remove ( invoke ) ) ;
addinternalevent ( te ) ;
private boolean running ;
+ type + " \ "" ) ;
+ " . invoke . cancel . failed" , triggerevent . error event ) ;
semantics . initiateinvokes ( this , exctx , step ) ;
public invoker newinvoker ( string type ) throws invokerexception {
exctx . geteventdispatcher ( ) . cancel ( sendid ) ;
semantics . followtransitions ( exctx , step ) ;
scxmlexecutioncontext ( scxmlexecutor executor ) {
private final actionexecutioncontext actionexecutioncontext ;
step = new step ( event , getcurrentstatus ( ) ) ;
scinstance = instance ;
public void addinternalevent ( triggerevent event ) {
scxml statemachine = getstatemachine ( ) ;
payloadmap . put ( evt . getname ( ) , evt . getpayload ( ) ) ;
import java . util . uuid ;
public boolean isrunning ( ) {
public void attachinstance ( scinstance instance ) {
public map < invoke , string > getinvokeids ( ) {
return executor . newinvoker ( type ) ;
externaleventqueue . add ( evt ) ;
private final scxmlexecutor executor ;
private evaluator evaluator ;
this . evaluator = expevaluator ;
semantics . updatehistorystates ( exctx , step ) ;
public string setinvoker ( final invoke invoke , final invoker invoker ) {
return !externaleventqueue . isempty ( ) ;
public scinstance getscinstance ( ) {
invokeids . put ( invoke , invokeid ) ;
this . evaluator = evaluator ;
public void triggerevents ( final triggerevent [ ] evts )
final int triggereventtype = evt . gettype ( ) ;
scxmlhelper . clonedatamodel ( rootdm , rootcontext , getevaluator ( ) , log ) ;
private final queue < triggerevent > externaleventqueue = new concurrentlinkedqueue < triggerevent > ( ) ;
for ( invoke invoke : new arraylist < invoke > ( invokeids . keyset ( ) ) ) {
if ( event ! = null ) {
eventvar = new eventvariable ( evt . getname ( ) , eventtype , null , null , null , null , eventdata ) ;
} while ( event ! = null ) ;
if ( triggereventtype = = triggerevent . error event | | triggereventtype = = triggerevent . change event ) {
running = true ;
return executor . getscinstance ( ) ;
public evaluator getevaluator ( ) {
import org . apache . commons . logging . logfactory ;
public void addevent ( final triggerevent evt ) {
return executor . getstatemachine ( ) ;
return executor . geterrorreporter ( ) ;
step step = new step ( null , getcurrentstatus ( ) ) ;
throws modelexception {
invokerclasses . put ( type , invokerclass ) ;
invokerclasses . remove ( type ) ;
triggerevent te = new triggerevent ( invokeid
invokeid = uuid . randomuuid ( ) . tostring ( ) ;
return applog ;
scinstance . setexecutor ( null ) ;
if ( scinstance ! = null ) {
invokers . get ( invokeid ) . cancel ( ) ;
evaluator eval = exctx . getevaluator ( ) ;
import org . apache . commons . scxml2 . invoke . invokerexception ;
if ( configuration . isstrict ( ) ) {
" : no src and no content defined" ) ;
invokermanager . registerinvoker ( this , invoker ) ;
param . setlocation ( readrequiredav ( reader , elem param , attr location ) ) ;
throw new modelexception ( "missing invokermanager instance in context under key : " + getinvokermanagerkey ( ) ) ;
if ( xmlns scxml . equals ( nsuri ) ) {
string name , nsuri ;
final paramscontainer parent )
public void setidlocation ( final string idlocation ) {
addnamelistdatatopayload ( exctx , payloaddatamap ) ;
import org . w3c . dom . node ;
raise . setparent ( executable ) ;
private static final string type scxml = "scxml" ;
int next = reader . next ( ) ;
import org . apache . commons . scxml2 . scxmlsystemcontext ;
import org . apache . commons . scxml2 . triggerevent ;
}
scxml externalscxml ;
map < string , object > payloaddatamap = new hashmap < string , object > ( ) ;
if ( !children | | parent ! = root ) {
content . setbody ( children . item ( 0 ) . getnodevalue ( ) ) ;
scxml . setdatamodelname ( readav ( reader , attr datamodel ) ) ;
import org . apache . commons . scxml2 . xpathbuiltin ;
while ( elementstoskip > 0 & & reader . hasnext ( ) ) {
return builtin . ismember ( ctx , state ) ;
import org . apache . commons . scxml2 . model . content ;
if ( send . getnamelist ( ) = = null & & send . getparams ( ) . isempty ( ) ) {
if ( attrvalue ! = null ) {
src = string . valueof ( contentvalue ) ;
if ( send . getcontent ( ) = = null ) {
script . setbody ( readbody ( reader ) ) ;
addparamstopayload ( exctx , payloaddatamap ) ;
public boolean in ( final string state ) {
} else {
class < ? > clazz ;
ctx . setlocal ( getnamespaceskey ( ) , getnamespaces ( ) ) ;
executable . addaction ( raise ) ;
private string typeexpr ;
pushnamespaces ( reader , configuration ) ;
reportconflictingattribute ( reader , configuration , elem param , attr location , attr expr ) ;
if ( children . getlength ( ) = = 1 & & children . item ( 0 ) . getnodetype ( ) = = node . text node ) {
document document = documentbuilderfactory . newinstance ( ) . newdocumentbuilder ( ) . newdocument ( ) ;
case xmlstreamconstants . start element :
string location = readav ( reader , attr location ) ;
if ( invokermanager = = null ) {
datanode . setattribute ( "id" , datum . getid ( ) ) ;
reporter . report ( sb . tostring ( ) , "commons scxml" , null , reader . getlocation ( ) ) ;
elementstoskip - - ;
catch ( parserconfigurationexception pce ) {
stringbuilder body = new stringbuilder ( ) ;
catch ( scxmlexpressionexception see ) {
assign . setattr ( attrvalue ) ;
element datanode = document . createelement ( "data" ) ;
public object data ( string expression ) throws scxmlexpressionexception {
public class invoke extends namelistholder implements pathresolverholder , contentcontainer {
if ( typevalue = = null & & typeexpr ! = null ) {
this . typeexpr = typeexpr ;
final contentcontainer contentcontainer )
try {
datum . setnode ( readnode ( reader , configuration , xmlns scxml , elem data , new string [ ] { "id" } ) ) ;
elementstoskip + + ;
public void setcontent ( final content content ) {
ctx . setlocal ( datum . getid ( ) , value ) ;
pathresolver pr = getpathresolver ( ) ;
exctx . geterrorreporter ( ) . onerror ( errorconstants . execution error , e . getmessage ( ) , this ) ;
public enterablestate getparententerablestate ( ) {
import org . apache . commons . scxml2 . env . simplecontext ;
content content = new content ( ) ;
@ suppresswarnings ( "unchecked" )
break loop ;
private static final string attr attr = "attr" ;
catch ( invokerexception e ) {
log . warn ( sb . tostring ( ) ) ;
new simplecontext ( systemcontext ) : evaluator . newcontext ( rootcontext ) ;
assign . setlocation ( readrequiredav ( reader , elem assign , attr location ) ) ;
string src = getsrc ( ) ;
import java . util . hashmap ;
invokermanager invokermanager = ( invokermanager ) ctx . getvars ( ) . get ( getinvokermanagerkey ( ) ) ;
import org . apache . commons . scxml2 . scxmlioprocessor ;
public content getcontent ( ) {
. append ( " = \ "" ) . append ( value ) . append ( " \ " at " ) . append ( reader . getlocation ( ) ) ;
root . settextcontent ( null ) ;
errorreporter . onerror ( errorconstants . execution error , pce . getmessage ( ) , datum ) ;
attr attr = document . createattributens ( xmlns default , attr1 ) ;
public string getidlocation ( ) {
else if ( contentvalue ! = null ) {
invokeid = parentstate . getid ( ) + " . " + ctx . get ( scxmlsystemcontext . sessionid key ) ;
return parent ;
if ( internalioprocessor ! = null ) {
public jsfunctions ( context ctx ) {
private enterablestate parent ;
content . setexpr ( readav ( reader , attr expr ) ) ;
root . settextcontent ( root . gettextcontent ( ) . trim ( ) ) ;
throw new modelexception ( "incompatible scxml document datamodel \ "" + statemachine . getdatamodelname ( ) + " \ ""
import org . apache . commons . scxml2 . actionexecutioncontext ;
exctx . geterrorreporter ( ) . onerror ( errorconstants . expression error , e . getmessage ( ) , this ) ;
ctx . setlocal ( datum . getid ( ) , datanode ) ;
exctx . getinternalioprocessor ( ) . addevent ( new triggerevent ( triggerevent . error execution , triggerevent . error event ) ) ;
readnamespaces ( configuration , raise ) ;
throw new modelexception ( sb . tostring ( ) ) ;
nodelist children = body . getchildnodes ( ) ;
if ( parent = = null ) {
else {
public string getinvokermanagerkey ( ) {
import org . apache . commons . scxml2 . builtin ;
rootcontext = evaluator . null data model . equals ( evaluator . getsupporteddatamodel ( ) )
attr . setvalue ( readav ( reader , attr1 ) ) ;
invoker . setevaluator ( exctx . getevaluator ( ) ) ;
if ( datamodel = = null | | evaluator . null data model . equals ( evaluator . getsupporteddatamodel ( ) ) ) {
contentcontainer . setcontent ( content ) ;
" : type expression \ "" + typeexpr + " \ " evaluated to null or empty string" ) ;
import org . apache . commons . scxml2 . invoke . invoker ;
if ( typevalue = = null ) {
attrvalue = readav ( reader , attr attr ) ;
private static string readbody ( final xmlstreamreader reader )
final string element , final string attr , final string value )
return content ;
private static final string invoker manager key = " invoker manager" ;
this . idlocation = idlocation ;
string expr = readav ( reader , attr expr ) ;
if ( reporter ! = null ) {
import javax . xml . parsers . documentbuilderfactory ;
public object location ( string expression ) throws scxmlexpressionexception {
break ;
object value ;
} else if ( elem content . equals ( name ) ) {
invoker . setparentioprocessor ( ioprocessor ) ;
return invoker manager key ;
schema schema ;
import org . w3c . dom . document ;
return xpathbuiltin . eval ( ctx , expression ) ;
assign . settype ( evaluator . assigntype . fromvalue ( attrvalue ) ) ;
else if ( location = = null ) {
import org . apache . commons . scxml2 . evaluator ;
nsuri = reader . getnamespaceuri ( ) ;
srcnode = ( ( node ) contentvalue ) . clonenode ( true ) ;
else if ( next = = xmlstreamconstants . end element ) {
context ctx = exctx . getcontext ( parentstate ) ;
package org . apache . commons . scxml2 . env . javascript ;
readcontent ( reader , configuration , send ) ;
throw new scxmlexpressionexception ( " < invoke > for state " + parentstate . getid ( ) +
node srcnode = null ;
import org . apache . commons . scxml2 . context ;
if ( invokeid = = null ) {
if ( body . haschildnodes ( ) ) {
document document ;
int elementstoskip = 1 ;
org . apache . commons . logging . log log = logfactory . getlog ( scxmlreader . class ) ;
return xpathbuiltin . evallocation ( ctx , expression ) ;
string attrvalue = readav ( reader , attr type ) ;
object contentvalue ;
return typeexpr ;
if ( ctx . has ( datum . getid ( ) ) ) {
boolean children = false ;
children = true ;
private string idlocation ;
return idlocation ;
reportignoredelement ( reader , configuration , elem send , nsuri , name ) ;
readparam ( reader , configuration , send ) ;
if ( getid ( ) = = null & & getidlocation ( ) ! = null ) {
if ( evaluator . xpath data model . equals ( evaluator . getsupporteddatamodel ( ) ) ) {
method method ;
invoker . setinvokeid ( invokeid ) ;
this . content = content ;
if ( content . getexpr ( ) ! = null ) {
private content content ;
if ( src = = null & & srcnode = = null ) {
typevalue = ( string ) gettextcontentifnoderesult ( eval . eval ( ctx , typeexpr ) ) ;
errorreporter . onerror ( errorconstants . expression error , see . getmessage ( ) , datum ) ;
contentvalue = eval . eval ( ctx , content . getexpr ( ) ) ;
sb . append ( "ignoring unknown or invalid < " ) . append ( element ) . append ( " > attribute " ) . append ( attr )
private static void skiptoendelement ( final xmlstreamreader reader ) throws xmlstreamexception {
contentvalue = content . getbody ( ) ;
scxmlioprocessor ioprocessor = ( ( map < string , scxmlioprocessor > ) ctx . get ( scxmlsystemcontext . ioprocessors key ) ) .
@ override
parent . addaction ( raise ) ;
private static void reportignoredattribute ( final xmlstreamreader reader , final configuration configuration ,
public class jsfunctions {
content . setbody ( body ) ;
case xmlstreamconstants . end element :
ctx . setlocal ( getnamespaceskey ( ) , null ) ;
throws xmlstreamexception , modelexception {
if ( statemachine . getdatamodelname ( ) ! = null & & !statemachine . getdatamodelname ( ) . equals ( evaluator . getsupporteddatamodel ( ) ) ) {
readcontent ( reader , configuration , invoke ) ;
import org . apache . commons . scxml2 . semantics . errorconstants ;
if ( src = = null & & getcontent ( ) ! = null ) {
stringbuilder sb = new stringbuilder ( ) ;
if ( evaluator . assigntype . add attribute . equals ( assign . gettype ( ) ) ) {
import org . w3c . dom . element ;
reportignoredattribute ( reader , configuration , elem assign , attr type , attrvalue ) ;
this . parent = parent ;
import javax . xml . parsers . parserconfigurationexception ;
src = getpathresolver ( ) . resolvepath ( src ) ;
parent . getparams ( ) . add ( param ) ;
if ( parent ! = null ) {
import org . apache . commons . scxml2 . scxmlexpressionexception ;
log . warn ( "ignoring xml content in < script > element , encountered element with local name : "
if ( assign . gettype ( ) = = null ) {
continue ;
xmlreporter reporter = configuration . reporter ;
public void setparententerablestate ( final enterablestate parent ) {
skiptoendelement ( reader ) ;
string typevalue = type ;
loop : while ( reader . hasnext ( ) ) {
param . setlocation ( location ) ;
if ( elem param . equals ( name ) ) {
import org . apache . commons . scxml2 . model . paramscontainer ;
if ( location ! = null ) {
default :
if ( src ! = null ) {
param . setexpr ( expr ) ;
invoker invoker = invokermanager . newinvoker ( typevalue ) ;
string invokeid = getid ( ) ;
popnamespaces ( reader , configuration ) ;
globalscript . setbody ( readbody ( reader ) ) ;
raise . setevent ( readav ( reader , attr event ) ) ;
eval . evalassign ( ctx , idlocation , invokeid , evaluator . assigntype . replace children , null ) ;
src = ( string ) gettextcontentifnoderesult ( eval . eval ( ctx , getsrcexpr ( ) ) ) ;
finally {
if ( contentvalue instanceof node ) {
public string gettypeexpr ( ) {
import org . apache . commons . scxml2 . invokermanager ;
for ( final string attr1 : attrs ) {
if ( !children & & root . haschildnodes ( ) ) {
enterablestate parentstate = getparententerablestate ( ) ;
node body = readnode ( reader , configuration , xmlns scxml , elem content , new string [ ] { } ) ;
get ( scxmlioprocessor . scxml event processor ) ;
reportignoredattribute ( reader , configuration , elem assign , attr attr , attrvalue ) ;
evaluator . evalassign ( ctx , "$" + datum . getid ( ) , value , evaluator . assigntype . replace children , null ) ;
if ( pr ! = null ) {
internalioprocessor . addevent ( new triggerevent ( triggerevent . error execution , triggerevent . error event ) ) ;
throw new illegalargumentexception ( "parent parameter cannot be null" ) ;
object actionobject ;
return ;
import org . apache . commons . scxml2 . model . contentcontainer ;
systemcontext = new scxmlsystemcontext ( internalcontext ) ;
if ( src = = null & & getsrcexpr ( ) ! = null ) {
this . ctx = ctx ;
context internalcontext = evaluator . null data model . equals ( evaluator . getsupporteddatamodel ( ) ) ?
invoker . invoke ( src , payloaddatamap ) ;
? new simplecontext ( ) : evaluator . newcontext ( null ) ;
if ( next = = xmlstreamconstants . start element ) {
if ( expr ! = null ) {
switch ( reader . next ( ) ) {
catch ( scxmlexpressionexception e ) {
private context ctx ;
name = reader . getlocalname ( ) ;
raise raise = new raise ( ) ;
public void settypeexpr ( final string typeexpr ) {
if ( !configuration . issilent ( ) & & log . iswarnenabled ( ) ) {
public void execute ( final actionexecutioncontext exctx ) throws modelexception {
this . internalioprocessor = null ;
typevalue = type scxml ;
if ( bindings . containskey ( nashorn global ) ) {
return context . has ( nashorn global ) ;
private static final string nashorn global = "nashorn . global" ;
private bindings getglobalbindings ( ) {
for ( string key : getglobalbindings ( ) . keyset ( ) ) {
return bindings . get ( key ) ;
return context . getvars ( ) . remove ( key ) ;
return ( bindings ) bindings . get ( nashorn global ) ;
return false ;
for ( string key : bindings . keyset ( ) ) {
getglobalbindings ( ) . remove ( key ) ;
set . put ( key , getglobalbindings ( ) . get ( key ) ) ;
if ( context . has ( key . tostring ( ) ) ) {
if ( hasglobalbindings ( ) & & getglobalbindings ( ) . containsvalue ( value ) ) {
private boolean hasglobalbindings ( ) {
if ( context . has ( nashorn global ) ) {
}
return ( bindings ) context . get ( nashorn global ) ;
return getglobalbindings ( ) . put ( name , value ) ;
this . bindings = bindings ;
return true ;
return bindings . remove ( key ) ;
return getglobalbindings ( ) . get ( key ) ;
public jsbindings ( context context , bindings bindings ) {
this . context = context ;
set . put ( key , bindings . get ( key ) ) ;
if ( hasglobalbindings ( ) & & getglobalbindings ( ) . containskey ( key ) ) {
keys . addall ( getglobalbindings ( ) . keyset ( ) ) ;
bindings . clear ( ) ;
bindings . putall ( list ) ;
if ( hasglobalbindings ( ) & & !getglobalbindings ( ) . isempty ( ) ) {
if ( bindings . containsvalue ( value ) ) {
return null ;
if ( bindings . containskey ( key ) ) {
if ( hasglobalbindings ( ) ) {
if ( !bindings . isempty ( ) ) {
} else if ( hasglobalbindings ( ) & & getglobalbindings ( ) . containskey ( name ) ) {
}
private object getsrcdata ( ) {
data = getsrcdata ( ) ;
ctx . setlocal ( datum . getid ( ) , evaluator . clonedata ( datum . getvalue ( ) ) ) ;
return contentparser . default parser . parseresource ( resolvedsrc ) ;
else {
if ( datum . getexpr ( ) ! = null ) {
import org . apache . commons . scxml2 . io . contentparser ;
private final map < history , set < enterablestate > > histories = new hashmap < > ( ) ;
private final map < enterablestate , context > contexts = new hashmap < > ( ) ;
ctx . setlocal ( datum . getid ( ) , value ) ;
payload = payloaddatamap ;
payload = eval . clonedata ( eval . eval ( ctx , content . getexpr ( ) ) ) ;
eventvalue = ( string ) eval . eval ( ctx , eventexpr ) ;
payload = eval . clonedata ( content . getbody ( ) ) ;
map < string , object > payloaddatamap = new linkedhashmap < > ( ) ;
new simplecontext ( ) : evaluator . newcontext ( rootcontext ) ;
object delayvalue = eval . eval ( ctx , delayexpr ) ;
targetvalue = ( string ) eval . eval ( ctx , targetexpr ) ;
typevalue = ( string ) eval . eval ( ctx , typeexpr ) ;
histories . put ( history , new hashset < > ( lc ) ) ;
private static string initglobalsscript ;
( ( jsbindings ) scriptcontext . getbindings ( scriptcontext . global scope ) ) . setcontext ( jscontext ) ;
scriptcontext = new simplescriptcontext ( ) ;
copyjavascriptglobalstoscxmlcontext ( scriptcontext . getbindings ( scriptcontext . engine scope ) , effectivecontext ) ;
protected scriptengine getengine ( ) {
return engine ;
protected scriptcontext getscriptcontext ( jscontext jscontext ) throws scriptexception {
private static scriptengine engine ;
if ( scriptcontext = = null ) {
object ret = getengine ( ) . eval ( expression , scriptcontext ) ;
}
catch ( ioexception ioe ) {
jscontext effectivecontext = geteffectivecontext ( ( jscontext ) context ) ;
import javax . script . simplescriptcontext ;
import javax . script . scriptexception ;
throw new runtimeexception ( "failed to load init global . js from classpath" , ioe ) ;
for ( string key : global . keyset ( ) ) {
engine = new scriptenginemanager ( ) . getenginebyname ( "javascript" ) ;
scriptcontext . getbindings ( scriptcontext . engine scope ) . put ( scxml system context , jscontext . getsystemcontext ( ) . getvars ( ) ) ;
initglobalsscript = ioutils . tostring ( jsevaluator . class . getresourceasstream ( "init global . js" ) , "utf - 8" ) ;
import java . io . ioexception ;
initengine ( ) ;
private transient scriptcontext scriptcontext ;
jscontext . set ( key , global . get ( key ) ) ;
private void copyjavascriptglobalstoscxmlcontext ( final bindings global , final jscontext jscontext ) {
scriptcontext scriptcontext = getscriptcontext ( effectivecontext ) ;
else {
import org . apache . commons . io . ioutils ;
return scriptcontext ;
protected synchronized static void initengine ( ) {
if ( global ! = null ) {
getengine ( ) . eval ( initglobalsscript , scriptcontext ) ;
private static final string scxml system context = " scxmlsystemcontext" ;
if ( engine = = null ) {
if ( !scxml system context . equals ( key ) ) {
scriptcontext . setbindings ( getengine ( ) . createbindings ( ) , scriptcontext . engine scope ) ;
scriptcontext . setbindings ( new jsbindings ( jscontext ) , scriptcontext . global scope ) ;
try {
public class assign extends action {
data = getsrcdata ( exctx . getstatemachine ( ) . getpathresolver ( ) ) ;
private object getsrcdata ( final pathresolver pathresolver ) {
result = content . getvalue ( ) ;
import org . apache . commons . scxml2 . scxmlexpressionexception ;
evaluator eval = exctx . getevaluator ( ) ;
result = null ;
return result ;
exctx . geterrorreporter ( ) . onerror ( errorconstants . expression error ,
} else if ( content . getvalue ( ) ! = null ) {
public void setdonedata ( final donedata donedata ) {
import org . apache . commons . scxml2 . evaluator ;
result = payloaddatamap ;
import java . util . linkedhashmap ;
exctx . getinternalioprocessor ( ) . addevent ( new eventbuilder ( triggerevent . error execution , triggerevent . error event ) . build ( ) ) ;
import org . apache . commons . scxml2 . triggerevent ;
}
payloadbuilder . addparamstopayload ( exctx . getscinstance ( ) . getglobalcontext ( ) ,
} catch ( scxmlexpressionexception e ) {
import org . apache . commons . scxml2 . context ;
return donedata ;
if ( !payloaddatamap . isempty ( ) ) {
import org . apache . commons . scxml2 . eventbuilder ;
} else {
public object processdonedata ( scxmlexecutioncontext exctx ) throws modelexception {
public donedata getdonedata ( ) {
this . donedata = donedata ;
else if ( content . getbody ( ) ! = null ) {
context ctx = exctx . getscinstance ( ) . getglobalcontext ( ) ;
map < string , object > payloaddatamap = new linkedhashmap < > ( ) ;
result = eval . clonedata ( content . getbody ( ) ) ;
if ( content ! = null ) {
object result = null ;
if ( content . getexpr ( ) ! = null ) {
import org . apache . commons . scxml2 . scxmlexecutioncontext ;
private donedata donedata ;
result = eval . clonedata ( eval . eval ( ctx , content . getexpr ( ) ) ) ;
import java . util . map ;
if ( donedata ! = null ) {
content content = donedata . getcontent ( ) ;
import org . apache . commons . scxml2 . semantics . errorconstants ;
exctx . getevaluator ( ) , donedata . getparams ( ) , payloaddatamap ) ;
"failed to process final donedata due to error : " + e . getmessage ( ) , getparent ( ) ) ;
try {
}
value = httpmatcher . group ( parse http idn scheme )
private static final int parse http idn scheme = 1 ;
+ domainvalidator . unicodetoascii ( httpmatcher . group ( parse http idn auth ) )
private static final int parse http idn rest = 3 ;
return false ;
matcher httpmatcher = http idn pattern . matcher ( value ) ;
private static final int parse http idn auth = 2 ;
private static final pattern http idn pattern = pattern . compile ( " ( https ? : / / ) ( [ ^ / ] + ) ( . * ) " , pattern . case insensitive ) ;
if ( httpmatcher . lookingat ( ) ) {
+ httpmatcher . group ( parse http idn rest ) ;
} else {
if ( !ascii pattern . matcher ( value ) . matches ( ) ) {
import java . net . idn ;
return idn . toascii ( input ) ;
private final list < string > dependencylist = collections . synchronizedlist ( new arraylist < string > ( ) ) ;
@ suppresswarnings ( "unchecked" )
msg msg = i . next ( ) ;
map < string , arg > argmap = this . args [ i ] ;
string varkey = i . next ( ) ;
for ( iterator < ? > i = hvars . keyset ( ) . iterator ( ) ; i . hasnext ( ) ; ) {
string validatorname = entry . getkey ( ) ;
entry < string , arg > entry = iter . next ( ) ;
map < string , arg > argmap = this . args [ arg . getposition ( ) ] ;
validatoraction action = actions . get ( depend ) ;
argmap . put ( validatorname , ( arg ) arg . clone ( ) ) ;
for ( iterator < msg > i = hmsgs . values ( ) . iterator ( ) ; i . hasnext ( ) ; ) {
string replacevalue = entry . getvalue ( ) ;
iterator < entry < string , arg > > iter = argmap . entryset ( ) . iterator ( ) ;
iterator < string > dependencies = this . dependencylist . iterator ( ) ;
argmap = new hashmap < string , arg > ( ) ;
map < string , arg > argmap = new hashmap < string , arg > ( this . args [ i ] ) ;
public list < string > getdependencylist ( ) {
iterator < string > i = this . hvars . keyset ( ) . iterator ( ) ;
iterator < arg > iter = argmap . values ( ) . iterator ( ) ;
arg arg = iter . next ( ) ;
string depend = iter . next ( ) ;
string key = entry . getkey ( ) ;
entry < string , string > entry = i . next ( ) ;
void process ( map < string , string > globalconstants , map < string , string > constants ) {
arg arg = entry . getvalue ( ) ;
list < string > dependentvalidators = va . getdependencylist ( ) ;
map < string , arg > [ ] newargs = new map [ arg . getposition ( ) + 1 ] ;
for ( iterator < entry < string , string > > i = constants . entryset ( ) . iterator ( ) ; i . hasnext ( ) ; ) {
protected map < string , arg > [ ] args = new map [ 0 ] ;
map < string , validatoraction > actions ,
for ( iterator < entry < string , string > > i = globalconstants . entryset ( ) . iterator ( ) ; i . hasnext ( ) ; ) {
string depend = dependencies . next ( ) ;
iterator < string > iter = dependentvalidators . iterator ( ) ;
super ( ) ;
this . allowtld = false ;
return email validator with tld ;
return domainvalidator . isvalid ( domain ) | | domainvalidator . isvalidtld ( domain ) ;
this . allowtld = allowtld ;
}
public static emailvalidator getinstance ( boolean allowlocal , boolean allowtld ) {
return getinstance ( allowlocal , false ) ;
this . allowlocal = allowlocal ;
} else {
if ( allowtld ) {
private static final emailvalidator email validator = new emailvalidator ( false , false ) ;
public static emailvalidator getinstance ( boolean allowlocal ) {
return email validator ;
protected emailvalidator ( boolean allowlocal , boolean allowtld ) {
return email validator with local ;
private static final emailvalidator email validator with local = new emailvalidator ( true , false ) ;
private final boolean allowtld ;
return domainvalidator . isvalid ( domain ) ;
return email validator with local with tld ;
private static final emailvalidator email validator with tld = new emailvalidator ( false , true ) ;
private static final emailvalidator email validator with local with tld = new emailvalidator ( true , true ) ;
string hostlocation = authoritymatcher . group ( parse authority host ip ) ;
return false ;
}
private static final string authority chars regex = " \ \ p { alnum } \ \ - \ \ . " ;
matcher matchurl ( string value ) {
private static final int parse authority port = 3 ;
} else {
private static final int parse authority host ip = 2 ;
domainvalidator domainvalidator = domainvalidator . getinstance ( ison ( allow local urls ) ) ;
private static final string ipv6 regex = " [ 0 - 9a - fa - f : ] + " ;
string ipv6 = authoritymatcher . group ( parse authority ipv6 ) ;
if ( !inetaddressvalidator . isvalidinet6address ( ipv6 ) ) {
private static final int parse authority extra = 4 ;
return url pattern . matcher ( value ) ;
private static final int parse authority ipv6 = 1 ;
if ( !inetaddressvalidator . isvalidinet4address ( hostlocation ) ) {
" ^ ( ? : \ \ [ ( " + ipv6 regex + " ) \ \ ] | ( [ " + authority chars regex + " ] * ) ) ( : \ \ d * ) ? ( . * ) ? " ;
if ( !domainvalidator . isvalid ( hostlocation ) ) {
if ( ipv6 ! = null ) {
inetaddressvalidator inetaddressvalidator = inetaddressvalidator . getinstance ( ) ;
}
if ( authority . contains ( " : " ) ) {
return false ;
" ( ? : \ \ [ ( " + ipv6 regex + " ) \ \ ] | ( ? : ( ? : " + userinfo field regex + " ) ? ( [ " + authority chars regex + " ] * ) ) ) ( : \ \ d * ) ? ( . * ) ? " ;
if ( !"" . equals ( authority ) ) {
} else {
if ( "file" . equals ( scheme ) ) {
private static final int parse authority host ip = 2 ;
private static final int ipv6 max hex groups = 8 ;
private static final int base 16 = 16 ;
octetint = integer . valueof ( octet , base 16 ) . intvalue ( ) ;
if ( octets . length > ipv6 max hex groups ) {
private static final int ipv6 max hex digits per group = 4 ;
if ( octetint < 0 | | octetint > max unsigned short ) {
if ( octet . length ( ) > ipv6 max hex digits per group ) {
if ( validoctets < ipv6 max hex groups & & !containscompressedzeroes ) {
if ( iipsegment > ipv4 max octet value ) {
private static final int ipv4 max octet value = 255 ;
private static final int max unsigned short = 0xffff ;
}
} catch ( numberformatexception nfe ) {
string port = authoritymatcher . group ( parse authority port ) ;
if ( port ! = null & & port . length ( ) > 0 ) {
long iport = integer . parseint ( port ) ;
if ( iport < 0 | | iport > 0xffff ) {
return false ;
private static final int parse authority port = 3 ;
try {
url url = enumresources . nextelement ( ) ;
final arraylist < string > schemes = new arraylist < string > ( ) ;
final arraylist < string > schemas = new arraylist < string > ( ) ;
return schemas . toarray ( new string [ schemas . size ( ) ] ) ;
final class < ? > clazz = findclassloader ( ) . loadclass ( classname ) ;
return classes . toarray ( new string [ classes . size ( ) ] ) ;
return schemes . toarray ( new string [ schemes . size ( ) ] ) ;
@ suppresswarnings ( "unused" )
final arraylist < string > classes = new arraylist < string > ( ) ;
enumeration < url > enumresources ;
private final map < filename , filemonitoragent > monitormap = new hashmap < filename , filemonitoragent > ( ) ;
private final stack < fileobject > deletestack = new stack < fileobject > ( ) ;
map < filename , object > newchildrenmap = new hashmap < filename , object > ( ) ;
this . removefile ( this . deletestack . pop ( ) ) ;
this . monitormap . get ( parent . getname ( ) ) ;
this . children = new hashmap < filename , object > ( ) ;
fileobject child = missingchildren . pop ( ) ;
private map < filename , object > children ;
this . addfile ( this . addstack . pop ( ) ) ;
private final stack < fileobject > addstack = new stack < fileobject > ( ) ;
stack < fileobject > missingchildren = new stack < fileobject > ( ) ;
agent = this . monitormap . get ( filename ) ;
final stringbuilder buffer = new stringbuilder ( ) ;
protected abstract void appendrooturi ( stringbuilder buffer , boolean addpassword ) ;
public abstractfilename ( final string scheme , final string abspath , filetype type )
new concurrenthashmap < filesystem , concurrentmap < filename , fileobject > > ( 10 ) ;
import java . util . concurrent . concurrentmap ;
protected concurrentmap < filename , fileobject > getorcreatefilesystemcache ( filesystem filesystem )
import java . util . concurrent . concurrenthashmap ;
filesystemcache . putifabsent ( filesystem , new concurrenthashmap < filename , fileobject > ( ) ) ;
concurrentmap < filename , fileobject > files = filesystemcache . get ( filesystem ) ;
private final concurrentmap < filesystem , concurrentmap < filename , fileobject > > filesystemcache =
files = filesystemcache . get ( filesystem ) ;
{
import java . util . concurrent . atomic . atomicinteger ;
count = openstreams . get ( ) ;
if ( count < 1 )
notifyallstreamsclosed ( ) ;
do
int count ;
openstreams . incrementandget ( ) ;
if ( count = = 1 )
return ;
private atomicinteger openstreams = new atomicinteger ( 0 ) ;
} while ( openstreams . compareandset ( count , count - 1 ) ) ;
return openstreams . get ( ) > 0 ;
usecount . decrementandget ( ) ;
return usecount . get ( ) < 1 ;
usecount . incrementandget ( ) ;
private atomiclong usecount = new atomiclong ( 0 ) ;
import java . util . concurrent . atomic . atomiclong ;
filesystemandnamekey key = new filesystemandnamekey ( file . getfilesystem ( ) , file . getname ( ) ) ;
import java . util . concurrent . concurrentmap ;
log . debug ( "putfile : " + file . getname ( ) ) ;
if ( thread ! = null )
return false ;
lock . unlock ( ) ;
files . remove ( key . getfilename ( ) ) ;
} while ( filesystemcache . putifabsent ( filesystem , files ) = = null ) ;
thread = softrefreleasethread . get ( ) ;
reference < fileobject > old = files . put ( file . getname ( ) , ref ) ;
} while ( softrefreleasethread . compareandset ( null , newthread ) ) ;
reference < fileobject > ref = createreference ( file , refqueue ) ;
refreversemap . remove ( old ) ;
}
softrefreleasethread thread = softrefreleasethread . getandset ( null ) ;
if ( old ! = null )
if ( files . containskey ( file . getname ( ) ) & & files . get ( file . getname ( ) ) . get ( ) ! = null )
{
if ( files ! = null )
if ( key . getfilesystem ( ) = = filesystem )
import java . util . concurrent . locks . reentrantlock ;
return files ;
finally
if ( key ! = null )
thread thread ;
do
try
refreversemap . put ( ref , key ) ;
return true ;
new concurrenthashmap < filesystem , map < filename , reference < fileobject > > > ( ) ;
softrefreleasethread newthread ;
iterator < filesystemandnamekey > iterkeys = refreversemap . values ( ) . iterator ( ) ;
filesystemandnamekey key = iterkeys . next ( ) ;
if ( removefile ( key ) )
private atomicreference < softrefreleasethread > softrefreleasethread = new atomicreference < softrefreleasethread > ( ) ;
endthread ( ) ;
refreversemap . remove ( ref ) ;
thread . requestend = true ;
private lock lock = new reentrantlock ( ) ;
filesystemandnamekey key = refreversemap . get ( ref ) ;
lock . lock ( ) ;
if ( newthread ! = null )
iterkeys . remove ( ) ;
while ( iterkeys . hasnext ( ) )
import java . util . concurrent . locks . lock ;
thread . interrupt ( ) ;
filesystemclose ( key . getfilesystem ( ) ) ;
if ( filesystemcache . size ( ) < 1 )
files = new hashmap < filename , reference < fileobject > > ( ) ;
public boolean putfileifabsent ( final fileobject file )
filesystemclose ( filesystem ) ;
startthread ( ) ;
import java . util . concurrent . atomic . atomicreference ;
newthread = null ;
filesystemcache . remove ( filesystem ) ;
import java . util . concurrent . concurrenthashmap ;
map < filename , reference < fileobject > > files ;
newthread . start ( ) ;
break ;
newthread = new softrefreleasethread ( ) ;
private final concurrentmap < filesystem , map < filename , reference < fileobject > > > filesystemcache =
map < filename , reference < fileobject > > files = getorcreatefilesystemcache ( file . getfilesystem ( ) ) ;
files = filesystemcache . get ( filesystem ) ;
if ( log . isdebugenabled ( ) )
if ( this = = o )
appendrooturi ( buffer , usepassword ) ;
return false ;
return getkey ( ) . compareto ( name . getkey ( ) ) ;
abstractfilename that = ( abstractfilename ) o ;
}
public int hashcode ( )
{
key = createuri ( true , true ) ;
public string getfriendlyuri ( )
return key ;
private string createuri ( boolean useabsolutepath , boolean usepassword )
return true ;
return createuri ( false , false ) ;
private string getkey ( )
if ( key = = null )
if ( o = = null | | getclass ( ) ! = o . getclass ( ) )
return createuri ( false , true ) ;
private string key = null ;
buffer . append ( useabsolutepath ? abspath : getpath ( ) ) ;
return ( getkey ( ) . equals ( that . getkey ( ) ) ) ;
return getkey ( ) . hashcode ( ) ;
public boolean equals ( object o )
& & basefile . getname ( ) . isfile ( ) )
if ( base ! = null & & vfs . isuristyle ( ) & & base . isfile ( ) )
new atomicreference < softrefreleasethread > ( ) ;
private final atomicreference < softrefreleasethread > softrefreleasethread =
if ( iterfilenames % getchecksperrun ( ) = = 0 )
children [ i ] = fs . getfilesystemmanager ( ) . resolvename ( name , file , namescope . child ) ;
return fs . getfilesystemmanager ( ) . getfilecontentinfofactory ( ) ;
if ( !fs . hascapability ( capability . list children ) )
if ( !fs . hascapability ( capability . random access read ) )
return fs . resolvefile ( fs . getfilesystemmanager ( ) . resolvename ( this . name , name , scope ) ) ;
return fs = = newfile . getfilesystem ( ) ;
if ( bappend & & !fs . hascapability ( capability . append content ) )
& & fs . hascapability ( capability . get last modified ) )
if ( !fs . hascapability ( capability . random access write ) )
final filename othername = fs . getfilesystemmanager ( ) . resolvename ( name , path ) ;
private static final string known hosts = prefix + " . known hosts" ;
private static final sftpfilesystemconfigbuilder builder = new sftpfilesystemconfigbuilder ( ) ;
public static final proxytype proxy socks5 = new proxytype ( "socks" ) ;
}
return ( file ) this . getparam ( opts , known hosts ) ;
public file getknownhosts ( final filesystemoptions opts )
public userinfo getuserinfo ( final filesystemoptions opts )
public string getproxyhost ( final filesystemoptions opts )
public file [ ] getidentities ( final filesystemoptions opts )
this . setparam ( opts , proxy port , integer . valueof ( proxyport ) ) ;
public void setuserinfo ( final filesystemoptions opts , final userinfo info )
if ( this . proxytype ! = null ? !this . proxytype . equals ( ptype . proxytype ) : ptype . proxytype ! = null )
this . setparam ( opts , identities , identityfiles ) ;
final proxytype ptype = ( proxytype ) obj ;
private static final string timeout = prefix + " . timeout" ;
public void setcompression ( final filesystemoptions opts , final string compression ) throws filesystemexception
private sftpfilesystemconfigbuilder ( )
this . setparam ( opts , known hosts , sshdir ) ;
return this . proxytype . hashcode ( ) ;
return sftpfilesystem . class ;
return this . proxytype . compareto ( ptype . proxytype ) ;
this . setparam ( opts , compression , compression ) ;
if ( this = = obj )
private static final string compression = prefix + "compression" ;
{
public boolean getuserdirisroot ( final filesystemoptions opts )
return this . getstring ( opts , proxy host ) ;
public string getstricthostkeychecking ( final filesystemoptions opts )
throws filesystemexception
protected class < ? extends filesystem > getconfigclass ( )
this . setparam ( opts , preferred authentications , preferredauthentications ) ;
public void setidentities ( final filesystemoptions opts , final file . . . identityfiles ) throws filesystemexception
super ( "sftp . " ) ;
private static final string identities = prefix + " . identities" ;
return ( file [ ] ) this . getparam ( opts , identities ) ;
private static final string user dir is root = prefix + " . user dir is root" ;
public void setknownhosts ( final filesystemoptions opts , final file sshdir ) throws filesystemexception
return this . getboolean ( opts , user dir is root , boolean . true ) ;
return builder ;
throw new filesystemexception ( "vfs . provider . sftp / stricthostkeychecking - arg . error" , hostkeychecking ) ;
return ( string ) this . getparam ( opts , preferred authentications ) ;
public int compareto ( final proxytype ptype )
return this . getstring ( opts , strict host key checking , host key check no ) ;
this . setparam ( opts , proxy type , proxytype ) ;
| | ( !hostkeychecking . equals ( host key check ask ) & & !hostkeychecking . equals ( host key check no ) & & !hostkeychecking
public void setproxytype ( final filesystemoptions opts , final proxytype proxytype )
return this . getinteger ( opts , timeout ) ;
public int getproxyport ( final filesystemoptions opts )
public string getpreferredauthentications ( final filesystemoptions opts )
public string getcompression ( final filesystemoptions opts )
. equals ( host key check yes ) ) )
this . setparam ( opts , proxy host , proxyhost ) ;
private static final string prefix = sftpfilesystemconfigbuilder . class . getname ( ) ;
if ( obj = = null | | this . getclass ( ) ! = obj . getclass ( ) )
@ override
this . setparam ( opts , userinfo . class . getname ( ) , info ) ;
return ( userinfo ) this . getparam ( opts , userinfo . class . getname ( ) ) ;
private static final string proxy type = prefix + " . proxy type" ;
return ( proxytype ) this . getparam ( opts , proxy type ) ;
public void setpreferredauthentications ( final filesystemoptions opts , final string preferredauthentications )
return this . getinteger ( opts , proxy port , 0 ) ;
private static final string host key check no = "no" ;
this . setparam ( opts , user dir is root , userdirisroot ? boolean . true : boolean . false ) ;
private static final string strict host key checking = prefix + " . strict host key checking" ;
public boolean equals ( final object obj )
if ( hostkeychecking = = null
return this . getstring ( opts , compression ) ;
private static final string host key check yes = "yes" ;
public static sftpfilesystemconfigbuilder getinstance ( )
public proxytype getproxytype ( final filesystemoptions opts )
public integer gettimeout ( final filesystemoptions opts )
this . setparam ( opts , strict host key checking , hostkeychecking ) ;
public void setuserdirisroot ( final filesystemoptions opts , final boolean userdirisroot )
public void setstricthostkeychecking ( final filesystemoptions opts , final string hostkeychecking )
public void setproxyhost ( final filesystemoptions opts , final string proxyhost )
private static final string preferred authentications = prefix + " . preferred authentications" ;
public void setproxyport ( final filesystemoptions opts , final int proxyport )
this . setparam ( opts , timeout , timeout ) ;
private static final string proxy port = prefix + " . proxy port" ;
public void settimeout ( final filesystemoptions opts , final integer timeout )
private static final string host key check ask = "ask" ;
public static final proxytype proxy http = new proxytype ( "http" ) ;
private static final string proxy host = prefix + " . proxy host" ;
catch ( final exception exc )
userauthenticatorutils . tochar ( rootname . getpassword ( ) ) ) ,
return false ;
sftpfileprovider . authenticator types ) ;
if ( uid < 0 )
}
channel . disconnect ( ) ;
private int executecommand ( string command , stringbuilder output ) throws jschexception , ioexception
else
userauthenticatorutils . getdata ( authdata , userauthenticationdata . username ,
throw new filesystemexception ( "vfs . provider / set - readable . error" , name , exc ) ;
ensuresession ( ) ;
this . groupsids = groupsids ;
e ) ;
public int getuid ( ) throws jschexception , ioexception
return dosetexecutable ( readable , owneronly ) ;
userauthenticationdata authdata = null ;
import com . jcraft . jsch . * ;
while ( ( read = stream . read ( buffer , 0 , buffer . length ) ) > = 0 )
session session ;
public boolean setreadable ( boolean readable , boolean owneronly ) throws filesystemexception
if ( code ! = 0 )
public boolean setexecutable ( boolean readable , boolean owneronly ) throws filesystemexception
{
public int [ ] getgroupsids ( ) throws jschexception , ioexception
return dosetwritable ( readable , owneronly ) ;
this . session = session ;
authdata = userauthenticatorutils . authenticate ( getfilesystemoptions ( ) ,
return channel . getexitstatus ( ) ;
final inputstreamreader stream = new inputstreamreader ( channel . getinputstream ( ) ) ;
throw new filesystemexception ( "vfs . provider / set - executable . error" , name , exc ) ;
userauthenticatorutils . getdata ( authdata , userauthenticationdata . password ,
if ( this . session = = null | | !this . session . isconnected ( ) )
protected boolean dosetexecutable ( boolean writable , boolean owneronly ) throws exception
output . append ( buffer , 0 , read ) ;
protected boolean dosetwritable ( boolean writable , boolean owneronly ) throws exception
private void ensuresession ( ) throws filesystemexception
doclosecommunicationlink ( ) ;
catch ( final exception e )
getfilesystemoptions ( ) ) ;
channelexec channel = ( channelexec ) session . openchannel ( "exec" ) ;
rootname . gethostname ( ) ,
finally
try
if ( groupsids = = null )
groupsids [ i ] = integer . parseint ( groups [ i ] ) ;
for ( int i = 0 ; i < groups . length ; i + + )
getrootname ( ) ,
userauthenticatorutils . tochar ( rootname . getusername ( ) ) ) ,
import java . io . inputstreamreader ;
char [ ] buffer = new char [ 128 ] ;
channel . seterrstream ( system . err , true ) ;
stringbuilder output = new stringbuilder ( ) ;
channel . setcommand ( command ) ;
@ override
final string [ ] groups = output . tostring ( ) . trim ( ) . split ( " \ \ s + " ) ;
import java . io . stringwriter ;
rootname . getport ( ) ,
int code = executecommand ( "id - u" , output ) ;
int code = executecommand ( "id - g" , output ) ;
session = sftpclientfactory . createconnection (
if ( exists ( ) )
throw new jschexception ( "could not get the user id of the current user ( error code : " + code + " ) " ) ;
uid = integer . parseint ( output . tostring ( ) . trim ( ) ) ;
channel . setinputstream ( null ) ;
public boolean setwritable ( boolean readable , boolean owneronly ) throws filesystemexception
try { thread . sleep ( 100 ) ; } catch ( exception ee ) { }
return groupsids ;
throw new filesystemexception ( "vfs . provider / set - writeable . error" , name , exc ) ;
return dosetreadable ( readable , owneronly ) ;
int [ ] groupsids = new int [ groups . length ] ;
throw new jschexception ( "could not get the groups id of the current user ( error code : " + code + " ) " ) ;
protected boolean dosetreadable ( boolean readable , boolean owneronly ) throws exception
private int uid = - 1 ;
return uid ;
final genericfilename rootname = ( genericfilename ) getrootname ( ) ;
stream . close ( ) ;
int read ;
userauthenticatorutils . cleanup ( authdata ) ;
channel . connect ( ) ;
while ( !channel . isclosed ( ) ) {
private int [ ] groupsids ;
throw new filesystemexception ( "vfs . provider . sftp / connect . error" ,
jsch . addidentity ( privatekeyfile . getabsolutepath ( ) ) ;
session = jsch . getsession ( new string ( username ) , hostname , port ) ;
private static void setknownhosts ( jsch jsch , file sshdir , file knownhostsfile ) throws filesystemexception
string stricthostkeychecking = builder . getstricthostkeychecking ( filesystemoptions ) ;
if ( knownhostsfile ! = null )
jsch . setknownhosts ( knownhostsfile . getabsolutepath ( ) ) ;
catch ( final jschexception e )
if ( identities ! = null )
}
{
sshdir = findsshdir ( ) ;
addindentity ( jsch , privatekeyfile ) ;
if ( knownhostsfile . isfile ( ) & & knownhostsfile . canread ( ) )
private static void addidentities ( jsch jsch , file sshdir , file [ ] identities ) throws filesystemexception
setknownhosts ( jsch , sshdir , knownhostsfile ) ;
else
final file privatekeyfile = new file ( sshdir , "id rsa" ) ;
try
throw new filesystemexception ( "vfs . provider . sftp / known - hosts . error" , knownhostsfile . getabsolutepath ( ) , e ) ;
string preferredauthentications = builder . getpreferredauthentications ( filesystemoptions ) ;
if ( privatekeyfile . isfile ( ) & & privatekeyfile . canread ( ) )
throw new filesystemexception ( "vfs . provider . sftp / load - private - key . error" , privatekeyfile , e ) ;
private static void addindentity ( jsch jsch , final file privatekeyfile ) throws filesystemexception
catch ( jschexception e )
for ( final file privatekeyfile : identities )
addidentities ( jsch , sshdir , identities ) ;
knownhostsfile = new file ( sshdir , "known hosts" ) ;
import org . apache . commons . vfs2 . util . randomaccessmode ;
return filetype . file ;
return false ;
import org . apache . commons . vfs2 . provider . abstractfileobject ;
protected boolean doisreadable ( ) throws exception
import java . io . unsupportedencodingexception ;
return this . stat ! = null ;
final path filepath = new path ( path ) ;
}
fo [ i ] = this . fs . resolvefile ( p . touri ( ) . tostring ( ) ) ;
else
private final filesystem hdfs ;
import org . apache . hadoop . conf . configuration ;
private filestatus stat ;
for ( final filestatus status : files )
attrs . put ( hdfsfileattributes . group . tostring ( ) , this . stat . getgroup ( ) ) ;
attrs . put ( hdfsfileattributes . permissions . tostring ( ) , this . stat . getpermission ( ) . tostring ( ) ) ;
import java . io . ioexception ;
path = name . getpath ( ) ;
if ( null = = this . stat )
capabilities . addall ( hdfsfileprovider . capabilities ) ;
package org . apache . commons . vfs2 . provider . hdfs ;
path = urldecoder . decode ( name . getpath ( ) , "utf - 8" ) ;
return filetype . folder ;
public boolean canrenameto ( final fileobject newfile )
catch ( final filenotfoundexception fnfe )
import org . apache . commons . vfs2 . randomaccesscontent ;
synchronized ( this )
protected hdfsfileobject ( final abstractfilename name , final hdfsfilesystem fs , final filesystem hdfs , final path p )
import org . apache . hadoop . fs . filesystem ;
catch ( final ioexception e )
import java . util . map ;
super . close ( ) ;
protected fileobject createfile ( final abstractfilename name ) throws exception
protected boolean doishidden ( ) throws exception
protected boolean doissamefile ( final fileobject destfile ) throws filesystemexception
public boolean equals ( final object o )
if ( null = = stat )
if ( this . dogettype ( ) ! = filetype . folder )
this . hdfs = hdfs ;
catch ( final unsupportedencodingexception e )
fs = org . apache . hadoop . fs . filesystem . get ( conf ) ;
import java . util . hashmap ;
{
conf . set ( org . apache . hadoop . fs . filesystem . fs default name key , hdfsuri ) ;
protected void doremoveattribute ( final string attrname ) throws exception
this . path = p ;
final string [ ] children = new string [ files . length ] ;
if ( o instanceof hdfsfileobject )
protected boolean dosetlastmodifiedtime ( final long modtime ) throws exception
attrs . put ( hdfsfileattributes . length . tostring ( ) , this . stat . getlen ( ) ) ;
doattach ( ) ;
throw new filesystemexception ( "operation not supported" ) ;
return fo ;
return true ;
import org . apache . commons . vfs2 . fileobject ;
public boolean exists ( ) throws filesystemexception
private final path path ;
return this . stat . getmodificationtime ( ) ;
attrs . put ( hdfsfileattributes . last access time . tostring ( ) , this . stat . getaccesstime ( ) ) ;
super ( rootname , null , filesystemoptions ) ;
protected string [ ] dolistchildren ( ) throws exception
final configuration conf = new configuration ( true ) ;
import org . apache . hadoop . fs . path ;
public fileobject resolvefile ( final filename name ) throws filesystemexception
if ( mode . equals ( randomaccessmode . readwrite ) )
import org . apache . commons . vfs2 . provider . abstractfilename ;
log . error ( "error connecting to filesystem " + hdfsuri , e ) ;
return stat . getlen ( ) ;
throw new filesystemexception ( "error connecting to filesystem " + hdfsuri , e ) ;
return this . hdfs . open ( this . path ) ;
protected hdfsfilesystem ( final filename rootname , final filesystemoptions filesystemoptions )
return super . dogetattributes ( ) ;
throw new filenotfolderexception ( this ) ;
catch ( final exception e )
protected inputstream dogetinputstream ( ) throws exception
if ( o = = this )
protected void doattach ( ) throws exception
int i = 0 ;
return children ;
if ( null ! = fs )
import org . apache . commons . vfs2 . filesystemoptions ;
import org . apache . commons . vfs2 . filenotfolderexception ;
public int hashcode ( )
catch ( final filenotfoundexception e )
import org . apache . commons . logging . log ;
this . putfiletocache ( file ) ;
for ( int i = 0 ; i < children . length ; i + + )
attrs . put ( hdfsfileattributes . owner . tostring ( ) , this . stat . getowner ( ) ) ;
import org . apache . commons . vfs2 . capability ;
try
protected boolean doiswriteable ( ) throws exception
throw new filesystemexception ( "unable to check existance " , e ) ;
children [ i + + ] = status . getpath ( ) . getname ( ) ;
attrs . put ( hdfsfileattributes . modification time . tostring ( ) , this . stat . getmodificationtime ( ) ) ;
import java . io . filenotfoundexception ;
protected long dogetlastmodifiedtime ( ) throws exception
fs . close ( ) ;
fileobject file = this . getfilefromcache ( name ) ;
private filesystem fs ;
if ( null = = file )
@ override
return this . path . getname ( ) . tostring ( ) . hashcode ( ) ;
protected map < string , object > dogetattributes ( ) throws exception
import java . util . collection ;
import java . io . inputstream ;
return attrs ;
final string hdfsuri = name . getrooturi ( ) ;
return null ;
final path p = new path ( this . path , children [ i ] ) ;
final fileobject [ ] fo = new fileobject [ children . length ] ;
import org . apache . hadoop . fs . filestatus ;
final filestatus [ ] files = this . hdfs . liststatus ( this . path ) ;
protected void addcapabilities ( final collection < capability > capabilities )
catch ( final filenotfoundexception fne )
string path = null ;
public class hdfsfilesystem extends abstractfilesystem
protected long dogetcontentsize ( ) throws exception
if ( null = = o )
if ( other . path . equals ( this . path ) )
import org . apache . commons . vfs2 . filesystemexception ;
attrs . put ( hdfsfileattributes . block size . tostring ( ) , this . stat . getblocksize ( ) ) ;
return file ;
return new hdfsrandomaccesscontent ( this . path , this . hdfs ) ;
return filetype . imaginary ;
protected void dosetattribute ( final string attrname , final object value ) throws exception
import org . apache . commons . vfs2 . provider . abstractfilesystem ;
public class hdfsfileobject extends abstractfileobject < hdfsfilesystem >
if ( null = = this . fs )
final map < string , object > attrs = new hashmap < string , object > ( ) ;
final string [ ] children = dolistchildren ( ) ;
protected filetype dogettype ( ) throws exception
file = new hdfsfileobject ( ( abstractfilename ) name , this , fs , filepath ) ;
if ( null ! = this . stat )
import org . apache . commons . vfs2 . filename ;
this . fs = null ;
private final hdfsfilesystem fs ;
protected randomaccesscontent dogetrandomaccesscontent ( final randomaccessmode mode ) throws exception
public void close ( )
import org . apache . commons . vfs2 . filetype ;
return - 1 ;
import org . apache . commons . logging . logfactory ;
return ;
super ( name , fs ) ;
this . fs = fs ;
throw new unsupportedoperationexception ( ) ;
private static final log log = logfactory . getlog ( hdfsfilesystem . class ) ;
protected fileobject [ ] dolistchildrenresolved ( ) throws exception
this . stat = this . hdfs . getfilestatus ( this . path ) ;
import java . net . urldecoder ;
final hdfsfileobject other = ( hdfsfileobject ) o ;
throw new runtimeexception ( "error closing hdfs client" , e ) ;
if ( stat . isdir ( ) )
}
{
this . putfiletocache ( file ) ;
else
if ( usecache )
file = this . getfilefromcache ( name ) ;
fileobject file ;
return file ;
if ( getfilesystemmanager ( ) . getcachestrategy ( ) . equals ( cachestrategy . on resolve ) )
import org . apache . commons . vfs2 . cachestrategy ;
file . refresh ( ) ;
file = null ;
boolean usecache = ( null ! = getcontext ( ) . getfilesystemmanager ( ) . getfilescache ( ) ) ;
{
addindentity ( jsch , privatekeyfile , password ) ;
private static void addindentity ( final jsch jsch , final file privatekeyfile , char [ ] password ) throws filesystemexception
private static void addidentities ( final jsch jsch , final file sshdir , final file [ ] identities , char [ ] password ) throws filesystemexception
addidentities ( jsch , sshdir , identities , password ) ;
jsch . addidentity ( privatekeyfile . getabsolutepath ( ) , password = = null ? null : new string ( password ) ) ;
this . method = method ;
return filetype . file ;
final header header = method . getresponseheader ( "last - modified" ) ;
super ( method . getresponsebodyasstream ( ) ) ;
return filetype . imaginary ;
private final getmethod method ;
if ( header = = null )
}
protected string encodepath ( final string decodedpath ) throws uriexception
{
protected filetype dogettype ( ) throws exception
else
protected void onclose ( ) throws ioexception
public httpinputstream ( final getmethod method )
static class httpinputstream extends monitorinputstream
| | status = = httpurlconnection . http gone )
if ( status = = httpurlconnection . http ok )
method . setrequestheader ( "user - agent" , "jakarta - commons - vfs" ) ;
final string pathencoded = ( ( urlfilename ) getname ( ) ) . getpathqueryencoded ( this . geturlcharset ( ) ) ;
return uriutil . encodepath ( decodedpath ) ;
protected randomaccesscontent dogetrandomaccesscontent ( final randomaccessmode mode ) throws exception
protected long dogetlastmodifiedtime ( ) throws exception
else if ( status = = httpurlconnection . http not found
method . setfollowredirects ( this . getfollowredirect ( ) ) ;
method . setpath ( pathencoded ) ;
return urlcharset ;
throws ioexception
method . releaseconnection ( ) ;
throw new filesystemexception ( "vfs . provider . http / head . error" , getname ( ) , integer . valueof ( status ) ) ;
protected string [ ] dolistchildren ( ) throws exception
return new httprandomaccesscontent ( this , mode ) ;
protected string geturlcharset ( )
throw new filesystemexception ( "vfs . provider . http / last - modified . error" , getname ( ) ) ;
throw new exception ( "not implemented . " ) ;
@ override
protected void setupmethod ( final httpmethod method ) throws filesystemexception , uriexception
final int status = this . getheadmethod ( ) . getstatuscode ( ) ;
return dateutil . parsedate ( header . getvalue ( ) ) . gettime ( ) ;
final list < fileobject > selected )
throw new filesystemexception ( "vfs . provider / resync . error" , filename , e ) ;
return filename ;
final boolean depthwise ,
return false ;
setfiletype ( filetype . imaginary ) ;
public boolean isfile ( ) throws filesystemexception
dodetach ( ) ;
private void attach ( ) throws filesystemexception
else
public void findfiles ( final fileselector selector ,
info . setdepth ( 0 ) ;
fileobjectutils . getabstractfileobject ( destfile ) . handlecreate ( gettype ( ) ) ;
| | destfile . gettype ( ) . haschildren ( )
if ( !file . exists ( ) )
throw new filesystemexception ( "vfs . provider / create - file . error" , filename , e ) ;
protected boolean doisexecutable ( ) throws exception
fs . firefiledeleted ( this ) ;
parent = null ;
final fileobject parent = getparent ( ) ;
throw new filesystemexception ( "vfs . provider / set - writeable . error" , filename , exc ) ;
super . finalize ( ) ;
return fs . getfilesystemmanager ( ) . getfilecontentinfofactory ( ) ;
throw new filesystemexception ( "vfs . provider / rename - parent - read - only . error" ,
return fs . resolvefile ( child ) ;
return objects ;
list . toarray ( children ) ;
return new defaultfilecontent ( this , getfilecontentinfofactory ( ) ) ;
final stringbuilder buf = new stringbuilder ( ) ;
public filetype gettype ( ) throws filesystemexception
} ) ;
return true ;
final abstractfileobject file = fileobjectutils . getabstractfileobject ( files . get ( i ) ) ;
throw new illegalstateexception ( e ) ;
if ( file = = null )
if ( !exists ( ) | | selector = = null )
injecttype ( newtype ) ;
protected void endoutput ( ) throws exception
return fs ;
final string file = files [ i ] ;
return attached ;
handlecreate ( filetype . folder ) ;
if ( operations = = null )
return fs . resolvefile ( fs . getfilesystemmanager ( ) . resolvename ( this . filename , name , scope ) ) ;
throw new filesystemexception ( "vfs . provider / write - not - supported . error" ) ;
public fileobject [ ] getchildren ( ) throws filesystemexception
if ( newtype . equals ( filetype . imaginary ) )
throw new filesystemexception ( "vfs . provider / get - type . error" , exc , filename ) ;
catch ( final exception e )
final string relpath = file . getname ( ) . getrelativename ( srcfile . getname ( ) ) ;
final string scheme = uriparser . extractscheme ( filename . geturi ( ) , buf ) ;
fs . firefilecreated ( this ) ;
catch ( final filesystemexception exc )
if ( files = = null )
return getoutputstream ( false ) ;
return parent . iswriteable ( ) ;
selected . add ( index , file ) ;
return operations ;
public void createfile ( ) throws filesystemexception
if ( content = = null )
exc = e ;
throw new filesystemexception ( "vfs . provider / create - folder - not - supported . error" ) ;
return null ;
final fileobject file = fileinfo . getfile ( ) ;
public filesystem getfilesystem ( )
cache [ i ] = fs . getfilesystemmanager ( ) . resolvename ( filename , file , namescope . child ) ;
if ( attached )
if ( exc ! = null )
return names ;
protected void dosetattribute ( final string attrname , final object value ) throws exception
if ( exists ( ) )
throw new filesystemexception ( "vfs . provider / copy - file . error" , e , srcfile , destfile ) ;
final fileobject [ ] objects = new fileobject [ children . length ] ;
throw new filesystemexception ( "vfs . provider / copy - missing - file . error" , file ) ;
protected certificate [ ] dogetcertificates ( ) throws exception
protected void handlechanged ( ) throws exception
return accesscontroller . doprivileged ( new privilegedexceptionaction < url > ( )
& & destfile . getfilesystem ( ) . hascapability ( capability . set last modified file )
public boolean setreadable ( final boolean readable , final boolean owneronly ) throws filesystemexception
private boolean deleteself ( ) throws filesystemexception
attached = true ;
throw new filesystemexception ( "vfs . provider / list - children . error" , exc , filename ) ;
onchildrenchanged ( childname , newtype ) ;
if ( childname ! = null & & newtype ! = null )
throw new filesystemexception ( "vfs . provider / set - attribute - not - supported . error" ) ;
public url run ( ) throws malformedurlexception
public void close ( ) throws filesystemexception
catch ( final exception exc )
destfile . createfolder ( ) ;
fileobject [ ] childrenobjects ;
protected boolean doisreadable ( ) throws exception
protected filecontent docreatefilecontent ( ) throws filesystemexception
return listfiles ( selectors . select all ) . iterator ( ) ;
final fileobject [ ] children = getchildren ( ) ;
throw new filesystemexception ( "vfs . provider / get - url . error" , filename , e . getexception ( ) ) ;
final filename othername = fs . getfilesystemmanager ( ) . resolvename ( filename , path ) ;
if ( file . gettype ( ) . haschildren ( ) & & file . getchildren ( ) . length ! = 0 )
if ( childrenobjects ! = null )
throw new filesystemexception ( "vfs . provider / set - executable . error" , filename , exc ) ;
public void moveto ( final fileobject destfile ) throws filesystemexception
protected void finalize ( ) throws throwable
children = null ;
destfile . close ( ) ;
return filename . geturi ( ) ;
onchange ( ) ;
public boolean setwritable ( final boolean readable , final boolean owneronly ) throws filesystemexception
public outputstream getoutputstream ( final boolean bappend ) throws filesystemexception
return list ;
children = extractnames ( childrenobjects ) ;
parent . createfolder ( ) ;
protected boolean doishidden ( ) throws exception
final int count = files . size ( ) ;
if ( children ! = null )
if ( fs . getparentlayer ( ) = = null )
if ( destfile . exists ( ) & & destfile . gettype ( ) ! = srcfile . gettype ( ) )
final list < fileobject > selected ) throws filesystemexception
injecttype ( filetype . imaginary ) ;
if ( file . deleteself ( ) )
for ( final fileobject srcfile : files )
protected void onchange ( ) throws exception
return collections . emptymap ( ) ;
{
protected void injecttype ( final filetype filetype )
protected afs getabstractfilesystem ( )
final filename [ ] names = new filename [ objects . length ] ;
throw new filesystemexception ( "vfs . provider / write . error" , exc , filename ) ;
catch ( final runtimeexception re )
doattach ( ) ;
getname ( ) ,
throw new filenotfolderexception ( filename ) ;
return type ;
throw new filesystemexception ( "vfs . provider / create - folder - mismatched - type . error" , filename ) ;
final fileobject [ ] children = file . getchildren ( ) ;
throw re ;
exc = new filesystemexception ( "vfs . provider / close . error" , filename , e ) ;
dorename ( destfile ) ;
public int deleteall ( ) throws filesystemexception
if ( file . gettype ( ) . haschildren ( ) & & selector . traversedescendents ( fileinfo ) )
attach ( ) ;
return filetype . folder . equals ( this . gettype ( ) ) ;
getoutputstream ( ) . close ( ) ;
public fileobject resolvefile ( final string name , final namescope scope )
if ( gettype ( ) ! = filetype . imaginary )
if ( child . getbasename ( ) . equals ( name ) )
public boolean isexecutable ( ) throws filesystemexception
throw new filesystemexception ( "vfs . provider / remove - attribute - not - supported . error" ) ;
finally
protected abstract long dogetcontentsize ( ) throws exception ;
for ( int i = 0 ; i < files . length ; i + + )
children = new filename [ list . size ( ) ] ;
final list < fileobject > list = this . listfiles ( selector ) ;
public boolean ishidden ( ) throws filesystemexception
catch ( final privilegedactionexception e )
children = cache ;
return delete ( selectors . select self ) > 0 ;
final fileobject destfile = resolvefile ( relpath , namescope . descendent or self ) ;
protected filecontentinfofactory getfilecontentinfofactory ( )
public fileoperations getfileoperations ( ) throws filesystemexception
@ override
final filename [ ] cache = new filename [ files . length ] ;
parent = fs . resolvefile ( filename . getparent ( ) ) ;
return content . isopen ( ) ;
destfile . deleteall ( ) ;
continue ;
if ( canrenameto ( destfile ) )
return doissamefile ( destfile ) ;
if ( parent ! = null )
if ( depthwise )
public boolean isattached ( )
file . findfiles ( selector , false , files ) ;
if ( !exists ( ) )
final arraylist < fileobject > list = new arraylist < fileobject > ( ) ;
public int delete ( final fileselector selector ) throws filesystemexception
catch ( final filesystemexception e )
& & fs . hascapability ( capability . get last modified ) )
return dogetoutputstream ( bappend ) ;
throws exception
protected fileobject [ ] dolistchildrenresolved ( ) throws exception
public fileobject [ ] findfiles ( final fileselector selector ) throws filesystemexception
setfiletype ( filetype ) ;
throw new filesystemexception ( "vfs . provider / get - type . error" , e , filename ) ;
for ( final fileobject element : children )
public fileobject getchild ( final string name ) throws filesystemexception
}
filesystemexception exc = null ;
for ( int iterobjects = 0 ; iterobjects < objects . length ; iterobjects + + )
final arraylist < fileobject > files = new arraylist < fileobject > ( ) ;
private void detach ( ) throws exception
for ( final fileobject child : children )
return new url ( scheme , "" , - 1 ,
private void removechildrencache ( )
return gettype ( ) ! = filetype . imaginary ;
private fileobject resolvefile ( final filename child ) throws filesystemexception
public list < fileobject > listfiles ( final fileselector selector ) throws filesystemexception
selected . add ( file ) ;
protected void notifyallstreamsclosed ( )
public boolean canrenameto ( final fileobject newfile )
protected void handlecreate ( final filetype newtype ) throws exception
objects . add ( strongref ) ;
throw new filesystemexception ( "vfs . provider / set - readable . error" , filename , exc ) ;
for ( int i = 0 ; i < count ; i + + )
nuofdeleted + + ;
attached = false ;
throw new filesystemexception ( "vfs . provider / rename - not - supported . error" ) ;
protected boolean dosetwritable ( final boolean writable , final boolean owneronly ) throws exception
protected boolean doissamefile ( final fileobject destfile ) throws filesystemexception
public boolean isreadable ( ) throws filesystemexception
public iterator < fileobject > iterator ( )
handlecreate ( filetype . file ) ;
if ( destfile . exists ( ) & & !issamefile ( destfile ) )
protected void doremoveattribute ( final string attrname ) throws exception
throw new filesystemexception ( "vfs . provider / read . error" , filename , exc ) ;
return parent ;
fileinfo . setdepth ( curdepth + 1 ) ;
protected boolean dosetlastmodifiedtime ( final long modtime ) throws exception
return childrenobjects ;
throw new filesystemexception ( "vfs . provider / get - last - modified - not - supported . error" ) ;
public boolean isfolder ( ) throws filesystemexception
children = empty file array ;
return fs = = newfile . getfilesystem ( ) ;
throw new filesystemexception ( "vfs . provider / create - folder . error" , filename , exc ) ;
public boolean iswriteable ( ) throws filesystemexception
names [ iterobjects ] = objects [ iterobjects ] . getname ( ) ;
if ( parent = = null )
public filename getname ( )
throw new filesystemexception ( "vfs . provider / rename - read - only . error" , getname ( ) ) ;
protected void childrenchanged ( final filename childname , final filetype newtype ) throws exception
return exists ( ) ? dosetreadable ( readable , owneronly ) : false ;
notifyparent ( this . getname ( ) , newtype ) ;
throw new filesystemexception ( "vfs . provider / set - last - modified - not - supported . error" ) ;
protected void doattach ( ) throws exception
getparent ( ) . getname ( ) ) ;
if ( gettype ( ) = = filetype . imaginary )
protected void onchildrenchanged ( final filename child , final filetype newtype ) throws exception
final filename child = element . getname ( ) ;
info . setbasefolder ( this ) ;
destfile . getname ( ) ) ;
protected boolean doiswriteable ( ) throws exception
throw new filesystemexception ( "vfs . provider / delete . error" , exc , filename ) ;
final int curdepth = fileinfo . getdepth ( ) ;
fileinfo . setdepth ( curdepth ) ;
protected long dogetlastmodifiedtime ( ) throws exception
files = dolistchildren ( ) ;
protected void dodetach ( ) throws exception
if ( exists ( ) & & !isfile ( ) )
return doiswriteable ( ) ;
objects = new arraylist < object > ( initial list size ) ;
info . setfile ( this ) ;
throw new filesystemexception ( "vfs . provider / rename . error" , exc ,
fileinfo . setfile ( child ) ;
if ( !iswriteable ( ) )
public string tostring ( )
throw new filesystemexception ( "vfs . provider / check - is - writeable . error" , filename , exc ) ;
list . add ( childname ) ;
objects [ iterchildren ] = resolvefile ( children [ iterchildren ] ) ;
return fs . resolvefile ( othername ) ;
public url geturl ( ) throws filesystemexception
private filename [ ] extractnames ( final fileobject [ ] objects )
final defaultfileselectorinfo info = new defaultfileselectorinfo ( ) ;
public void createfolder ( ) throws filesystemexception
content . close ( ) ;
protected randomaccesscontent dogetrandomaccesscontent ( final randomaccessmode mode ) throws exception
public int compareto ( final fileobject file )
protected void dodelete ( ) throws exception
final arraylist < filename > list = new arraylist < filename > ( arrays . aslist ( children ) ) ;
protected void handledelete ( ) throws exception
if ( srcfile . gettype ( ) . hascontent ( ) )
setfiletype ( null ) ;
protected boolean dosetexecutable ( final boolean writable , final boolean owneronly ) throws exception
public void refresh ( ) throws filesystemexception
content = null ;
throw new filesystemexception ( "vfs . provider / delete - not - supported . error" ) ;
if ( gettype ( ) . haschildren ( ) )
private fileobject [ ] resolvefiles ( final filename [ ] children ) throws filesystemexception
return this . delete ( selectors . select all ) ;
if ( content ! = null )
return resolvefile ( child ) ;
throw new filesystemexception ( "vfs . provider / write - append - not - supported . error" , filename ) ;
return resolvefiles ( children ) ;
list . remove ( childname ) ;
public void holdobject ( final object strongref )
else if ( files . length = = 0 )
if ( objects = = null )
dodelete ( ) ;
return fs . getparentlayer ( ) . getparent ( ) ;
public boolean delete ( ) throws filesystemexception
final fileselector selector ,
catch ( final ioexception e )
protected abstract string [ ] dolistchildren ( ) throws exception ;
protected boolean dosetreadable ( final boolean readable , final boolean owneronly ) throws exception
public void copyfrom ( final fileobject file , final fileselector selector )
this . findfiles ( selector , true , list ) ;
fileutil . copycontent ( srcfile , destfile ) ;
throw new filesystemexception ( "vfs . provider / random - access - not - supported . error" ) ;
protected boolean issamefile ( final fileobject destfile ) throws filesystemexception
public boolean setexecutable ( final boolean readable , final boolean owneronly ) throws filesystemexception
handledelete ( ) ;
throw new filesystemexception ( "vfs . provider / find - files . error" , filename , e ) ;
return exists ( ) ? doishidden ( ) : false ;
if ( children = = null )
public boolean exists ( ) throws filesystemexception
fs . fileobjectdestroyed ( this ) ;
protected void dorename ( final fileobject newfile ) throws exception
throws filesystemexception
return filetype . file . equals ( this . gettype ( ) ) ;
protected void docreatefolder ( ) throws exception
return exists ( ) ? dosetexecutable ( readable , owneronly ) : false ;
return list = = null ? null : list . toarray ( new fileobject [ list . size ( ) ] ) ;
else if ( srcfile . gettype ( ) . haschildren ( ) )
return exists ( ) ? dosetwritable ( readable , owneronly ) : false ;
notifyparent ( this . getname ( ) , filetype . imaginary ) ;
return exists ( ) ? doisreadable ( ) : false ;
protected outputstream dogetoutputstream ( final boolean bappend ) throws exception
public fileobject resolvefile ( final string path ) throws filesystemexception
buf . tostring ( ) , new defaulturlstreamhandler ( fs . getcontext ( ) , fs . getfilesystemoptions ( ) ) ) ;
throw new filesystemexception ( "vfs . provider / create - file . error" , filename ) ;
if ( type = = null )
findfiles ( selector , true , files ) ;
protected abstract inputstream dogetinputstream ( ) throws exception ;
throw new filesystemexception ( "vfs . provider / check - is - hidden . error" , filename , exc ) ;
int nuofdeleted = 0 ;
synchronized ( fs )
fs . firefilechanged ( this ) ;
private fileobject parent ;
deleteself ( ) ;
protected abstract filetype dogettype ( ) throws exception ;
docreatefolder ( ) ;
try
traverse ( fileinfo , selector , depthwise , selected ) ;
removechildrencache ( ) ;
if ( selector . includefile ( fileinfo ) )
if ( ( destfile . gettype ( ) . hascontent ( )
private static void traverse ( final defaultfileselectorinfo fileinfo ,
return this . tostring ( ) . comparetoignorecase ( file . tostring ( ) ) ;
return 1 ;
endoutput ( ) ;
throw new filesystemexception ( "vfs . provider / check - is - readable . error" , filename , exc ) ;
public fileobject getparent ( ) throws filesystemexception
setfiletype ( dogettype ( ) ) ;
protected map < string , object > dogetattributes ( ) throws exception
operations = new defaultfileoperations ( this ) ;
fileinfo . setfile ( file ) ;
public outputstream getoutputstream ( ) throws filesystemexception
if ( this = = fs . getroot ( ) )
throw new filesystemexception ( "vfs . provider / check - is - executable . error" , filename , exc ) ;
if ( !fs . hascapability ( capability . list children ) )
traverse ( info , selector , depthwise , selected ) ;
& & destfile . getfilesystem ( ) . hascapability ( capability . set last modified folder ) )
childrenobjects = dolistchildrenresolved ( ) ;
final int index = selected . size ( ) ;
detach ( ) ;
destfile . copyfrom ( this , selectors . select self ) ;
return nuofdeleted ;
if ( !getparent ( ) . iswriteable ( ) )
return ;
destfile . getcontent ( ) . setlastmodifiedtime ( this . getcontent ( ) . getlastmodifiedtime ( ) ) ;
final string [ ] files ;
throw exc ;
for ( int iterchildren = 0 ; iterchildren < children . length ; iterchildren + + )
if ( bappend & & !fs . hascapability ( capability . append content ) )
public boolean iscontentopen ( )
return exists ( ) ? doisexecutable ( ) : false ;
basefile = null ;
providers . clear ( ) ;
filescache = null ;
localfileprovider = null ;
private final arraylist < object > components = new arraylist < object > ( ) ;
map . clear ( ) ;
private final filetypemap map = new filetypemap ( ) ;
fileobjectdecorator = null ;
filecachestrategy = null ;
filecontentinfofactory = null ;
private final map < string , list < fileoperationprovider > > operationproviders =
closecomponent ( filescache ) ;
private final defaultvfscomponentcontext context = new defaultvfscomponentcontext ( this ) ;
operationproviders . clear ( ) ;
private filereplicator filereplicator ;
new hashmap < string , list < fileoperationprovider > > ( ) ;
private fileprovider defaultprovider ;
private localfileprovider localfileprovider ;
private final map < string , fileprovider > providers = new hashmap < string , fileprovider > ( ) ;
fileobjectdecoratorconst = null ;
private final virtualfileprovider vfsprovider = new virtualfileprovider ( ) ;
return typemap . getscheme ( file ) ! = null ;
filescache = null ;
vfsprovider . closefilesystem ( filesystem ) ;
log . warn ( "defaultfilesystemmanager . close : not all components are closed : " + components . tostring ( ) ) ;
else if ( filesystem instanceof virtualfilesystem )
if ( filescache = = null )
}
{
final string scheme = typemap . getscheme ( file ) ;
private final filetypemap typemap = new filetypemap ( ) ;
components . clear ( ) ;
private virtualfileprovider vfsprovider ;
defaultprovider = null ;
vfsprovider = null ;
typemap . addextension ( extension , scheme ) ;
filescache = new softreffilescache ( ) ;
filereplicator = null ;
closecomponent ( filescache ) ;
closecomponent ( defaultprovider ) ;
typemap . clear ( ) ;
vfsprovider = new virtualfileprovider ( ) ;
operationproviders . clear ( ) ;
if ( !components . isempty ( ) )
closecomponent ( vfsprovider ) ;
typemap . addmimetype ( mimetype , scheme ) ;
tempfilestore = null ;
}
@ override
getabstractfilesystem ( ) . getzipfile ( ) ;
protected void doattach ( ) throws exception {
getabstractfilesystem ( ) . close ( ) ;
protected void dodetach ( ) throws exception {
serverdata . getserviceworker ( ) ;
import org . apache . hadoop . io . writable ;
bytearrayonetoallmessages < i , m > onetoallmsgs ,
}
onetoallmsgs = new bytearrayonetoallmessages < i , m > (
new int2objectopenhashmap < bytearrayvertexidmessages > ( ) ;
m msg = onetoallmsgs . createmessage ( ) ;
i vertexid = getconf ( ) . createvertexid ( ) ;
onetoallmsgs . readfields ( input ) ;
idmsgs . getkey ( ) , idmsgs . getvalue ( ) ) ;
partitionidmsgs . get ( partitionid ) ;
immutableclassesgiraphconfiguration conf ) {
import java . io . ioexception ;
centralizedserviceworker < i , ? , ? > serviceworker =
for ( int i = 0 ; i < idcount ; i + + ) {
if ( !idmsgs . getvalue ( ) . isempty ( ) ) {
public void writerequest ( dataoutput output ) throws ioexception {
private bytearrayonetoallmessages < i , m > onetoallmsgs ;
public requesttype gettype ( ) {
import java . io . dataoutput ;
try {
extendeddatainput reader = onetoallmsgs . getonetoallmessagesreader ( ) ;
public class sendworkeronetoallmessagesrequest < i extends writablecomparable ,
vertexid . readfields ( reader ) ;
@ suppresswarnings ( "unchecked" )
import org . apache . giraph . bsp . centralizedserviceworker ;
getconf ( ) . < m > getoutgoingmessagevaluefactory ( ) ) ;
package org . apache . giraph . comm . requests ;
import it . unimi . dsi . fastutil . ints . int2objectopenhashmap ;
int partitionid = 0 ;
int idcount = 0 ;
import org . apache . giraph . conf . immutableclassesgiraphconfiguration ;
import org . apache . giraph . utils . bytearrayvertexidmessages ;
partitionowner owner =
onetoallmsgs . setconf ( getconf ( ) ) ;
idcount = reader . readint ( ) ;
m extends writable > extends writablerequest < i , writable , writable >
serverdata . getpartitionstore ( ) . getnumpartitions ( ) * 2 ;
import org . apache . hadoop . io . writablecomparable ;
int2objectopenhashmap < bytearrayvertexidmessages >
for ( entry < integer , bytearrayvertexidmessages > idmsgs :
return requesttype . send worker onetoall messages request ;
partitionidmsgs . put ( partitionid , idmsgs ) ;
this . onetoallmsgs . write ( output ) ;
idmsgs . initialize ( initialsize ) ;
public void readfieldsrequest ( datainput input ) throws ioexception {
msg . readfields ( reader ) ;
import java . io . datainput ;
import java . util . map . entry ;
if ( idmsgs = = null ) {
} catch ( ioexception e ) {
@ override
import org . apache . giraph . partition . partitionowner ;
public sendworkeronetoallmessagesrequest (
int initialsize = onetoallmsgs . getsize ( ) /
public int getserializedsize ( ) {
while ( reader . available ( ) ! = 0 ) {
partitionidmsgs =
this . onetoallmsgs = onetoallmsgs ;
import org . apache . giraph . utils . bytearrayonetoallmessages ;
import org . apache . giraph . comm . serverdata ;
idmsgs = new bytearrayvertexidmessages < i , m > (
idmsgs . add ( vertexid , msg ) ;
partitionid = owner . getpartitionid ( ) ;
throw new runtimeexception ( "dorequest : got ioexception . " , e ) ;
serverdata . getincomingmessagestore ( ) . addpartitionmessages (
partitionidmsgs . entryset ( ) ) {
public void dorequest ( serverdata serverdata ) {
bytearrayvertexidmessages < i , m > idmsgs =
setconf ( conf ) ;
return super . getserializedsize ( ) + this . onetoallmsgs . getserializedsize ( ) ;
idmsgs . setconf ( getconf ( ) ) ;
public sendworkeronetoallmessagesrequest ( ) { }
implements workerrequest < i , writable , writable > {
throw new runtimeexception ( "dorequest : got ioexception " , e ) ;
import org . apache . giraph . utils . extendeddatainput ;
serviceworker . getvertexpartitionowner ( vertexid ) ;
}
wlock . unlock ( ) ;
if ( lock . iswritelockedbycurrentthread ( ) ) {
}
if ( !file . createnewfile ( ) ) {
log . error ( "offloadpartition : failed to create directory " + file ) ;
import static org . apache . giraph . conf . giraphconstants . computation class ;
import com . google . common . collect . lists ;
inactive . isempty ( ) ) {
log . error ( "offloadpartition : failed to create file " + file ) ;
if ( !file . getparentfile ( ) . mkdirs ( ) ) {
import org . apache . log4j . logger ;
log . error ( "deletepartitionfiles : failed to delete file " + file ) ;
if ( !file . delete ( ) ) {
log . error ( "loadpartition : failed to delete file " + file ) ;
public void checkoutputspecs ( jobcontext context )
strconfoption edge output format subdir =
import org . apache . hadoop . io . writable ;
import org . apache . hadoop . mapreduce . recordwriter ;
import org . apache . giraph . io . edgewriter ;
}
new booleanconfoption ( "giraph . textoutputformat . reverse" , false ,
"vertexoutputformat sub - directory" ) ;
public taskattemptcontext getcontext ( ) {
return new text ( msg . tostring ( ) ) ;
"reverse values in the output" ) ;
strconfoption vertex output format subdir =
} else {
return linerecordwriter ;
public void initialize ( taskattemptcontext context ) throws ioexception ,
new strconfoption ( "giraph . edge . output . subdir" , "edges" ,
extends textedgeoutputformat < i , v , e > {
import java . io . ioexception ;
return textoutputformat . getrecordwriter ( context ) ;
public void close ( taskattemptcontext context ) throws ioexception ,
import org . apache . hadoop . io . text ;
protected recordwriter < text , text > createlinerecordwriter (
extends edgeoutputformat < i , v , e > {
import org . apache . giraph . edge . edge ;
public recordwriter < text , text > getrecordwriter ( ) {
protected string getsubdir ( ) {
strconfoption giraph text output format separator =
msg . append ( sourceid . tostring ( ) ) ;
private recordwriter < text , text > linerecordwriter ;
v sourcevalue , edge < i , e > edge ) throws ioexception ;
public outputcommitter getoutputcommitter ( taskattemptcontext context )
extends textedgewritertoeachline {
context ) throws ioexception , interruptedexception ;
interruptedexception {
import org . apache . giraph . io . edgeoutputformat ;
msg . append ( edge . gettargetvertexid ( ) . tostring ( ) ) ;
delimiter = giraph text output format separator . get ( getconf ( ) ) ;
import static org . apache . giraph . conf . giraphconstants . giraph text output format reverse ;
public void initialize ( taskattemptcontext context )
this . context = context ;
extends edgewriter < i , v , e > {
new strconfoption ( "giraph . vertex . output . subdir" , "" ,
throws ioexception , interruptedexception {
edgeoutputformat . class , "edgeoutputformat class" ) ;
import org . apache . hadoop . io . writablecomparable ;
protected abstract class textedgewritertoeachline extends textedgewriter {
new strconfoption ( "giraph . textoutputformat . separator" , "" ,
public textedgewriter createedgewriter ( taskattemptcontext context ) {
reverseoutput = giraph text output format reverse . get ( getconf ( ) ) ;
import static org . apache . giraph . conf . giraphconstants . edge output format subdir ;
super . initialize ( context ) ;
msg . append ( delimiter ) ;
throws ioexception {
import org . apache . hadoop . mapreduce . taskattemptcontext ;
taskattemptcontext context ) throws ioexception , interruptedexception {
msg . append ( edge . getvalue ( ) . tostring ( ) ) ;
package org . apache . giraph . io . formats ;
v extends writable , e extends writable >
public abstract textedgewriter createedgewriter ( taskattemptcontext
if ( reverseoutput ) {
private string delimiter ;
import org . apache . hadoop . mapreduce . outputcommitter ;
linerecordwriter . close ( context ) ;
protected class srciddstidedgevalueedgewriter
return edge output format subdir . get ( getconf ( ) ) ;
classconfoption < edgeoutputformat > edge output format class =
convertedgetoline ( sourceid , sourcevalue , edge ) , null ) ;
@ override
public final void writeedge ( i sourceid , v sourcevalue , edge < i , e > edge )
"edgeoutputformat sub - directory" ) ;
new giraphtextoutputformat ( ) {
import org . apache . hadoop . mapreduce . jobcontext ;
classconfoption . create ( "giraph . edgeoutputformatclass" , null ,
private boolean reverseoutput ;
protected abstract class textedgewriter
return textoutputformat . getoutputcommitter ( context ) ;
getrecordwriter ( ) . write (
protected text convertedgetoline ( i sourceid , v sourcevalue , edge < i , e > edge )
public class srciddstidedgevaluetextoutputformat < i extends writablecomparable ,
booleanconfoption giraph text output format reverse =
protected giraphtextoutputformat textoutputformat =
return new srciddstidedgevalueedgewriter ( ) ;
stringbuilder msg = new stringbuilder ( ) ;
"giraphtextouputformat separator" ) ;
linerecordwriter = createlinerecordwriter ( context ) ;
public abstract class textedgeoutputformat < i extends writablecomparable ,
textoutputformat . checkoutputspecs ( context ) ;
protected abstract text convertedgetoline ( i sourceid ,
import static org . apache . giraph . conf . giraphconstants . giraph text output format separator ;
private taskattemptcontext context ;
@ suppresswarnings ( "rawtypes" )
} ;
return context ;
currentpositionininputs + + ;
public boolean readboolean ( ) throws ioexception {
currentinput . readfully ( b ) ;
return currentinput . readunsignedbyte ( ) ;
private void movetonextdatainput ( ) {
return currentinput . readfloat ( ) ;
public int readunsignedshort ( ) throws ioexception {
public int getpos ( ) {
package org . apache . giraph . utils . io ;
import org . apache . giraph . utils . extendeddataoutput ;
return currentinput . readline ( ) ;
private static final extendeddatainput empty input =
import java . util . list ;
public void readfully ( byte [ ] b ) throws ioexception {
if ( currentpositionininputs < datainputs . size ( ) ) {
datainputs = new arraylist < extendeddatainput > (
}
public long readlong ( ) throws ioexception {
import org . apache . giraph . utils . extendedbytearraydatainput ;
available + = datainputs . get ( i ) . available ( ) ;
return n - byteslefttoskip + bytesskipped ;
public bigdatainput ( bigdataoutput bigdataoutput ) {
public class bigdatainput implements extendeddatainput {
currentinput . readfully ( b , off , len ) ;
int available = 0 ;
public float readfloat ( ) throws ioexception {
return currentinput . readbyte ( ) ;
public string readline ( ) throws ioexception {
public char readchar ( ) throws ioexception {
return currentinput . readutf ( ) ;
byteslefttoskip - = currentinput . available ( ) ;
for ( int i = currentpositionininputs ; i < datainputs . size ( ) ; i + + ) {
} else {
public string readutf ( ) throws ioexception {
currentinput = datainputs . get ( currentpositionininputs ) ;
private final list < extendeddatainput > datainputs ;
private int currentpositionininputs ;
datainputs . add ( bigdataoutput . getconf ( ) . createextendeddatainput (
public int readunsignedbyte ( ) throws ioexception {
return available ;
movetonextdatainput ( ) ;
while ( byteslefttoskip > = currentinput . available ( ) ) {
import java . io . ioexception ;
checkifshouldmovetonextdatainput ( ) ;
return pos ;
return currentinput . readlong ( ) ;
int bytesskipped = currentinput . skipbytes ( byteslefttoskip ) ;
return currentinput . readint ( ) ;
public int available ( ) {
return currentinput . readdouble ( ) ;
bigdataoutput . getnumberofdataoutputs ( ) ) ;
int byteslefttoskip = n ;
return currentinput . readboolean ( ) ;
public double readdouble ( ) throws ioexception {
public byte readbyte ( ) throws ioexception {
return currentinput . readchar ( ) ;
int pos = 0 ;
for ( extendeddataoutput dataoutput : bigdataoutput . getdataoutputs ( ) ) {
for ( int i = 0 ; i < = currentpositionininputs ; i + + ) {
private extendeddatainput currentinput ;
@ override
private void checkifshouldmovetonextdatainput ( ) {
public void readfully ( byte [ ] b , int off , int len ) throws ioexception {
public short readshort ( ) throws ioexception {
currentpositionininputs = - 1 ;
import java . util . arraylist ;
pos + = datainputs . get ( i ) . getpos ( ) ;
return currentinput . readshort ( ) ;
public int skipbytes ( int n ) throws ioexception {
currentinput = empty input ;
new extendedbytearraydatainput ( new byte [ 0 ] ) ;
dataoutput . getbytearray ( ) , 0 , dataoutput . getpos ( ) ) ) ;
import org . apache . giraph . utils . extendeddatainput ;
if ( currentinput . available ( ) = = 0 ) {
public int readint ( ) throws ioexception {
return currentinput . readunsignedshort ( ) ;
}
public boolean iszookeeperexternal ( ) {
zookeeper is external . set ( this , false ) ;
return zookeeper is external . get ( this ) ;
public void setzookeeperlist ( string zklist ) {
set ( zookeeper list , zklist ) ;
if ( serverportlist . isempty ( ) & & startzookeepermanager ( ) ) {
workerprogressbasepath , false , false , true ) ;
createmode . persistent ,
import org . apache . zookeeper . watcher ;
true ) ;
import org . apache . log4j . logger ;
}
private static final int write update period milliseconds = 10 * 1000 ;
public workerprogresswriter ( final string myprogresspath ,
verticeswritten % vertices to update progress ) ;
import java . io . ioexception ;
package org . apache . giraph . worker ;
verticestocompute + =
for ( int partitionid : getpartitionstore ( ) . getpartitionids ( ) ) {
if ( workerprogresswriter ! = null ) {
import org . apache . giraph . conf . giraphconfiguration ;
workerprogress . get ( ) . finishloadingedges ( ) ;
private thread writerthread ;
public void process ( watchedevent event ) {
private static final logger log =
private void instantiatebspservice ( )
workerprogress . get ( ) . finishloadingvertices ( ) ;
workerprogress . get ( ) . finishstoring ( ) ;
if ( combinedworkerprogress . isdone ( conf . getmaxworkers ( ) ) ) {
vertices to update progress ) ;
verticestostore + = getpartitionstore ( ) . getorcreatepartition (
} catch ( interruptedexception e ) {
} finally {
writerthread = new thread ( new runnable ( ) {
conf . getzookeeperopsmaxattempts ( ) ,
import org . apache . zookeeper . watchedevent ;
public jobprogresstracker ( final job submittedjob ,
try {
workerprogress . get ( ) . addverticesstored (
workerprogress . writetoznode ( zk , myprogresspath ) ;
writerthread . start ( ) ;
public void progress ( ) {
public class jobprogresstracker implements watcher {
thread . sleep ( ( long ) ( write update period milliseconds * factor ) ) ;
for ( string workerprogresspath : workerprogresspaths ) {
new combinedworkerprogress ( workerprogresses ) ;
private volatile boolean finished = false ;
import org . apache . giraph . zk . zookeeperext ;
verticestocompute ,
import org . apache . zookeeper . keeperexception ;
} ) ;
long nextupdateprogressvertices = 0 ;
double factor = 1 + math . random ( ) ;
final giraphconfiguration conf ) throws ioexception , interruptedexception {
private static final int update milliseconds = 5 * 1000 ;
final string basepath = counterutils . waitandgetcounternamefromgroup (
if ( zk . exists ( workerprogressbasepath , false ) ! = null ) {
long verticestostore = 0 ;
list < string > workerprogresspaths = zk . getchildrenext (
zkserver ,
workerprogresswriter = conf . trackjobprogressonclient ( ) ?
import java . util . arraylist ;
if ( verticeswritten > = nextupdateprogressvertices ) {
break ;
serviceworker = new bspserviceworker < i , v , e > ( context , this ) ;
import org . apache . hadoop . util . progressable ;
writableutils . readfieldsfrombytearray ( zkdata , workerprogress ) ;
this ,
basepath + bspservice . cleaned up dir + " / client" ,
import org . apache . giraph . utils . writableutils ;
writerthread . interrupt ( ) ;
import java . util . list ;
super ( context , graphtaskmanager ) ;
logger . getlogger ( workerprogresswriter . class ) ;
import org . apache . giraph . utils . counterutils ;
workerprogress . get ( ) . startstoring (
workerprogress . get ( ) . startsuperstep (
import org . apache . giraph . conf . giraphconstants ;
log . info ( "run : workerprogresswriter interrupted" , e ) ;
serviceworker . getpartitionstore ( ) . getnumpartitions ( ) ) ;
serviceworker . getsuperstep ( ) ,
serviceworker . getpartitionstore ( ) . getorcreatepartition (
workerprogresses . add ( workerprogress ) ;
public void run ( ) {
new progressable ( ) {
nextupdateprogressvertices + = vertices to update progress ;
workerprogress . get ( ) . incrementpartitionsstored ( ) ;
private final workerprogresswriter workerprogresswriter ;
workerprogress workerprogress = new workerprogress ( ) ;
import org . apache . hadoop . mapreduce . job ;
thread . sleep ( update milliseconds ) ;
@ override
import org . apache . giraph . bsp . bspservice ;
servicemaster = new bspservicemaster < i , v , e > ( context , this ) ;
private final thread writerthread ;
private static final long vertices to update progress = 100000 ;
zk . close ( ) ;
new arraylist < workerprogress > ( workerprogresspaths . size ( ) ) ;
if ( log . isinfoenabled ( ) ) {
string workerprogressbasepath = basepath + bspservice . worker progresses ;
null ,
log . info ( "run : exception occurred" , e ) ;
list < workerprogress > workerprogresses =
conf . getzookeepersessiontimeout ( ) ,
public void stop ( ) {
log . info ( combinedworkerprogress . tostring ( ) ) ;
import org . apache . giraph . worker . workerprogress ;
} catch ( interruptedexception | keeperexception e ) {
workerprogress . writetoznode ( getzkext ( ) , myprogresspath ) ;
partitionid ) . getvertexcount ( ) ;
new workerprogresswriter ( myprogresspath , getzkext ( ) ) : null ;
zk = new zookeeperext (
private static final logger log = logger . getlogger ( jobprogresstracker . class ) ;
public class workerprogresswriter {
zoodefs . ids . open acl unsafe ,
private zookeeperext zk ;
long verticestocompute = 0 ;
submittedjob , giraphconstants . zookeeper server port counter group ) ;
byte [ ] zkdata = zk . getdata ( workerprogresspath , false , null ) ;
verticestostore , getpartitionstore ( ) . getnumpartitions ( ) ) ;
instantiatebspservice ( ) ;
conf . getzookeeperopsretrywaitmsecs ( ) ,
final zookeeperext zk ) {
submittedjob , giraphconstants . zookeeper base path counter group ) ;
combinedworkerprogress combinedworkerprogress =
finished = true ;
import org . apache . zookeeper . zoodefs ;
while ( !finished ) {
string zkserver = counterutils . waitandgetcounternamefromgroup (
import org . apache . zookeeper . createmode ;
workerprogresswriter . stop ( ) ;
zk . createext (
package org . apache . giraph . job ;
log . info ( "run : workerprogresswriter interrupted" ) ;
"cause = " + writefuture . cause ( ) + " , " +
connectionfuture . cause ( ) ) ;
. option ( channeloption . connect timeout millis ,
import io . netty . channel . channelfuturelistener ;
. channel ( niosocketchannel . class )
. option ( channeloption . so sndbuf , sendbuffersize )
import io . netty . channel . channeloption ;
workergroup . shutdowngracefully ( ) ;
}
"clientoutboundbytecounter" ,
" , " + inboundbytecounter . getmetrics ( ) + " \ n" +
} else {
connectionfuture . channel ( ) ) ;
public static final attributekey < saslnettyclient > sasl =
import io . netty . channel . socket . socketchannel ;
import io . netty . channel . socket . nio . niosocketchannel ;
. option ( channeloption . tcp nodelay , true )
workergroup = new nioeventloopgroup ( maxpoolsize ,
outboundbytecounter . getmetrics ( ) ) ;
inboundbytecounter . resetall ( ) ;
"bound = " + channel . isregistered ( ) ) ;
handlertouseexecutiongroup + " . " ) ;
private final inboundbytecounter inboundbytecounter = new
new lengthfieldbasedframedecoder ( 1024 , 0 , 4 , 0 , 4 ) ,
pipelineutils . addlastwithexecutorcheck ( "sasl - client - handler" ,
inboundbytecounter ( ) ;
new threadfactorybuilder ( ) . setnameformat ( "netty - client - exec - % d" )
import io . netty . channel . channelinitializer ;
"fixed - length - frame - decoder" ,
return connectionfuture . channel ( ) ;
private final boolean useexecutiongroup ;
"destination = " + writefuture . channel ( ) . remoteaddress ( ) +
handlertouseexecutiongroup =
import io . netty . util . concurrent . eventexecutorgroup ;
if ( channel . isactive ( ) ) {
} ) ;
saslnettyclient saslnettyclient = channel . attr ( sasl ) . get ( ) ;
new saslclienthandler ( conf ) , handlertouseexecutiongroup ,
new responseclienthandler ( clientrequestidrequestinfomap ,
pipelineutils . addlastwithexecutorcheck ( "request - encoder" ,
bootstrap = new bootstrap ( ) ;
import io . netty . handler . codec . lengthfieldbasedframedecoder ;
if ( !writefuture . channel ( ) . isactive ( ) | |
import io . netty . handler . codec . fixedlengthframedecoder ;
. getmetrics ( ) ) ;
pipelineutils . addlastwithexecutorcheck (
import io . netty . channel . channel ;
if ( channel . remoteaddress ( ) = = null ) {
if ( channel . attr ( sasl ) . get ( ) = = null ) {
progressableutils . awaitterminationfuture ( executiongroup , context ) ;
channel channel = future . channel ( ) ;
"length - field - based - frame - decoder" ,
failures + " failures because of " + future . cause ( ) ) ;
executiongroup . shutdowngracefully ( ) ;
. handler ( new channelinitializer < socketchannel > ( ) {
outboundbytecounter ( ) ;
protected void initchannel ( socketchannel ch ) throws exception {
channel . attr ( sasl ) . set ( saslnettyclient ) ;
new fixedlengthframedecoder (
executiongroup = new defaulteventexecutorgroup ( executionthreads ,
log . info ( "using netty with authentication . " ) ;
import io . netty . util . attributekey ;
executiongroup , ch ) ;
pipelineutils . addlastwithexecutorcheck ( "response - handler" ,
. option ( channeloption . allocator , conf . getnettyallocator ( ) )
import io . netty . util . concurrent . defaulteventexecutorgroup ;
import io . netty . channel . eventloopgroup ;
private final outboundbytecounter outboundbytecounter = new
@ override
private final string handlertouseexecutiongroup ;
if ( useexecutiongroup ) {
. option ( channeloption . so rcvbuf , receivebuffersize )
private final eventloopgroup workergroup ;
channel . remoteaddress ( ) + " , open = " + channel . isopen ( ) ) ;
log . info ( "using netty without authentication . " ) ;
rotater . addchannel ( future . channel ( ) ) ;
new requestencoder ( conf ) , handlertouseexecutiongroup ,
handlertouseexecutiongroup , executiongroup , ch ) ;
if ( conf . authenticate ( ) ) {
private final eventexecutorgroup executiongroup ;
private final bootstrap bootstrap ;
max connection milliseconds default )
executiongroup = null ;
import io . netty . bootstrap . bootstrap ;
requestserverhandler . response bytes ) ,
conf ) , handlertouseexecutiongroup , executiongroup , ch ) ;
import org . apache . giraph . utils . pipelineutils ;
useexecutiongroup = netty client use execution handler . get ( conf ) ;
import io . netty . channel . nio . nioeventloopgroup ;
import io . netty . channel . channelfuture ;
inboundbytecounter , handlertouseexecutiongroup ,
. option ( channeloption . so keepalive , true )
pipelineutils . addlastwithexecutorcheck ( "clientinboundbytecounter" ,
inboundbytecounter . getmetrics ( ) + " \ n" + outboundbytecounter
attributekey . valueof ( "saslnettyclient" ) ;
outboundbytecounter . resetall ( ) ;
bootstrap . group ( workergroup )
if ( executiongroup ! = null ) {
outboundbytecounter , handlertouseexecutiongroup ,
writefuture . channel ( ) . isactive ( ) +
. build ( ) ) ;
partition < i , v , e > partition =
long2objectmap . entry < outedges < longwritable , e > > > {
return outedges ;
edge < i , e > edge = reuseedgeobjects ?
import org . apache . giraph . utils . progressableutils ;
service . getpartitionstore ( ) . getorcreatepartition ( partitionid ) ;
return ( ( long2objectmap < outedges < longwritable , e > > ) partitionedges )
import org . apache . hadoop . io . writable ;
extends abstractedgestore < longwritable , v , e , long ,
protected abstract iterator < et >
import org . apache . log4j . logger ;
import org . apache . hadoop . io . intwritable ;
}
immutableclassesgiraphconfiguration < intwritable , v , e > configuration ,
this . service = service ;
iterator < et > iterator =
super ( service , configuration , progressable ) ;
public callable < void > newcallable ( int callableid ) {
import org . apache . hadoop . io . longwritable ;
new int2objectopenhashmap < outedges < intwritable , e > > ( ) ) ;
map . entry < i , outedges < i , e > > > {
protected intwritable createvertexid (
i vertexid = getvertexid ( entry ,
new mapmaker ( ) . concurrencylevel (
vertex . addedge ( edge ) ;
longwritable representativevertexid ) {
getpartitionedgesiterator ( map < k , outedges < i , e > > partitionedges ) ;
outedges = configuration . createandinitializeinputoutedges ( ) ;
import it . unimi . dsi . fastutil . longs . long2objectmap ;
} else {
if ( !useinputoutedges ) {
int2objectmap . entry < outedges < intwritable , e > > > {
centralizedserviceworker < i , v , e > service ,
public abstract class abstractedgestore < i extends writablecomparable ,
outedges < i , e > outedges = partitionedges . get ( vertexid ) ;
immutableclassesgiraphconfiguration < i , v , e > configuration ,
int2objectmap . entry < outedges < intwritable , e > > entry ) {
map < long , outedges < longwritable , e > > partitionedgesin ) {
transientedges = new mapmaker ( ) . concurrencylevel (
long2objectmap < outedges < longwritable , e > > partitionedges =
centralizedserviceworker < longwritable , v , e > service ,
int2objectmaps . synchronize (
integer partitionid ;
. iterator ( ) ;
return entry . getkey ( ) ;
import it . unimi . dsi . fastutil . longs . long2objectopenhashmap ;
outedges . add ( edge ) ;
return new callable < void > ( ) {
return partitionedges . entryset ( ) . iterator ( ) ;
import java . util . map ;
representativevertexid ) ;
int2objectmap < outedges < intwritable , e > > newpartitionedges =
partitionedges . put ( vertexid . get ( ) , outedges ) ;
package org . apache . giraph . edge . primitives ;
representativevertexid . set ( entry . getintkey ( ) ) ;
import org . apache . giraph . partition . partition ;
vertexidedgeiterator . next ( ) ;
extends defaultimmutableclassesgiraphconfigurable < i , v , e >
vertex . setedges ( outedges ) ;
protected longwritable createvertexid (
return inputedges ;
protected abstract i createvertexid ( et entry ) ;
public simpleedgestore (
if ( outedges = = null ) {
partitionidqueue . addall ( transientedges . keyset ( ) ) ;
partition . savevertex ( vertex ) ;
import org . apache . giraph . bsp . centralizedserviceworker ;
return configuration . createandinitializeoutedges ( inputedges ) ;
if ( vertex = = null ) {
int2objectmap < outedges < intwritable , e > > partitionedges =
final blockingqueue < integer > partitionidqueue =
( long2objectmap < outedges < longwritable , e > > ) partitionedgesin ;
protected outedges < intwritable , e > removepartitionedges (
configuration . getnettyserverexecutionconcurrency ( ) ) . makemap ( ) ;
map . entry < i , outedges < i , e > > entry ,
concurrentmap < i , outedges < i , e > > newpartitionedges =
import org . apache . giraph . graph . vertex ;
protected longwritable getvertexid (
import it . unimi . dsi . fastutil . ints . int2objectopenhashmap ;
concurrentmap < i , outedges < i , e > > partitionedges =
int partitionid , bytearrayvertexidedges < i , e > edges ) {
return partitionedges ;
transientedges . clear ( ) ;
transientedges . putifabsent ( partitionid ,
protected abstract outedges < i , e > getvertexoutedges (
import it . unimi . dsi . fastutil . longs . long2objectmaps ;
import it . unimi . dsi . fastutil . ints . int2objectmaps ;
new long2objectopenhashmap < outedges < longwritable , e > > ( ) ) ;
progressable progressable ) {
protected long2objectmap < outedges < longwritable , e > > getpartitionedges (
partitionedges = ( concurrentmap < i , outedges < i , e > > )
final boolean createsourcevertex = configuration . getcreatesourcevertex ( ) ;
outedges < i , e > inputedges ) {
getpartitionedgesiterator ( map < i , outedges < i , e > > partitionedges ) {
i representativevertexid ) {
vertex < i , v , e > vertex = partition . getvertex ( vertexid ) ;
public class simpleedgestore < i extends writablecomparable ,
log . info ( "moveedgestovertices : moving incoming edges to vertices . " ) ;
public intedgestore (
import org . apache . giraph . conf . immutableclassesgiraphconfiguration ;
i vertexid = vertexidedgeiterator . getcurrentvertexid ( ) ;
bytearrayvertexidedges < i , e > . vertexidedgeiterator vertexidedgeiterator ,
return new intwritable ( entry . getintkey ( ) ) ;
public longedgestore (
getpartitionedgesiterator (
partitionedges = ( int2objectmap < outedges < intwritable , e > > )
synchronized ( partitionedges ) {
if ( vertex . getnumedges ( ) = = 0 ) {
long2objectmap . entry < outedges < longwritable , e > > entry ,
import it . unimi . dsi . fastutil . ints . int2objectmap ;
longwritable vertexid = vertexidedgeiterator . getcurrentvertexid ( ) ;
getpartitionedgesiterator ( partitionedges ) ;
protected outedges < i , e > removepartitionedges (
useinputoutedges = configuration . useinputoutedges ( ) ;
service . getpartitionstore ( ) . putpartition ( partition ) ;
return new longwritable ( entry . getlongkey ( ) ) ;
new arrayblockingqueue < > ( transientedges . size ( ) ) ;
outedges < longwritable , e > outedges = partitionedges . get ( vertexid . get ( ) ) ;
protected iterator < map . entry < i , outedges < i , e > > >
import org . apache . hadoop . io . writablecomparable ;
map < i , outedges < i , e > > partitionedgesin ) {
newpartitionedges ) ;
reuseedgeobjects = configuration . reuseedgeobjects ( ) ;
( concurrentmap < i , outedges < i , e > > ) partitionedgesin ;
package org . apache . giraph . edge ;
import org . apache . hadoop . util . progressable ;
log . info ( "moveedgestovertices : finished moving incoming edges to " +
protected immutableclassesgiraphconfiguration < i , v , e > configuration ;
synchronized ( outedges ) {
return partitionedges . put ( entry . getintkey ( ) , null ) ;
vertexidedgeiterator ,
if ( createsourcevertex ) {
outedges = partitionedges . get ( vertexid . get ( ) ) ;
outedges < intwritable , e > outedges = partitionedges . get ( vertexid . get ( ) ) ;
map < k , outedges < i , e > > partitionedgesin ) ;
outedges < i , e > newoutedges =
vertexidedgeiterator . releasecurrentvertexid ( ) ;
partitionedges ) ;
protected iterator < int2objectmap . entry < outedges < intwritable , e > > >
transientedges . remove ( partitionid ) ;
protected abstract map < k , outedges < i , e > > getpartitionedges ( int partitionid ) ;
import java . util . iterator ;
public class intedgestore < v extends writable , e extends writable >
long2objectmap < outedges < longwritable , e > > newpartitionedges =
if ( transientedges . isempty ( ) ) {
import java . util . concurrent . blockingqueue ;
vertex . initialize ( createvertexid ( entry ) ,
while ( iterator . hasnext ( ) ) {
long2objectmap . entry < outedges < longwritable , e > > entry ) {
bytearrayvertexidedges < longwritable , e > . vertexidedgeiterator
v extends writable , e extends writable >
edges . getvertexidedgeiterator ( ) ;
protected centralizedserviceworker < i , v , e > service ;
private outedges < i , e > convertinputtocomputeedges (
vertexidedgeiterator . getcurrentedge ( ) :
"move - edges - % d" , progressable ) ;
configuration . createvertexvalue ( ) , outedges ) ;
import org . apache . giraph . conf . defaultimmutableclassesgiraphconfigurable ;
bytearrayvertexidedges < i , e > . vertexidedgeiterator vertexidedgeiterator =
private static final logger log = logger . getlogger ( abstractedgestore . class ) ;
callablefactory < void > callablefactory = new callablefactory < void > ( ) {
protected i getvertexid ( map . entry < i , outedges < i , e > > entry ,
configuration . createandinitializeinputoutedges ( ) ;
this . configuration = configuration ;
public void addpartitionedges (
this . progressable = progressable ;
. long2objectentryset ( )
import org . apache . giraph . edge . abstractedgestore ;
partitionedges = newpartitionedges ;
outedges < i , e > outedges = convertinputtocomputeedges (
( long2objectmap < outedges < longwritable , e > > )
protected concurrentmap < integer , map < k , outedges < i , e > > > transientedges ;
protected progressable progressable ;
map < k , outedges < i , e > > partitionedges =
map < i , outedges < i , e > > partitionedges ) {
@ override
if ( partitionedges = = null ) {
import org . apache . giraph . utils . bytearrayvertexidedges ;
"vertices . " ) ;
return null ;
log . info ( "moveedgestovertices : no edges to move" ) ;
map < k , outedges < i , e > > partitionedges ) ;
( concurrentmap < i , outedges < i , e > > ) transientedges . get ( partitionid ) ;
protected abstract i getvertexid ( et entry , i representativevertexid ) ;
protected abstract outedges < i , e > removepartitionedges ( et entry ,
et entry = iterator . next ( ) ;
return partitionedges . put ( entry . getlongkey ( ) , null ) ;
return ( ( int2objectmap < outedges < intwritable , e > > ) partitionedges )
intwritable representativevertexid ) {
immutableclassesgiraphconfiguration < longwritable , v , e > configuration ,
if ( log . isinfoenabled ( ) ) {
protected i createvertexid ( map . entry < i , outedges < i , e > > entry ) {
import com . google . common . collect . mapmaker ;
( int2objectmap < outedges < intwritable , e > > ) partitionedgesin ;
return representativevertexid ;
while ( ( partitionid = partitionidqueue . poll ( ) ) ! = null ) {
extends abstractedgestore < intwritable , v , e , integer ,
while ( vertexidedgeiterator . hasnext ( ) ) {
public abstractedgestore (
vertexidedgeiterator . releasecurrentedge ( ) ;
public class longedgestore < v extends writable , e extends writable >
int2objectmap . entry < outedges < intwritable , e > > entry ,
import java . util . concurrent . concurrentmap ;
int partitionid ) {
for ( edge < i , e > edge : outedges ) {
protected boolean reuseedgeobjects ;
i representativevertexid = configuration . createvertexid ( ) ;
partitionedges = ( long2objectmap < outedges < longwritable , e > > )
. int2objectentryset ( )
protected outedges < longwritable , e > getvertexoutedges (
bytearrayvertexidedges < intwritable , e > . vertexidedgeiterator
transientedges . putifabsent ( partitionid , newpartitionedges ) ;
import java . util . concurrent . arrayblockingqueue ;
import java . util . concurrent . callable ;
protected iterator < long2objectmap . entry < outedges < longwritable , e > > >
centralizedserviceworker < intwritable , v , e > service ,
( int2objectmap < outedges < intwritable , e > > )
protected outedges < intwritable , e > getvertexoutedges (
map < long , outedges < longwritable , e > > partitionedges ) {
vertex = configuration . createvertex ( ) ;
public void moveedgestovertices ( ) {
return partitionedges . put ( entry . getkey ( ) , null ) ;
partition . putvertex ( vertex ) ;
intwritable vertexid = vertexidedgeiterator . getcurrentvertexid ( ) ;
protected outedges < longwritable , e > removepartitionedges (
return ;
protected concurrentmap < i , outedges < i , e > > getpartitionedges (
int numthreads = configuration . getnuminputsplitsthreads ( ) ;
v extends writable , e extends writable , k , et >
map < k , outedges < i , e > > partitionedges = getpartitionedges ( partitionid ) ;
long2objectmaps . synchronize (
extends abstractedgestore < i , v , e , i ,
} ;
map < integer , outedges < intwritable , e > > partitionedges ) {
outedges < i , e > outedges = getvertexoutedges ( vertexidedgeiterator ,
import org . apache . giraph . utils . callablefactory ;
outedges = newoutedges ;
protected outedges < i , e > getvertexoutedges (
implements edgestore < i , v , e > {
outedges = partitionedges . putifabsent ( vertexid , newoutedges ) ;
import org . apache . giraph . edge . outedges ;
map < integer , outedges < intwritable , e > > partitionedgesin ) {
public void call ( ) throws exception {
protected intwritable getvertexid (
protected boolean useinputoutedges ;
representativevertexid . set ( entry . getlongkey ( ) ) ;
protected int2objectmap < outedges < intwritable , e > > getpartitionedges (
progressableutils . getresultswithncallables ( callablefactory , numthreads ,
removepartitionedges ( entry , partitionedges ) ) ;
transientedges . get ( partitionid ) ;
extendeddatainput = conf . createextendeddatainput ( extendeddataoutput ) ;
immutableclassesgiraphconfiguration < i , ? , ? > conf ) {
}
"with null arguments" ) ;
return releasedvertexid ;
throw new illegalstateexception ( "cannot instantiate vertexiditerator " +
i releasedvertexid = vertexid ;
} else {
extendeddataoutput extendeddataoutput ,
return vertexid ;
protected final extendeddatainput extendeddatainput ;
return extendeddatainput . available ( ) > 0 ;
public i getcurrentvertexid ( ) {
import org . apache . giraph . conf . immutableclassesgiraphconfiguration ;
if ( extendeddataoutput ! = null & & conf ! = null ) {
@ override
public abstract class bytestructvertexiditerator < i extends writablecomparable >
implements vertexiditerator < i > {
package org . apache . giraph . utils ;
public boolean hasnext ( ) {
public bytestructvertexiditerator (
import org . apache . hadoop . io . writablecomparable ;
public i releasecurrentvertexid ( ) {
protected i vertexid ;
vertexid = null ;
inputsplitpathlist , getworkerinfo ( ) . gethostname ( ) ,
entriesloaded + = result ;
localdata = new localdata < > ( conf ) ;
translateedge = getconfiguration ( ) . edgetranslationinstance ( ) ;
private final localdata < i , v , e , ? extends writable > localdata ;
inputsplitpaths inputsplitpaths ,
return localdata ;
" threads ( s ) for " + inputsplitpathlist . size ( ) + " total splits . " ) ;
numthreads , "load - mapping - % d" , getcontext ( ) ) ;
}
int entriesloaded = 0 ;
mappinginputsplitsevents ) ;
return translateedge ;
log . debug ( "startsuperstep : addressesandpartitions" +
if ( log . isdebugenabled ( ) ) {
entriesloaded = loadmapping ( ) ;
inputsplitpathorganizer splitorganizer =
entriesloaded + " entries from inputsplits" ) ;
localdata . getmappingstore ( ) . postfilling ( ) ;
localdata . removemappingstoreifpossible ( ) ;
} catch ( interruptedexception e ) {
"originally " + getconfiguration ( ) . getnuminputsplitsthreads ( ) +
markcurrentworkerdonethenwaitforothers ( edgeinputsplitspaths ,
try {
false , false , true ) ;
markcurrentworkerdonethenwaitforothers ( mappinginputsplitspaths ,
progressableutils . getresultswithncallables (
getzkext ( ) . getchildrenext ( mappinginputsplitspaths . getpath ( ) ,
getcontext ( ) ,
int numthreads = math . min ( getconfiguration ( ) . getnuminputsplitsthreads ( ) ,
interruptedexception {
public localdata < i , v , e , ? extends writable > getlocaldata ( ) {
getgraphpartitionerfactory ( ) . initialize ( localdata ) ;
getconfiguration ( ) . useinputsplitlocality ( ) ) ;
list < integer > results =
splitorganizer ,
if ( getconfiguration ( ) . hasmappinginputformat ( ) ) {
inputsplitevents inputsplitevents ) {
"setup : loadmapping failed with keeperexception" , e ) ;
getcontext ( ) . progress ( ) ;
return entriesloaded ;
getzkext ( ) ) ;
edgeinputsplitsevents ) ;
this ,
getconfiguration ( ) ,
log . info ( "setup : finally loaded a total of " +
localdata . printstats ( ) ;
private final translateedge < i , e > translateedge ;
} catch ( keeperexception e ) {
addressesandpartitions . getworkerinfos ( ) ) ;
mappinginputsplitscallablefactory ,
mappinginputsplitscallablefactory < i , v , e , ? extends writable >
"setup : loadmapping failed with interruptedexception" , e ) ;
throw new illegalstateexception (
"markcurrentworkerdonethenwaitforothers : " +
if ( translateedge ! = null ) {
log . info ( "loadinputsplits : using " + numthreads + " thread ( s ) , " +
vertexinputsplitsevents ) ;
partitionowner . getworkerinfo ( ) ) ;
public translateedge < i , e > gettranslateedge ( ) {
. getpartitionowners ( ) ) {
if ( log . isinfoenabled ( ) ) {
maxinputsplitthreads ) ;
markcurrentworkerdonethenwaitforothers ( vertexinputsplitspaths ,
mappinginputsplitscallablefactory =
translateedge . initialize ( this ) ;
private void markcurrentworkerdonethenwaitforothers (
for ( partitionowner partitionowner : addressesandpartitions
new inputsplitpathorganizer ( getzkext ( ) ,
list < string > inputsplitpathlist =
int maxinputsplitthreads = inputsplitpathlist . size ( ) ;
for ( integer result : results ) {
log . debug ( partitionowner . getpartitionid ( ) + " " +
ensureinputsplitsready ( mappinginputsplitspaths , mappinginputsplitsevents ) ;
import org . apache . giraph . mapping . translate . translateedge ;
new mappinginputsplitscallablefactory < > (
private integer loadmapping ( ) throws keeperexception ,
getconfiguration ( ) . createwrappedmappinginputformat ( ) ,
private static final int mb = 1024 * 1024 ;
public void applicationfailed ( exception e ) { }
stop = false ;
private int sleepmillis ;
thread . setname ( "reactivejmaphistodumpersupervisorthread" ) ;
defaultimmutableclassesgiraphconfigurable implements
private volatile boolean stop = false ;
private void joinsupervisorthread ( ) {
private static final logger log = logger . getlogger (
import org . apache . log4j . logger ;
}
public void postsuperstep ( long superstep ) { }
log . error ( "failed to join jmap thread" ) ;
jmap . heaphistogramdump ( linestoprint ) ;
masterobserver , workerobserver {
thread . join ( sleepmillis + 5000 ) ;
import org . apache . giraph . conf . giraphconstants ;
} ) ;
public void startsupervisorthread ( ) {
if ( potentialmemory / mb < minfreememory ) {
joinsupervisorthread ( ) ;
linestoprint = giraphconstants . jmap print lines . get ( configuration ) ;
startsupervisorthread ( ) ;
import org . apache . giraph . conf . defaultimmutableclassesgiraphconfigurable ;
public void run ( ) {
public void presuperstep ( long superstep ) { }
stop = true ;
private int linestoprint ;
public void setconf ( immutableclassesgiraphconfiguration configuration ) {
public class reactivejmaphistodumper extends
import org . apache . giraph . conf . immutableclassesgiraphconfiguration ;
thread . start ( ) ;
runtime . totalmemory ( ) ) + runtime . freememory ( ) ;
import org . apache . giraph . master . masterobserver ;
log . warn ( "jmap histogram sleep interrupted" , e ) ;
thread = new thread ( new runnable ( ) {
import org . apache . giraph . worker . workerobserver ;
long potentialmemory = ( runtime . maxmemory ( ) -
@ override
private int minfreememory ;
} catch ( interruptedexception e ) {
minfreememory = giraphconstants . min free mbs on heap . get ( configuration ) ;
sleepmillis = giraphconstants . jmap sleep millis . get ( configuration ) ;
package org . apache . giraph . utils ;
public void postapplication ( ) {
final runtime runtime = runtime . getruntime ( ) ;
private thread thread ;
public void preapplication ( ) {
reactivejmaphistodumper . class ) ;
thread . sleep ( sleepmillis ) ;
while ( !stop ) {
try {
}
@ override
taskinfo mytaskinfo ,
if ( future . isdone ( ) & & !future . issuccess ( ) ) {
final thread . uncaughtexceptionhandler exceptionhandler ) {
private static class logonerrorchannelfuturelistener
writefuture . addlistener ( logerrorlistener ) ;
implements channelfuturelistener {
private final logonerrorchannelfuturelistener logerrorlistener =
"netty - client - worker - % d" , exceptionhandler ) ) ;
threadutils . createthreadfactory (
new logonerrorchannelfuturelistener ( ) ;
log . error ( "request failed" , future . cause ( ) ) ;
"netty - client - exec - % d" , exceptionhandler ) ) ;
import org . apache . giraph . utils . threadutils ;
public void operationcomplete ( channelfuture future ) throws exception {
partition < i , v , e > partition =
getpartitionstore ( ) . putpartition ( partition ) ;
partitionowner partitionowner = new basicpartitionowner ( partitionid ,
( numpartitions = = 0 ) ? new linkedlist < integer > ( ) :
int mrtaskid = finalizedstream . readint ( ) ;
finalizedoutputstream . writeint ( chosenworkerinfo . gettaskid ( ) ) ;
getfs ( ) . open ( metadatafilepath ) ;
checkpointoutputstream . writeint ( partitionid ) ;
fsdataoutputstream checkpointoutputstream =
giraphconstants . num checkpoint io threads . get ( getconfiguration ( ) ) ,
finalizedstream . close ( ) ;
public int compare ( partitionowner p1 , partitionowner p2 ) {
getconfiguration ( ) . createpartition ( partitionid , getcontext ( ) ) ;
}
metadataoutputstream . writeint ( partitionid ) ;
public callable < void > newcallable ( int callableid ) {
maxworkers ,
workerinfo worker = workersmap . get ( mrtaskid ) ;
for ( int i = 0 ; i < partitions ; i + + ) {
finalizedstream . readutf ( ) ;
checkpointoutputstream . close ( ) ;
} else {
new dataoutputstream (
import org . apache . hadoop . io . compress . compressioncodec ;
list < partitionowner > partitionowners = new arraylist < > ( ) ;
getserverdata ( ) . resetmessagestores ( ) ;
allpartitionstatslist ) ;
if ( getfs ( ) . delete ( validfilepath , false ) ) {
aggregatorhandler . readfields ( finalizedstream ) ;
private void assignpartitionowners ( ) {
import java . util . concurrent . concurrentlinkedqueue ;
throw new runtimeexception (
final compressioncodec codec =
return new callable < void > ( ) {
} catch ( interruptedexception e ) {
log . info ( "loaded checkpoint in " + ( system . currenttimemillis ( ) - t0 ) +
new datainputstream (
return new path ( getsavedcheckpointbasepath ( superstep ) + " . " +
datainputstream stream = codec = = null ? compressedstream :
"zkdeletenode : interruptedexception" , e ) ;
try {
"zkdeletenode : keeperexception" , e ) ;
" . " + mrtaskid + checkpoint metadata postfix ) ) ;
new compressioncodecfactory ( getconfiguration ( ) )
assignpartitionowners ( ) ;
for ( int i = 0 ; i < prefixfilecount ; + + i ) {
path validfilepath = new path ( getcheckpointbasepath ( getsuperstep ( ) ) + " . " +
new concurrentlinkedqueue < > ( partitions ) ;
collections . sort ( partitionowners , new comparator < partitionowner > ( ) {
int partitionid = checkpointstream . readint ( ) ;
private path createcheckpointfilepathsafe ( string name ) throws ioexception {
getfs ( ) . open ( path ) ;
iterables . addall ( partitionidqueue , getpartitionstore ( ) . getpartitionids ( ) ) ;
int partitionid = metadatastream . readint ( ) ;
import it . unimi . dsi . fastutil . ints . int2objectopenhashmap ;
getsuperstep ( ) ) ;
dataoutputstream stream = codec = = null ? uncompressedstream :
"loadcheckpoint : failed for superstep = " + superstep , e ) ;
integer partitionid = partitionidqueue . poll ( ) ;
workergraphpartitioner . getpartitionowners ( ) . size ( ) +
for ( workerinfo worker : chosenworkerinfolist ) {
} ) ;
datainputstream checkpointstream =
codec . createoutputstream ( uncompressedstream ) ) ;
log . info ( "loadcheckpoint : loaded " +
mastergraphpartitioner . generatechangedpartitionowners (
private void storecheckpointvertices ( ) {
fsdatainputstream compressedstream =
" total . " ) ;
workerclient . setup ( getconfiguration ( ) . authenticate ( ) ) ;
string checkpointfile =
uncompressedstream . close ( ) ;
import it . unimi . dsi . fastutil . ints . int2objectmap ;
createcheckpointfilepathsafe ( checkpoint valid postfix ) ;
giraphconstants . checkpoint compression codec
return new vertexedgecount ( globalstats . getvertexcount ( ) ,
final int numpartitions = getpartitionstore ( ) . getnumpartitions ( ) ;
if ( partitionid = = null ) {
storecheckpointvertices ( ) ;
gettaskpartition ( ) + name ) ;
getcontext ( ) . progress ( ) ;
list < integer > partitionids = new arraylist < > ( partitions ) ;
import org . apache . hadoop . fs . fsdatainputstream ;
superstepclasses superstepclasses = new superstepclasses ( ) ;
break ;
if ( getsuperstep ( ) = = input superstep ) {
path metadatafilepath =
numpartitions ) ;
} catch ( keeperexception . nonodeexception e ) {
list < integer > partitions ) {
getfs ( ) . liststatus ( new path ( savedcheckpointbasepath ) ,
fsdataoutputstream uncompressedstream =
"load - vertices - % d" , getcontext ( ) ) ;
validfilepath ) ;
private void loadcheckpointvertices ( final long superstep ,
datainputstream metadatastream =
partitions . size ( ) ) ;
mastergraphpartitioner . setpartitionowners ( partitionowners ) ;
getsavedcheckpointbasepath ( superstep ) + checkpoint finalized postfix ;
getpartitionstore ( ) . getorcreatepartition ( partitionid ) ;
final queue < integer > partitionidqueue =
import org . apache . giraph . partition . basicpartitionowner ;
int partitions = metadatastream . readint ( ) ;
} catch ( keeperexception e ) {
private collection < partitionowner > preparecheckpointrestart ( long superstep )
createcheckpointfilepathsafe ( checkpoint metadata postfix ) ;
checkpoint vertices postfix ) ;
getserverdata ( ) . getcurrentmessagestore ( ) . readfieldsforpartition (
getsavedcheckpoint ( superstep , " " + partitionid +
log . info ( "loading checkpoint from " + finalizedcheckpointpath ) ;
"checkpoint - vertices - % d" , getcontext ( ) ) ;
workercontext . readfields ( checkpointstream ) ;
allpartitionstatslist ,
return partitionowners ;
metadatastream . close ( ) ;
createcheckpointfilepathsafe ( checkpoint data postfix ) ;
getserverdata ( ) . getcurrentmessagestore ( ) . writepartition (
callablefactory < void > callablefactory = new callablefactory < void > ( ) {
getsavedcheckpoint ( superstep , checkpoint metadata postfix ) ;
import org . apache . hadoop . io . compress . compressioncodecfactory ;
private void zkdeletenode ( string path ) {
getfs ( ) . open ( checkpointfilepath ) ;
log . info ( "setjobstate : " + jobstate ) ;
zkdeletenode ( vertexinputsplitspaths . getpath ( ) ) ;
createcheckpointfilepathsafe ( " " + partitionid +
} catch ( ioexception e ) {
getworkerinfo ( ) , mastersetpartitionowners ) ;
@ override
datainputstream metadatastream = fs . open ( new path ( checkpointfile +
chosenworkerinfolist ,
getzkext ( ) . deleteext ( path , - 1 , true ) ;
return null ;
log . info ( "save checkpoint in " + ( system . currenttimemillis ( ) - t0 ) +
globalstats . getedgecount ( ) ) ;
for ( long p = 0 ; p < partitions ; + + p ) {
" assigned to " + partitionowner ) ;
checkpointstream , partitionid ) ;
partition . write ( stream ) ;
if ( log . isinfoenabled ( ) ) {
int2objectmap < workerinfo > workersmap = new int2objectopenhashmap < > ( ) ;
return validfilepath ;
getfs ( ) . create ( path ) ;
partitionutils . analyzepartitionstats ( partitionowners ,
" ms , using " + numthreads + " threads" ) ;
codec . createinputstream ( compressedstream ) ) ;
log . info ( "zkdeletenode : node has already been removed " + path ) ;
. getcodecbyclassname (
partition . readfields ( stream ) ;
long t0 = system . currenttimemillis ( ) ;
workercontext . write ( checkpointoutputstream ) ;
log . warn ( "storecheckpoint : removed " + name + " file " +
int numthreads = math . min (
finalizedoutputstream . writeutf ( getcheckpointbasepath ( superstep ) ) ;
getpartitionstore ( ) . addpartition ( partition ) ;
path checkpointfilepath =
partitionowners = preparecheckpointrestart ( getsuperstep ( ) ) ;
worker ) ;
globalstats globalstats = new globalstats ( ) ;
getfs ( ) . create ( checkpointfilepath ) ;
zkdeletenode ( edgeinputsplitspaths . getpath ( ) ) ;
partitionowners . add ( partitionowner ) ;
log . info ( "preparecheckpointrestart partitionid = " + partitionid +
mastercompute . readfields ( finalizedstream ) ;
string finalizedcheckpointpath =
partitionids . add ( partitionid ) ;
private path getsavedcheckpoint ( long superstep , string name ) {
stream . close ( ) ;
} else if ( getrestartedsuperstep ( ) = = getsuperstep ( ) ) {
loadcheckpointvertices ( superstep , partitionids ) ;
. get ( getconfiguration ( ) ) ) ;
new arrayblockingqueue < integer > ( numpartitions ) ;
workersmap . put ( worker . gettaskid ( ) , worker ) ;
} ;
while ( !partitionidqueue . isempty ( ) ) {
checkpartitions ( mastergraphpartitioner . getcurrentpartitionowners ( ) ) ;
checkpointoutputstream , partitionid ) ;
checkpointstream . close ( ) ;
path path =
partitionowners =
public void call ( ) throws exception {
return integer . compare ( p1 . getpartitionid ( ) , p2 . getpartitionid ( ) ) ;
getsavedcheckpoint ( superstep , checkpoint data postfix ) ;
progressableutils . getresultswithncallables ( callablefactory , numthreads ,
for ( integer partitionid : getpartitionstore ( ) . getpartitionids ( ) ) {
public void writepartition ( dataoutput out , int partitionid )
. setnameformat ( "asyncmessagestorewrapper - % d" ) . build ( ) ) ;
import org . apache . giraph . comm . messages . messagestore ;
store . clearall ( ) ;
import java . util . concurrent . linkedblockingqueue ;
import org . apache . hadoop . io . writable ;
public void finalizestore ( ) {
blockingqueue < partitionmessage < i , m > > queue ) {
import org . apache . log4j . logger ;
}
store . readfieldsforpartition ( in , partitionid ) ;
private semaphore completionsemaphore ;
for ( blockingqueue < partitionmessage < i , m > > queue : queues ) {
if ( message . getmessage ( ) ! = null ) {
return store . hasmessagesforvertex ( vertexid ) ;
new partitionmessage ( - 1 , null ) ;
private class messagestorequeueworker implements runnable {
} else {
completionsemaphore . acquire ( ) ;
executor service . submit ( new messagestorequeueworker ( queues [ i ] ) ) ;
private final messagestore < i , m > store ;
import java . io . ioexception ;
store . finalizestore ( ) ;
completionsemaphore . release ( ) ;
import it . unimi . dsi . fastutil . ints . int2intarraymap ;
private static final logger log =
completionsemaphore = new semaphore ( 1 - threadscount ) ;
} catch ( interruptedexception e ) {
public boolean ispointerlistencoding ( ) {
import java . io . dataoutput ;
logger . getlogger ( asyncmessagestorewrapper . class ) ;
message = queue . take ( ) ;
import java . util . concurrent . semaphore ;
try {
import java . util . concurrent . executorservice ;
return store . getvertexmessages ( vertexid ) ;
store . writepartition ( out , partitionid ) ;
public boolean hasmessagesforvertex ( i vertexid ) {
import java . util . concurrent . executors ;
executors . newcachedthreadpool (
private static final executorservice executor service =
public void clearvertexmessages ( i vertexid ) throws ioexception {
private static final partitionmessage clear queue message =
throw new runtimeexception ( e ) ;
import it . unimi . dsi . fastutil . ints . int2intmap ;
import org . apache . giraph . utils . vertexidmessages ;
int cnt = 0 ;
private final int threadscount ;
while ( true ) {
public void readfieldsforpartition ( datainput in , int partitionid )
public void addpartitionmessages (
log . info ( "asyncmessagestorewrapper enabled . threads = " + threadscount ) ;
import org . apache . hadoop . io . writablecomparable ;
this . store = store ;
iterable < integer > partitions ,
private final blockingqueue < partitionmessage < i , m > > queue ;
private final blockingqueue < partitionmessage < i , m > > [ ] queues ;
store . clearpartition ( partitionid ) ;
package org . apache . giraph . comm . messages . queue ;
throws ioexception {
int threadcount ) {
private static final partitionmessage shutdown queue message =
public iterable < i > getpartitiondestinationvertices ( int partitionid ) {
import java . util . concurrent . blockingqueue ;
int hash = partition2queue . get ( partitionid ) ;
for ( int partitionid : partitions ) {
log . error ( "messagestorequeueworker . run : " + message , e ) ;
public iterable < m > getvertexmessages ( i vertexid ) throws ioexception {
public void run ( ) {
this . threadscount = threadcount ;
import java . io . datainput ;
@ override
queue . put ( clear queue message ) ;
queues = new blockingqueue [ threadscount ] ;
private final int2intmap partition2queue ;
new threadfactorybuilder ( )
this . queue = queue ;
queues [ i ] = new linkedblockingqueue < > ( ) ;
m extends writable > implements messagestore < i , m > {
public void waittocomplete ( ) {
public void clearall ( ) throws ioexception {
queues [ hash ] . put ( new partitionmessage < > ( partitionid , messages ) ) ;
partition2queue . put ( partitionid , cnt + + % threadscount ) ;
return store . getpartitiondestinationvertices ( partitionid ) ;
queue . put ( shutdown queue message ) ;
for ( int i = 0 ; i < threadscount ; i + + ) {
} catch ( ioexception | interruptedexception e ) {
return store . ispointerlistencoding ( ) ;
partition2queue = new int2intarraymap ( ) ;
public void clearpartition ( int partitionid ) throws ioexception {
if ( message = = shutdown queue message ) {
import com . google . common . util . concurrent . threadfactorybuilder ;
public final class asyncmessagestorewrapper < i extends writablecomparable ,
return ;
public asyncmessagestorewrapper ( messagestore < i , m > store ,
partitionmessage < i , m > message = null ;
int partitionid = message . getpartitionid ( ) ;
store . addpartitionmessages ( partitionid , message . getmessage ( ) ) ;
store . clearvertexmessages ( vertexid ) ;
int partitionid , vertexidmessages < i , m > messages ) throws ioexception {
private messagestorequeueworker (
public static final string converged = "scccompute . converged" ;
booleanwritable converged = getaggregatedvalue ( converged ) ;
backward traversal start , backward traversal rest
import org . apache . giraph . aggregators . booleanoverwriteaggregator ;
public static final string new maximum = "scccompute . max" ;
import org . apache . hadoop . io . booleanwritable ;
case forward traversal :
import org . apache . hadoop . io . intwritable ;
}
default :
private phases getphase ( ) {
import org . apache . giraph . aggregators . intoverwriteaggregator ;
case backward traversal rest :
switch ( currphase ) {
setphase ( phases . backward traversal rest ) ;
import org . apache . giraph . master . defaultmastercompute ;
public void initialize ( ) throws instantiationexception ,
setphase ( phases . transpose ) ;
} else {
case backward traversal start :
if ( getsuperstep ( ) = = 0 ) {
public static final string phase = "scccompute . phase" ;
booleanwritable newmaxfound = getaggregatedvalue ( new maximum ) ;
setphase ( phases . backward traversal start ) ;
setaggregatedvalue ( phase , new intwritable ( phase . ordinal ( ) ) ) ;
case trimming :
package org . apache . giraph . examples . scc ;
public void compute ( ) {
phases currphase = getphase ( ) ;
case transpose :
public enum phases {
registerpersistentaggregator ( phase , intoverwriteaggregator . class ) ;
setphase ( phases . forward traversal ) ;
if ( !converged . get ( ) ) {
private void setphase ( phases phase ) {
public static phases getphase ( intwritable phaseint ) {
@ override
registeraggregator ( converged , booleanoverwriteaggregator . class ) ;
return getphase ( phaseint ) ;
registeraggregator ( new maximum , booleanoverwriteaggregator . class ) ;
} ;
illegalaccessexception {
setphase ( phases . trimming ) ;
transpose , trimming ,
return phases . values ( ) [ phaseint . get ( ) ] ;
forward traversal ,
break ;
public class sccphasemastercompute extends defaultmastercompute {
intwritable phaseint = getaggregatedvalue ( phase ) ;
if ( !newmaxfound . get ( ) ) {
aggregatorreduceoperation < writable > cleanreduceop =
out . writeint ( registeredaggregators . size ( ) ) ;
( aggregatorwrapper < a > ) registeredaggregators . get ( name ) ;
import org . apache . hadoop . io . writable ;
private aggregatorreduceoperation < a > reduceop ;
entry . getvalue ( ) . createreduceop ( ) ;
}
( string name , writablefactory < ? extends aggregator < a > > aggregatorfactory ,
extends defaultimmutableclassesgiraphconfigurable
import org . apache . giraph . aggregators . aggregator ;
} else {
public void readfields ( datainput in ) throws ioexception {
import java . io . ioexception ;
globalcomm . getreduced ( entry . getkey ( ) ) ;
implements writable {
reduceop . readfields ( in ) ;
globalcomm . registerreduce (
reduceop . write ( out ) ;
import com . google . common . base . preconditions ;
public aggregatortoglobalcommtranslation ( masterglobalcommusage globalcomm ) {
import java . io . dataoutput ;
private final masterglobalcommusage globalcomm ;
return persistent ;
string name = in . readutf ( ) ;
public aggregatorreduceoperation < a > createreduceop ( ) {
globalcomm . broadcast ( entry . getkey ( ) , value ) ;
agg . readfields ( in ) ;
reduceop = new aggregatorreduceoperation < > ( ) ;
preconditions . checkstate ( currentvalue = = null , "aggregatorwrapper " +
import java . util . hashmap ;
public aggregatorreduceoperation < a > getreduceop ( ) {
persistent = in . readboolean ( ) ;
entry . getkey ( ) , cleanreduceop , value ) ;
boolean persistent ) throws instantiationexception ,
private < a extends writable > aggregatorwrapper < a > registeraggregator
value = entry . getvalue ( ) . getreduceop ( ) . createinitialvalue ( ) ;
return registeraggregator ( name , aggregatorfactory , true ) ! = null ;
this . persistent = persistent ;
boolean persistent ) {
currentvalue = null ;
if ( aggregatorwrapper = = null ) {
out . writeutf ( entry . getkey ( ) ) ;
return registeraggregator ( name , aggregatorfactory , false ) ! = null ;
registeredaggregators . put (
return aggregatorwrapper ;
import org . apache . giraph . utils . writablefactory ;
public void postmastercompute ( ) {
"shouldn't have value at the end of the superstep" ) ;
public < a extends writable > void setaggregatedvalue ( string name , a value ) {
aggregatorwrapper < a > aggregatorwrapper =
entry . getvalue ( ) . write ( out ) ;
return registeraggregator ( name , aggregator , false ) ! = null ;
entry . getvalue ( ) . getcurrentvalue ( ) :
import org . apache . giraph . conf . defaultimmutableclassesgiraphconfigurable ;
out . writeboolean ( persistent ) ;
name , ( aggregatorwrapper < writable > ) aggregatorwrapper ) ;
public class aggregatortoglobalcommtranslation
this . currentvalue = currentvalue ;
if ( value = = null ) {
return reduceop . createcopy ( ) ;
import java . io . datainput ;
import java . util . map . entry ;
class < ? extends aggregator < a > > aggregatorclass ) throws
new aggregatorwrapper < a > ( aggregatorfactory , persistent ) ;
return globalcomm . getreduced ( name ) ;
entry . getvalue ( ) . setcurrentvalue ( null ) ;
registeredaggregators . entryset ( ) ) {
this . globalcomm = globalcomm ;
for ( int i = 0 ; i < numaggregators ; i + + ) {
private boolean persistent ;
@ override
new classaggregatorfactory < a > ( aggregatorclass ) ;
writablefactory < ? extends aggregator < a > > aggregatorfactory ,
classaggregatorfactory < a > aggregatorfactory =
public < a extends writable > a getaggregatedvalue ( string name ) {
aggregatorwrapper =
public < a extends writable > boolean registerpersistentaggregator ( string name ,
import org . apache . giraph . aggregators . classaggregatorfactory ;
public aggregatorwrapper ( ) {
public void write ( dataoutput out ) throws ioexception {
aggregatorwrapper < writable > aggregator = registeredaggregators . get ( name ) ;
registeredaggregators . clear ( ) ;
aggregatorwrapper < writable > agg = new aggregatorwrapper < > ( ) ;
entry . getkey ( ) , cleanreduceop ) ;
aggregator . setcurrentvalue ( value ) ;
public boolean ispersistent ( ) {
this . reduceop = new aggregatorreduceoperation < > ( aggregatorfactory ) ;
public < a extends writable > boolean registeraggregator ( string name ,
public void setcurrentvalue ( a currentvalue ) {
public aggregatorwrapper (
implements masteraggregatorusage , writable {
package org . apache . giraph . master ;
registeredaggregators = new hashmap < > ( ) ;
if ( entry . getvalue ( ) . ispersistent ( ) ) {
instantiationexception , illegalaccessexception {
writablefactory < ? extends aggregator < a > > aggregator ) throws
private static class aggregatorwrapper < a extends writable >
private a currentvalue ;
return reduceop ;
return currentvalue ;
private final hashmap < string , aggregatorwrapper < writable > >
writable value = entry . getvalue ( ) . currentvalue ! = null ?
registeredaggregators . put ( name , agg ) ;
public a getcurrentvalue ( ) {
for ( entry < string , aggregatorwrapper < writable > > entry :
int numaggregators = in . readint ( ) ;
illegalaccessexception {
" . " + mrtaskid + checkpointingutils . checkpoint metadata postfix ) ) ;
long firstcheckpoint = input superstep + 1 + checkpointfrequency ;
string finalizedcheckpointpath = getsavedcheckpointbasepath ( superstep ) +
checkpointingutils . checkpoint finalized postfix ) ;
checkpointingutils . checkpoint finalized postfix ;
. getsnapretaincount ( ) , quorumpeerconfig . getpurgeinterval ( ) ) ;
runfromconfig ( quorumpeerconfig ) ;
datadircleanupmanager purgemgr = new datadircleanupmanager (
serverconfig serverconfig = new serverconfig ( ) ;
quorumpeerconfig
serverrunner . stop ( ) ;
package org . apache . giraph . zk ;
logger . getlogger ( inprocesszookeeperrunner . class ) ;
public void stop ( ) {
log . warn ( "neither quorum nor server is set" ) ;
" in standalone mode" ) ;
serverrunner . start ( configfilepath ) ;
serverconfig . parse ( configfilepath ) ;
quorumpeer . shutdown ( ) ;
} else if ( serverrunner ! = null ) {
log . error ( "unable to start zookeeper" , e ) ;
private quorumrunner quorumrunner = new quorumrunner ( ) ;
import org . apache . log4j . logger ;
} catch ( jmexception e ) {
}
private static class quorumrunner extends quorumpeermain {
extends defaultimmutableclassesgiraphconfigurable
import org . apache . zookeeper . server . quorum . quorumpeermain ;
zkthread . setdaemon ( true ) ;
log . info ( "initialization ended" ) ;
private static class zookeeperserverrunner extends zookeeperservermain {
if ( quorumpeerconfig . getservers ( ) . size ( ) > 0 ) {
quorumpeerconfig . parse ( configfilepath ) ;
log . error ( "invalid config , zookeeper failed" , e ) ;
. getdatadir ( ) , quorumpeerconfig . getdatalogdir ( ) , quorumpeerconfig
quorumpeer . join ( ) ;
import org . apache . zookeeper . jmx . managedutil ;
log . warn ( "unable to register log4j jmx control" , e ) ;
} ) ;
log . error ( "unable to cleanly shutdown zookeeper" , e ) ;
quorumrunner . stop ( ) ;
} else {
if ( quorumpeer ! = null ) {
quorumpeerconfig . configexception , ioexception {
runfromconfig ( serverconfig ) ;
import org . apache . zookeeper . server . zookeeperservermain ;
import org . apache . giraph . conf . defaultimmutableclassesgiraphconfigurable ;
public void start ( string configfilepath ) throws ioexception ,
public void run ( ) {
quorumpeerconfig quorumpeerconfig = new quorumpeerconfig ( ) ;
import javax . management . jmexception ;
import org . apache . zookeeper . server . serverconfig ;
serverrunner = new zookeeperserverrunner ( ) ;
import java . io . ioexception ;
quorumrunner . start ( configfilepath ) ;
managedutil . registerlog4jmbeans ( ) ;
implements zookeeperrunner {
} catch ( quorumpeerconfig . configexception e ) {
import org . apache . zookeeper . server . datadircleanupmanager ;
import org . apache . zookeeper . server . quorum . quorumpeerconfig ;
private zookeeperserverrunner serverrunner ;
} catch ( ioexception e ) {
private static final logger log =
@ override
public void cleanup ( ) {
} catch ( interruptedexception e ) {
public void start ( string zkdir , final string configfilepath ) {
log . warn ( "either no config or no quorum defined in config , running " +
thread zkthread = new thread ( new runnable ( ) {
public class inprocesszookeeperrunner
quorumpeerconfig . configexception {
public void start ( string configfilepath ) throws
zkthread . start ( ) ;
shutdown ( ) ;
public void stop ( ) throws interruptedexception {
purgemgr . start ( ) ;
try {
thread . setdaemon ( true ) ;
writerthread . setdaemon ( true ) ;
} , "workerprogressthread" ) ;
log . info ( "jmap histogram sleep interrupted" , e ) ;
private volatile boolean stop = false ;
thread . interrupt ( ) ;
private static final int memory observer sleep ms = 1000 ;
private final atomiclong lastmanualgc = new atomiclong ( ) ;
import org . apache . log4j . logger ;
}
if ( msfromlastgc > minmsbetweenfullgcs & &
long msfromlastgc = system . currenttimemillis ( ) - lastmanualgc . get ( ) ;
package org . apache . giraph . worker ;
new byte [ 0 ] ,
new intconfoption ( "giraph . memoryobserver . minmsbetweenfullgcs" , 60 * 1000 ,
import org . apache . giraph . conf . giraphconfiguration ;
public class memoryobserver {
import org . apache . giraph . conf . floatconfoption ;
} catch ( interruptedexception e ) {
if ( !use memory observer . get ( conf ) ) {
try {
import org . apache . giraph . zk . zookeeperext ;
lastmanualgc . compareandset ( last , system . currenttimemillis ( ) ) ) {
private void setwatcher ( ) {
long last = lastmanualgc . get ( ) ;
import org . apache . zookeeper . keeperexception ;
} ) ;
false ) ;
log . info ( "calling gc manually" ) ;
system . gc ( ) ;
import org . apache . giraph . conf . intconfoption ;
zk . getchildrenext ( zkpath , true , false , false ) ;
free memory fraction for gc . get ( conf ) ;
import org . apache . giraph . conf . booleanconfoption ;
while ( true ) {
new booleanconfoption ( "giraph . memoryobserver . enabled" , false ,
final string zkpath , giraphconfiguration conf ) {
import org . apache . giraph . utils . memoryutils ;
private final zookeeperext zk ;
thread thread = new thread ( new runnable ( ) {
public void callgc ( ) {
minmsbetweenfullgcs = min ms between full gcs . get ( conf ) ;
final float freememoryfractionforgc =
thread . sleep ( memory observer sleep ms ) ;
public static final floatconfoption free memory fraction for gc =
createmode . ephemeral ,
double freememoryfraction = memoryutils . freememoryfraction ( ) ;
"for which fraction of free memory will we issue manual gc calls" ) ;
this . zk = zk ;
public void run ( ) {
createmode . persistent , true ) ;
@ override
} catch ( keeperexception | interruptedexception e ) {
setwatcher ( ) ;
"minimum milliseconds between two manual gc calls" ) ;
private final string zkpath ;
public static final booleanconfoption use memory observer =
if ( log . isinfoenabled ( ) ) {
freememoryfraction + " % free ) " ) ;
public static final intconfoption min ms between full gcs =
private static final logger log = logger . getlogger ( memoryobserver . class ) ;
new floatconfoption ( "giraph . memoryobserver . freememoryfractionforgc" , 0 . 1f ,
public memoryobserver ( final zookeeperext zk ,
import java . util . concurrent . atomic . atomiclong ;
zoodefs . ids . open acl unsafe ,
zk . createonceext ( zkpath , null , zoodefs . ids . open acl unsafe ,
freememoryfraction < freememoryfractionforgc ) {
log . info ( "exception occurred" , e ) ;
thread . setname ( "memory - observer" ) ;
if ( system . currenttimemillis ( ) - last > minmsbetweenfullgcs & &
import org . apache . zookeeper . zoodefs ;
log . warn ( "exception occurred" , e ) ;
import org . apache . zookeeper . createmode ;
return ;
private final int minmsbetweenfullgcs ;
"whether or not to use memory observer" ) ;
thread . setdaemon ( true ) ;
zk . createext (
this . zkpath = zkpath ;
log . info ( "manual gc call done" ) ;
log . info ( "notifying others about low memory ( " +
zkpath + " / " + system . currenttimemillis ( ) ,
thread . start ( ) ;
import com . google . common . collect . maps ;
import java . io . file ;
return processingstate ;
import static org . apache . giraph . conf . giraphconstants . one mb ;
if ( oldpair ! = null ) {
t entry ) ;
return false ;
while ( it . hasnext ( ) ) {
meta . setincomingmessagesstate ( storagestate . in mem ) ;
public void setpartitionstate ( storagestate state ) {
oldnumbuffersondisk + numbuffers ) ;
checkstate ( file . delete ( ) , "loadpartitiondata : failed to delete % s . " ,
metapartition temp = partitions . putifabsent ( partitionid , meta ) ;
return numinmemorypartitions . get ( ) ;
metapartition meta = new metapartition ( partitionid ) ;
numpartitionsprocessed . set ( 0 ) ;
if ( log . isdebugenabled ( ) ) {
if ( messagestore ! = null ) {
checkstate ( unprocessedpartitions . get ( storagestate . in transit ) . size ( ) = =
processingstate = processingstate . unprocessed ;
metapartition meta = it . next ( ) ;
set < metapartition > partitionset = perthreadpartitions . get ( threadid )
return no partition to process ;
public void doneoffloadingbuffer ( int partitionid ) {
return entry ;
perthreadpartitions . get ( owner ) . add ( meta , storagestate . on disk ) ;
perthreadvertexedgebuffers . add ( sets . < integer > newconcurrenthashset ( ) ) ;
private final atomicinteger numpartitionsprocessed = new atomicinteger ( 0 ) ;
. getedgestore ( ) ;
sb . append ( "incoming messages : " + incomingmessagesstate + " ; " ) ;
iterator < metapartition > it = partitionset . iterator ( ) ;
import java . util . map ;
this . partitionstate = storagestate . in mem ;
partitionset = processedpartitions . get ( state ) ;
return basepath + " - p" + partitionid ;
0 ) ;
partitionset = processedpartitions . get ( storagestate . on disk ) ;
checkstate ( file . exists ( ) ) ;
it = partitionset . iterator ( ) ;
bspserviceworker . input superstep ) {
private static final logger log = logger . getlogger (
metapartition meta = partitions . get ( partitionid ) ;
( diskbackedmessagestore < ? , ? > ) ( oocengine . getserverdata ( )
checkstate ( numbuffers > 0 ) ;
return unprocessedpartitions . get ( storagestate . in mem ) ;
meta . setpartitionstate ( storagestate . in mem ) ;
return true ;
newpair . getright ( ) . add ( entry ) ;
mutablepair < integer , list < t > > newpair =
return partitions . containskey ( partitionid ) ;
perthreadmessagebuffers . add ( sets . < integer > newconcurrenthashset ( ) ) ;
public boolean startoffloadingpartition ( int partitionid ) {
rwlock . readlock ( ) . unlock ( ) ;
partitionset . add ( meta ) ;
return readwritelock ;
sets . < metapartition > newlinkedhashset ( ) ) ;
incomingmessagesstate = storagestate . in mem ;
set < metapartition > partitionset ;
partitionid = popfromset ( perthreadvertexedgebuffers . get ( threadid ) ) ;
private static string getpath ( string basepath , int partitionid ) {
public static final int no partition to process = - 1 ;
datainputstream dis = new datainputstream ( bis ) ;
this . partitionid = partitionid ;
new mutablepair < > ( entrysize , entrylist ) ;
synchronized ( meta ) {
( diskbackedpartitionstore < ? , ? , ? > ) ( oocengine . getserverdata ( )
partitionset = processedpartitions . get ( storagestate . in mem ) ;
databuffers . putifabsent ( partitionid , newpair ) ;
public boolean ispartitionondisk ( int partitionid ) {
"ul unreleased lock exception path" )
meta . setincomingmessagesstate ( storagestate . in transit ) ;
do {
checkstate ( !pair . getright ( ) . isempty ( ) ) ;
public perthreadpartitionstatus ( ) {
return currentmessagesstate ;
return meta . getpartitionid ( ) ;
public storagestate getincomingmessagesstate ( ) {
return null ;
currentmessagesstate = = storagestate . on disk ;
protected abstract int entryserializedsize ( t entry ) ;
import java . util . concurrent . atomic . atomicinteger ;
import java . util . concurrent . concurrentmap ;
public void resetmessages ( ) {
this . processingstate = processingstate ;
import static com . google . common . base . preconditions . checkstate ;
set < metapartition > partitionset = status . getinmemoryunprocessed ( ) ;
import java . util . random ;
public void addpartition ( int partitionid ) {
. getpartitionstore ( ) ) ;
public void offloadpartitiondata ( int partitionid , string basepath )
metapartition meta = peekfromset ( partitionset ) ;
it . remove ( ) ;
log . info ( "remove : partition " + meta . getpartitionid ( ) + " is " +
meta . resetpartition ( ) ;
checkstate ( numpartition = = partitions . size ( ) ) ;
return removed ;
meta . setprocessingstate ( processingstate . processed ) ;
private final atomicinteger numinmemorypartitions = new atomicinteger ( 0 ) ;
. getinmemoryprocessed ( ) ;
meta . resetmessages ( ) ;
" partition " + partitionid + " from " + file . getabsolutepath ( ) ) ;
for ( perthreadpartitionstatus status : perthreadpartitions ) {
public void offloadbuffers ( int partitionid , string basepath )
perthreadpartitions . get ( ownerthread ) . getinmemoryprocessed ( ) ;
int numthreads = perthreadpartitions . size ( ) ;
. get ( oocengine . getioscheduler ( ) . getownerthreadid ( partitionid ) )
partitionset = unprocessedpartitions . get ( storagestate . on disk ) ;
processedpartitions . put ( storagestate . on disk ,
bufferedoutputstream bos = new bufferedoutputstream ( fos ) ;
unprocessedpartitions . put ( storagestate . in mem ,
boolean removed = false ;
if ( temp = = null ) {
private storagestate currentmessagesstate ;
this . partitionstate = state ;
private storagestate incomingmessagesstate ;
public void remove ( metapartition meta ) {
set < metapartition > partitionset = perthreadpartitions
public set < metapartition > getinmemoryunprocessed ( ) {
} else {
} while ( index ! = startindex ) ;
this . incomingmessagesstate = incomingmessagesstate ;
private map < storagestate , set < metapartition > >
public void setcurrentmessagesstate ( storagestate currentmessagesstate ) {
protected abstract void addentrytoimmemorypartitiondata ( int partitionid ,
import org . apache . commons . lang3 . tuple . pair ;
popfromset ( perthreadmessagebuffers . get ( threadid ) ) ;
public void resetpartition ( ) {
protected abstract void offloadinmemorypartitiondata ( int partitionid ,
meta . setcurrentmessagesstate ( storagestate . on disk ) ;
private static class metapartition {
newpair . setleft ( oldpair . getleft ( ) + entrysize ) ;
for ( t entry : pair . getvalue ( ) ) {
processedpartitions . put ( storagestate . in transit ,
public static final intconfoption minimum buffer size to flush =
perthreadpartitions . get ( owner ) . add ( meta , storagestate . in transit ) ;
public integer getoffloadmessageid ( int threadid ) {
public boolean isondisk ( ) {
if ( haspartitiondataondisk . contains ( partitionid ) ) {
numpartitionsprocessed . getandincrement ( ) ;
checkstate ( unprocessedpartitions . get ( storagestate . in mem ) . size ( ) = = 0 ) ;
readwritelock readwritelock = locks . get ( partitionid ) ;
log . info ( "add : partition " + meta . getpartitionid ( ) + " is already " +
import java . io . datainputstream ;
new intconfoption ( "giraph . flushbuffersize" , 8 * one mb ,
public boolean startoffloadingbuffer ( int partitionid ) {
processedpartitions = maps . newconcurrentmap ( ) ;
public void doneoffloadingmessages ( int partitionid ) {
private static < t > t popfromset ( set < t > set ) {
set < metapartition > partitionset =
if ( removed ) {
if ( partitionid = = null ) {
string path )
import java . util . arraylist ;
return processedpartitions . get ( storagestate . in mem ) ;
for ( int i = 0 ; i < numbuffers ; + + i ) {
public void doneoffloadingpartition ( int partitionid ) {
. addall ( partitionstore . getcandidatebufferstooffload ( ) ) ;
partitionset = status . getinmemoryprocessed ( ) ;
private enum storagestate { in mem , on disk , in transit } ;
return meta . isondisk ( ) ;
diskbackededgestore < ? , ? , ? > edgestore =
perthreadpartitions . get ( index ) . getinmemoryunprocessed ( ) ;
diskbackedmessagestore < ? , ? > messagestore =
meta . setpartitionstate ( storagestate . in transit ) ;
import java . util . iterator ;
public metapartitionmanager ( int numiothreads , outofcoreengine oocengine ) {
throws ioexception {
int entrysize = entryserializedsize ( entry ) ;
( diskbackededgestore < ? , ? , ? > ) ( oocengine . getserverdata ( ) )
readwritelock = temp ;
private static < t > t peekfromset ( set < t > set ) {
fileoutputstream fos = new fileoutputstream ( file , true ) ;
public processingstate getprocessingstate ( ) {
partitionset = processedpartitions . get ( storagestate . in transit ) ;
return partitionstate ;
log . debug ( "loadpartitiondata : loading " + numbuffers + " buffers of" +
synchronized ( partitionset ) {
public boolean remove ( metapartition meta , storagestate state ) {
file file = new file ( getbufferspath ( basepath , partitionid ) ) ;
meta . setpartitionstate ( storagestate . on disk ) ;
partitionset = perthreadpartitions . get ( threadid ) . getindiskunprocessed ( ) ;
protected abstract void loadinmemorypartitiondata ( int partitionid ,
public set < metapartition > getindiskprocessed ( ) {
haspartitiondataondisk . add ( partitionid ) ;
@ override
return partitionid ;
perthreadpartitions . get ( owner ) . add ( meta , storagestate . in mem ) ;
if ( oocengine . getserviceworker ( ) . getsuperstep ( ) = =
metapartition meta ;
boolean shouldload = meta . getpartitionstate ( ) = = storagestate . on disk ;
pair < integer , list < t > > pair = databuffers . remove ( partitionid ) ;
perthreadmessagebuffers = new arraylist < > ( numiothreads ) ;
. remove ( meta , storagestate . in transit ) ;
return unprocessedpartitions . get ( storagestate . in mem ) . size ( ) +
public set < metapartition > getinmemoryprocessed ( ) {
metapartition meta = partitions . remove ( partitionid ) ;
public void setprocessingstate ( processingstate processingstate ) {
readwritelock = new reentrantreadwritelock ( ) ;
sb . append ( "id : " + partitionid + " ; " ) ;
private final concurrentmap < integer , readwritelock > locks =
private final concurrentmap < integer , pair < integer , list < t > > > databuffers =
this . randomgenerator = new random ( ) ;
dataoutputstream dos = new dataoutputstream ( bos ) ;
sb . append ( " \ nmetadata : { " ) ;
public storagestate getpartitionstate ( ) {
. getincomingmessagestore ( ) ) ;
numpartition + = status . reset ( ) ;
if ( meta . getprocessingstate ( ) = = processingstate . unprocessed ) {
this . oocengine = oocengine ;
if ( pair ! = null ) {
if ( oldnumbuffersondisk ! = null ) {
set < metapartition > partitionset = null ;
this . currentmessagesstate = storagestate . in mem ;
if ( temp ! = null ) {
databuffers . entryset ( ) ) {
currentmessagesstate = incomingmessagesstate ;
integer oldnumbuffersondisk =
meta . setcurrentmessagesstate ( storagestate . in mem ) ;
return processedpartitions . get ( storagestate . on disk ) ;
import org . apache . log4j . logger ;
for ( map . entry < integer , pair < integer , list < t > > > entry :
}
public int reset ( ) {
if ( meta . getcurrentmessagesstate ( ) = = storagestate . on disk ) {
. addall ( messagestore . getcandidatebufferstooffload ( ) ) ;
public void add ( metapartition meta , storagestate state ) {
import org . apache . giraph . ooc . outofcoreengine ;
checknotnull ( pair ) ;
for ( metapartition meta : partitions . values ( ) ) {
sb . append ( " \ nunprocessedpartitions : " + unprocessedpartitions ) ;
import java . io . ioexception ;
unprocessedpartitions = maps . newconcurrentmap ( ) ;
rwlock . writelock ( ) . lock ( ) ;
"removal is done before start of an iteration over all partitions" ) ;
rwlock . readlock ( ) . lock ( ) ;
if ( meta ! = null ) {
private static final logger log =
import org . apache . giraph . worker . bspserviceworker ;
unprocessedpartitions . get ( storagestate . on disk ) . size ( ) ;
unprocessedpartitions . put ( storagestate . in transit ,
int numpartition = 0 ;
unprocessedpartitions . putall ( processedpartitions ) ;
protected void addentry ( int partitionid , t entry ) {
public void makepartitioninaccessible ( int partitionid ) {
numinmemorypartitions . getandincrement ( ) ;
return getpath ( basepath , partitionid ) + " buffers" ;
unprocessedpartitions . clear ( ) ;
partitionset = unprocessedpartitions . get ( state ) ;
fileinputstream fis = new fileinputstream ( file ) ;
public void setpartitionisprocessed ( int partitionid ) {
maps . newconcurrentmap ( ) ;
entrylist . add ( entry ) ;
int owner = oocengine . getioscheduler ( ) . getownerthreadid ( partitionid ) ;
return unprocessedpartitions . get ( storagestate . on disk ) ;
public boolean haspartition ( integer partitionid ) {
private final concurrentmap < integer , integer > numdatabuffersondisk =
return partitions . keyset ( ) ;
synchronized ( oldpair ) {
int ownerthread = oocengine . getioscheduler ( )
public boolean startloadingpartition ( int partitionid , long superstep ) {
logger . getlogger ( metapartitionmanager . class ) ;
this . processingstate = processingstate . processed ;
if ( partitionset ! = null ) {
diskbackedpartitionstore < ? , ? , ? > partitionstore =
public int getnumpartitions ( ) {
unprocessedpartitions . put ( storagestate . on disk ,
partitionset . remove ( meta ) ;
dis . close ( ) ;
public void loadpartitiondata ( int partitionid , string basepath )
if ( entry . getvalue ( ) . getleft ( ) > minbuffersizetooffload ) {
integer partitionid =
package org . apache . giraph . ooc . data ;
import java . util . list ;
public void doneloadingpartition ( int partitionid , long superstep ) {
iterator < t > it = set . iterator ( ) ;
integer numbuffers = numdatabuffersondisk . remove ( partitionid ) ;
import java . io . bufferedoutputstream ;
if ( superstep = = oocengine . getserviceworker ( ) . getsuperstep ( ) ) {
import java . util . concurrent . locks . reentrantreadwritelock ;
stringbuffer sb = new stringbuffer ( ) ;
import java . util . concurrent . locks . readwritelock ;
private final int minbuffersizetooffload ;
status . getindiskprocessed ( ) . add ( meta ) ;
if ( readwritelock = = null ) {
if ( !set . isempty ( ) ) {
private final list < set < integer > > perthreadmessagebuffers ;
processedpartitions . clear ( ) ;
perthreadvertexedgebuffers = new arraylist < > ( numiothreads ) ;
if ( pair = = null | | pair . getleft ( ) < minbuffersizetooffload ) {
public integer getoffloadpartitionbufferid ( int threadid ) {
meta . setprocessingstate ( processingstate . in process ) ;
partitionset = unprocessedpartitions . get ( storagestate . in transit ) ;
private final list < set < integer > > perthreadvertexedgebuffers ;
removed = partitionset . remove ( meta ) ;
import java . io . dataoutputstream ;
perthreadpartitions = new arraylist < > ( numiothreads ) ;
. remove ( meta , storagestate . in mem ) ;
this . minbuffersizetooffload = minimum buffer size to flush . get ( conf ) ;
"being processed!" ) ;
unprocessedpartitions . get ( storagestate . in transit ) . size ( ) +
return shouldload ;
rwlock . writelock ( ) . unlock ( ) ;
public set < integer > getcandidatebufferstooffload ( ) {
perthreadpartitions . add ( new perthreadpartitionstatus ( ) ) ;
public abstract class outofcoredatamanager < t > {
return ( meta ! = null ) ? meta . getpartitionid ( ) : null ;
if ( partitionset . remove ( meta ) ) {
if ( removed | | meta . getprocessingstate ( ) = = processingstate . in process ) {
sb . append ( "current messages : " + currentmessagesstate + " ; " ) ;
numinmemorypartitions . getanddecrement ( ) ;
partitionset = unprocessedpartitions . get ( storagestate . in mem ) ;
import static com . google . common . base . preconditions . checknotnull ;
haspartitiondataondisk . remove ( partitionid ) ;
checkstate ( unprocessedpartitions . get ( storagestate . on disk ) . size ( ) = = 0 ) ;
throws ioexception ;
popfromset ( perthreadvertexedgebuffers . get ( threadid ) ) ;
return partitions . size ( ) ;
if ( numbuffers ! = null ) {
return set . iterator ( ) . next ( ) ;
. getownerthreadid ( partitionid ) )
pair < integer , list < t > > oldpair =
private final list < perthreadpartitionstatus > perthreadpartitions ;
sb . append ( "processed ? : " + processingstate + " } " ) ;
list < t > entrylist = new arraylist < > ( ) ;
result . add ( entry . getkey ( ) ) ;
public iterable < integer > getpartitionids ( ) {
checkstate ( !meta . isondisk ( ) ) ;
import java . io . fileinputstream ;
public set < metapartition > getindiskunprocessed ( ) {
boolean removed = perthreadpartitions . get ( owner )
. getownerthreadid ( partitionid ) ;
numdatabuffersondisk . putifabsent ( partitionid , numbuffers ) ;
private final outofcoreengine oocengine ;
import java . io . dataoutput ;
if ( meta . getincomingmessagesstate ( ) = = storagestate . in mem ) {
protected abstract void writeentry ( t entry , dataoutput out )
dos . close ( ) ;
file . getabsolutefile ( ) ) ;
if ( oocengine . getserviceworker ( ) . getsuperstep ( ) ! =
public int getpartitionid ( ) {
numdatabuffersondisk . replace ( partitionid ,
public class metapartitionmanager {
import java . io . fileoutputstream ;
protected abstract t readnextentry ( datainput in ) throws ioexception ;
public storagestate getcurrentmessagesstate ( ) {
. getindiskprocessed ( ) ;
import java . util . set ;
writeentry ( entry , dos ) ;
import org . apache . commons . lang3 . tuple . mutablepair ;
private final random randomgenerator ;
int index = randomgenerator . nextint ( numthreads ) ;
"minimum size of a buffer ( in bytes ) to flush to disk . " ) ;
import org . apache . giraph . conf . immutableclassesgiraphconfiguration ;
. remove ( meta , storagestate . on disk ) ;
. addall ( edgestore . getcandidatebufferstooffload ( ) ) ;
sb . append ( "partition : " + partitionstate + " ; " ) ;
t entry = readnextentry ( dis ) ;
import org . apache . giraph . conf . intconfoption ;
shouldload | = meta . getcurrentmessagesstate ( ) = = storagestate . on disk ;
} else if ( meta . getprocessingstate ( ) = = processingstate . processed ) {
public integer getnextpartition ( ) {
pair < integer , list < t > > pair = databuffers . get ( partitionid ) ;
private final set < integer > haspartitiondataondisk =
loadinmemorypartitiondata ( partitionid , getpath ( basepath , partitionid ) ) ;
private storagestate partitionstate ;
public void setincomingmessagesstate ( storagestate incomingmessagesstate ) {
set < integer > result = new hashset < > ( ) ;
meta . setincomingmessagesstate ( storagestate . on disk ) ;
sets . newconcurrenthashset ( ) ;
index = ( index + 1 ) % numthreads ;
partitionid = popfromset ( perthreadmessagebuffers . get ( threadid ) ) ;
public int getnuminmemorypartitions ( ) {
perthreadpartitions . get ( threadid ) . getindiskunprocessed ( ) ;
return sb . tostring ( ) ;
import java . io . bufferedinputstream ;
outofcoredatamanager ( immutableclassesgiraphconfiguration conf ) {
private enum processingstate { processed , unprocessed , in process } ;
private volatile processingstate processingstate ;
meta . setcurrentmessagesstate ( storagestate . in transit ) ;
import java . util . hashset ;
public string tostring ( ) {
metapartition ( int partitionid ) {
for ( t entry : pair . getright ( ) ) {
readwritelock temp = locks . putifabsent ( partitionid , readwritelock ) ;
addentrytoimmemorypartitiondata ( partitionid , entry ) ;
public void removepartition ( integer partitionid ) {
offloadinmemorypartitiondata ( partitionid , getpath ( basepath , partitionid ) ) ;
return partitionstate = = storagestate . on disk | |
import java . io . datainput ;
private final concurrentmap < integer , metapartition > partitions =
if ( numpartitionsprocessed . get ( ) > = partitions . size ( ) ) {
perthreadpartitions . get ( oocengine . getioscheduler ( )
partitionset = perthreadpartitions . get ( threadid ) . getinmemoryunprocessed ( ) ;
public integer getprefetchpartitionid ( int threadid ) {
return incomingmessagesstate ;
return result ;
this . currentmessagesstate = currentmessagesstate ;
outofcoredatamanager . class ) ;
"already being processed! this should happen only if partition " +
shouldload | = meta . getincomingmessagesstate ( ) = = storagestate . on disk ;
@ edu . umd . cs . findbugs . annotations . suppresswarnings (
public integer getoffloadmessagebufferid ( int threadid ) {
sb . append ( " \ nprocessed partitions : " + processedpartitions + " ; " ) ;
perthreadvertexedgebuffers . get ( threadid )
meta = popfromset ( partitionset ) ;
for ( int i = 0 ; i < numiothreads ; + + i ) {
for ( metapartition meta : partitionset ) {
public boolean startoffloadingmessages ( int partitionid ) {
private int partitionid ;
status . getindiskunprocessed ( ) . add ( meta ) ;
pair = databuffers . remove ( partitionid ) ;
import com . google . common . collect . sets ;
public integer getoffloadpartitionid ( int threadid ) {
perthreadmessagebuffers . get ( threadid )
private readwritelock getpartitionlock ( int partitionid ) {
private static class perthreadpartitionstatus {
return ;
private static string getbufferspath ( string basepath , int partitionid ) {
int numbuffers = pair . getright ( ) . size ( ) ;
int startindex = index ;
newpair = ( mutablepair < integer , list < t > > ) oldpair ;
. remove ( meta ) ;
bufferedinputstream bis = new bufferedinputstream ( fis ) ;
t entry = it . next ( ) ;
processedpartitions . put ( storagestate . in mem ,
readwritelock rwlock = getpartitionlock ( partitionid ) ;
final long maxallowedjobtimems =
log . fatal ( reason ) ;
import org . apache . log4j . logger ;
}
giraphconstants . max allowed job time ms . get ( conf ) ;
if ( !conf . trackjobprogressonclient ( ) ) {
} else {
if ( log . isdebugenabled ( ) ) {
private giraphconfiguration conf ;
log . info ( "got all " + mappersstarted + " mappers" ) ;
jobgotallmappers ( ) ;
import java . io . ioexception ;
log . error ( logline ) ;
" , cleaning up . . . " ) ;
public void logfailure ( string reason ) {
private giraphjobobserver jobobserver ;
import org . apache . giraph . conf . giraphconfiguration ;
killthread . start ( ) ;
public static jobprogresstrackerservice createjobprogresstrackerservice (
private thread writerthread ;
jobobserver . jobgotallmappers ( job ) ;
private static final logger log =
if ( combinedworkerprogress . isdone ( conf . getmaxworkers ( ) ) ) {
import java . util . map ;
} catch ( interruptedexception e ) {
public void logerror ( string logline ) {
import java . util . concurrent . concurrenthashmap ;
if ( maxallowedjobtimems > 0 ) {
writerthread = new thread ( new runnable ( ) {
public void updateprogress ( workerprogress workerprogress ) {
jobprogresstrackerservice jobprogresstrackerservice =
log . warn ( "failed to kill job" , e ) ;
try {
implements jobprogresstrackerservice {
log . info ( logline ) ;
writerthread . start ( ) ;
job . killjob ( ) ;
public class defaultjobprogresstrackerservice
thread killthread = new thread ( new runnable ( ) {
private void startwriterthread ( ) {
private static final int update milliseconds = 10 * 1000 ;
private volatile boolean finished = false ;
giraphconfiguration conf , giraphjobobserver jobobserver ) {
} ) ;
return jobprogresstrackerservice ;
maxallowedjobtimems + " milliseconds" ) ;
new combinedworkerprogress ( workerprogresses . values ( ) , conf ) ;
startwriterthread ( ) ;
log . info ( "job " + ( succeeded ? "finished successfully" : "failed" ) +
private final map < integer , workerprogress > workerprogresses =
public void setjob ( job job ) {
killthread . setdaemon ( true ) ;
break ;
private long lasttimemappersstartedlogged ;
log . info ( "progress thread interrupted" ) ;
public synchronized void mapperstarted ( ) {
"interrupted" ) ;
public void init ( giraphconfiguration conf , giraphjobobserver jobobserver ) {
private job job ;
writerthread . interrupt ( ) ;
log . warn ( "killing job because it took longer than " +
import org . apache . giraph . conf . giraphconstants ;
log . info ( "got " + mappersstarted + " but needs " +
public void run ( ) {
( conf . getmaxworkers ( ) + 1 ) + " mappers" ) ;
mappersstarted + + ;
if ( mappersstarted = = conf . getmaxworkers ( ) + 1 & &
update milliseconds ) {
import org . apache . hadoop . mapreduce . job ;
} catch ( ioexception e ) {
public void loginfo ( string logline ) {
thread . sleep ( update milliseconds ) ;
@ override
return null ;
log . debug ( "thread checking for jobs max allowed time " +
this . conf = conf ;
if ( log . isinfoenabled ( ) ) {
this . job = job ;
writerthread . setdaemon ( true ) ;
jobprogresstrackerservice . init ( conf , jobobserver ) ;
log . info ( combinedworkerprogress . tostring ( ) ) ;
import org . apache . giraph . worker . workerprogress ;
public void stop ( boolean succeeded ) {
private void jobgotallmappers ( ) {
private int mappersstarted ;
thread . sleep ( maxallowedjobtimems ) ;
combinedworkerprogress combinedworkerprogress =
workerprogresses . put ( workerprogress . gettaskid ( ) , workerprogress ) ;
finished = true ;
while ( !finished ) {
if ( mappersstarted = = conf . getmaxworkers ( ) + 1 ) {
log . info ( "waiting for job to start . . . ( this may take a minute ) " ) ;
new concurrenthashmap < > ( ) ;
this . jobobserver = jobobserver ;
lasttimemappersstartedlogged = system . currenttimemillis ( ) ;
if ( system . currenttimemillis ( ) - lasttimemappersstartedlogged >
logger . getlogger ( jobprogresstrackerservice . class ) ;
giraphconstants . job progress tracker class . newinstance ( conf ) ;
!workerprogresses . isempty ( ) ) {
package org . apache . giraph . job ;
printaggregatedmetric ( out , "bytes loaded from disk" , "bytes" , bytesloaded ) ;
printaggregatedmetric ( out , "graph in mem" , " % " , graphinmem ) ;
aggregatedmetric = new aggregatedmetricdouble ( ) ;
aggregatedmetric = new aggregatedmetriclong ( ) ;
out . println ( prefix + vh . getvalue ( ) + ' ' + unit +
string unit ,
get ( outofcoreiocallable . bytes load from disk ) ;
return this ;
import org . apache . giraph . ooc . outofcoreiocallable ;
private map < string , aggregatedmetric < ? > > metrics = maps . newhashmap ( ) ;
aggregatedmetriclong aggregatedmetric =
string hostnamepartitionid ) {
}
aggregatedmetric bytesloaded =
public aggregatedmetrics add ( string name , double value ,
workermetrics . getbytesstoredondisk ( ) , hostname ) ;
aggregatedmetric graphinmem =
printaggregatedmetric ( out , "bytes stored to disk" , "bytes" , bytesstored ) ;
metrics . put ( name , aggregatedmetric ) ;
import org . apache . giraph . ooc . outofcoreengine ;
workermetrics . getbytesloadedfromdisk ( ) , hostname ) ;
if ( aggregatedmetric = = null ) {
string unit , valuewithhostname vh ) {
add ( outofcoreengine . graph percentage in memory ,
aggregatedmetricdouble aggregatedmetric =
( aggregatedmetricdouble ) metrics . get ( name ) ;
aggregatedmetric . additem ( value , hostnamepartitionid ) ;
out . println ( " mean : " + aggregatedmetric . mean ( ) + " " + unit ) ;
get ( outofcoreiocallable . bytes store to disk ) ;
workermetrics . getgraphpercentageinmemory ( ) , hostname ) ;
add ( outofcoreiocallable . bytes load from disk ,
printvaluefromhost ( out , " smallest : " , unit , aggregatedmetric . max ( ) ) ;
printvaluefromhost ( out , " largest : " , unit , aggregatedmetric . min ( ) ) ;
( aggregatedmetriclong ) metrics . get ( name ) ;
add ( outofcoreiocallable . bytes store to disk ,
get ( outofcoreengine . graph percentage in memory ) ;
aggregatedmetric bytesstored =
public map < string , aggregatedmetric < ? > > getall ( ) {
giraphconstants . default zookeeper max client cnxns ) ;
import java . io . file ;
int port = zkrunner . start ( zkdir , config ) ;
filetxnsnaplog ftxn = new filetxnsnaplog ( new
string serverlistfile =
}
zkthread . setdaemon ( true ) ;
while ( host = = null ) {
if ( hostnametaskarray . length ! = 3 ) {
" ( polling period is " +
private void generatezookeeperconfig ( ) {
return quorumrunner . start ( config ) ;
zkbaseport = integer . parseint ( hostnametaskarray [ 2 ] ) ;
serverrunner = new zookeeperserverrunner ( ) ;
zkserver . setmaxsessiontimeout ( config . getmaxsessiontimeout ( ) ) ;
hostname task separator + port +
hostname task separator ) ;
int start ( string zkdir , zookeeperconfig config ) ;
filestatus . getpath ( ) . getname ( ) . split (
checkstate ( filestatusarray . length = = 1 ,
zookeeperrunner runner = new inprocesszookeeperrunner ( ) ;
} catch ( interruptedexception e ) {
log . info ( "starting server" ) ;
config . setmaxsessiontimeout ( conf . getzookeepermaxsessiontimeout ( ) ) ;
hostname task separator ;
checkstate ( hostnametaskarray . length = = 2 ,
try {
string zkdirdefault ;
log . info ( "getzookeeperserverlist : found " +
zkservertask = integer . parseint ( serverhostlist [ 1 ] ) ;
zkserverportstring = zkserverhost + " : " + zkbaseport ;
cnxnfactory . startup ( zkserver ) ;
giraphconstants . zookeeper snap retain count ,
generatezookeeperconfig ( ) ;
log . warn ( "either no config or no quorum defined in config , " +
if ( zkservertask = = taskpartition ) {
import org . apache . zookeeper . server . persistence . filetxnsnaplog ;
config . setdatalogdir ( zkdir ) ;
config . setclientportaddress ( new inetsocketaddress ( zkbaseport ) ) ;
return serverrunner . start ( config ) ;
} ) ;
foundserver = hostnametaskarray [ 0 ] ;
zkserver = new zookeeperserver ( ) ;
zkbaseport = port ;
file ( config . getdatalogdir ( ) ) , new file ( config . getdatadir ( ) ) ) ;
if ( port > 0 ) {
updatezkportstring ( ) ;
private void updatezkportstring ( ) {
zkserverhost = serverhostlist [ 0 ] ;
private servercnxnfactory cnxnfactory ;
return zkserver . getclientport ( ) ;
private zookeeperserver zkserver ;
file zkdirfile = new file ( this . zkdir ) ;
private string zkserverhost ;
host = hostnametaskarray [ 0 ] ;
. getdatadir ( ) , config . getdatalogdir ( ) ,
new path ( basedirectory , serverlistfile ) ;
config . setdatadir ( zkdir ) ;
zkserver . setticktime ( giraphconstants . default zookeeper tick time ) ;
thread zkthread = new thread ( new runnable ( ) {
string [ ] hostnametaskarray =
"createzookeeperserverlist : task 0 failed " +
zkserver . shutdown ( ) ;
"createzookeeperserverlist : too many " +
runfromconfig ( config ) ;
zkserver . settxnlogfactory ( ftxn ) ;
arrays . tostring ( serverhostlist ) +
cnxnfactory . join ( ) ;
"status files found " + arrays . tostring ( filestatusarray ) ) ;
import org . apache . giraph . conf . giraphconstants ;
filestatus filestatus = filestatusarray [ 0 ] ;
public void run ( ) {
giraphconstants . zookeeper purge interval ) ;
cnxnfactory . shutdown ( ) ;
"to parse " + filestatus . getpath ( ) . getname ( ) ) ;
cnxnfactory = servercnxnfactory . createfactory ( ) ;
foundserver + " on port " +
config . setminsessiontimeout ( conf . getzookeeperminsessiontimeout ( ) ) ;
port = integer . parseint ( hostnametaskarray [ 1 ] ) ;
private int zkbaseport ;
hostname task separator + taskpartition +
string [ ] serverhostlist = serverlistfile . substring (
int port = 0 ;
import org . apache . zookeeper . server . zookeeperserver ;
} catch ( ioexception e ) {
@ override
zookeeper server list file prefix + host +
if ( zkserver . isrunning ( ) ) {
private final zookeeperconfig config ;
string foundserver = null ;
import org . apache . zookeeper . server . servercnxnfactory ;
zkthread . start ( ) ;
zkbaseport +
public int start ( string zkdir , final zookeeperconfig config ) {
if ( log . isinfoenabled ( ) ) {
hostname task separator + zkbaseport ) ;
private int zkservertask ;
log . info ( "generatezookeeperconfig : with base port " +
import static com . google . common . base . preconditions . checkstate ;
config = new zookeeperconfig ( ) ;
string host = null ;
public void onlinezookeeperserver ( ) {
if ( zkserverhost . equals ( foundserver ) ) {
zkdirfile . getname ( ) + " = " + mkdirret ) ;
log . error ( "unable to start zookeeper" , e ) ;
log . info ( "generatezookeeperconfigfile : make directory of " +
public void runfromconfig ( zookeeperconfig config ) throws ioexception {
log . error ( e . getmessage ( ) , e ) ;
cnxnfactory . configure ( config . getclientportaddress ( ) ,
config
return - 1 ;
log . warn ( "server interrupted" , e ) ;
boolean mkdirret = zkdirfile . mkdirs ( ) ;
public static class zookeeperserverrunner {
zkserver . setminsessiontimeout ( config . getminsessiontimeout ( ) ) ;
"running in process" ) ;
public int start ( zookeeperconfig config ) throws ioexception {
}
@ override
private void checkrequestsafterchannelfailure ( final channelfuture future ) {
if ( shouldresendrequestpredicate . apply ( requestinfo ) ) {
resendrequestswhenneeded ( new predicate < requestinfo > ( ) {
channelfuture writefuture = requestinfo . getwritefuture ( ) ;
return !writefuture . channel ( ) . isactive ( ) | |
( requestinfo . getelapsedmsecs ( ) > maxrequestmilliseconds ) ;
( writefuture . isdone ( ) & & !writefuture . issuccess ( ) ) | |
public boolean apply ( requestinfo requestinfo ) {
predicate < requestinfo > shouldresendrequestpredicate ) {
} ) ;
checkrequestsafterchannelfailure ( future ) ;
import org . apache . giraph . function . predicate ;
return requestinfo . getwritefuture ( ) = = future ;
private void resendrequestswhenneeded (
private class logonerrorchannelfuturelistener
protected int getmaxsize ( ) {
}
protected list < extendeddataoutput > dataoutputs ;
return currentdataoutput ;
return conf . createextendeddataoutput ( size ) ;
protected final immutableclassesgiraphconfiguration conf ;
protected extendeddataoutput currentdataoutput ;
currentdataoutput = createoutput ( initialsize ) ;
protected extendeddataoutput createoutput ( int size ) {
return max size ;
if ( currentdataoutput . getpos ( ) + additionalsize > = getmaxsize ( ) ) {
getdataoutputtowriteto ( len + size delta ) . write ( b , off , len ) ;
private extendeddataoutput getdataoutputtowriteto ( int additionalsize ) {
dataoutputs = new arraylist < > ( 1 ) ;
currentdataoutput = createoutput ( getmaxsize ( ) ) ;
return getdataoutputtowriteto ( size delta ) ;
getdataoutputtowriteto ( b . length + size delta ) . write ( b ) ;
channel . remoteaddress ( ) ) ;
@ override
log . error ( "channel failed " + ctx . channel ( ) ) ;
}
checkrequestsafterchannelfailure ( future . channel ( ) ) ;
checkrequestsafterchannelfailure ( ctx . channel ( ) ) ;
super . channelunregistered ( ctx ) ;
public void channelunregistered ( channelhandlercontext ctx ) throws
exception {
private void checkrequestsafterchannelfailure ( final channel channel ) {
return requestinfo . getwritefuture ( ) . channel ( ) . remoteaddress ( ) . equals (
import io . netty . channel . channelhandlercontext ;
throw new illegalstateexception ( "missing memory usage after gc info" ) ;
return false ;
private double [ ] extreme = new double [ 6 ] ;
offloading ,
receivedbytes * coefficient [ 3 ] +
validindex = !allequal ;
abstractedgestore . progress counter . getprogress ( ) ;
ioaction . store messages and buffers ,
thread . sleep ( checkmemoryinterval ) ;
if ( !firstvalseen ) {
for ( int i = 0 ; i < 5 ; + + i ) {
xvalues [ i ] [ validcolumnindices . size ( ) ] = 1 ;
private final atomiclong numbytestooffload = new atomiclong ( 0 ) ;
if ( log . isdebugenabled ( ) ) {
result = true ;
bytesreceived , oocbytesinjected } ) ;
if ( coefficient [ coefindex ] < lowerbound | |
validindex = true ;
validcolumnindices . remove ( ptr ) ;
thread . setname ( "ooc - memory - checker" ) ;
throw new illegalstateexception ( "bad memory pool" ) ;
import java . util . concurrent . locks . lock ;
stable ,
if ( equal ( val2 , 0 ) ) {
public static final floatconfoption am low threshold =
import java . util . map ;
} catch ( interruptedexception e ) {
public void addrecord ( long memused , long edges , long vertices ,
public memoryestimatororacle ( immutableclassesgiraphconfiguration conf ,
boolean firstvalseen = false ;
checkstate ( memused > 0 , "memory usage cannot be negative" ) ;
arrays . fill ( coefficient , 0 ) ;
if ( used > manualgcmemorypressure ) {
import org . apache . giraph . ooc . command . waitiocommand ;
if ( state = = state . offloading & & numbytestooffload . get ( ) < = 0 ) {
log . info ( "manual gc done . it took " +
int col1 , int col2 ) {
value = upperbound ;
isvalid = setisvalid ;
} ) ;
public static final floatconfoption credit high threshold =
printstats ( ) ;
return true ;
coefficient [ 5 ] ) ;
string poolname = entry . getkey ( ) ;
system . gc ( ) ;
xvalues [ i ] [ j ] = sourcevalues . get ( i ) [ validcolumnindices . get ( j ) ] ;
if ( normalname . contains ( "old" ) | | normalname . contains ( "tenured" ) ) {
boolean setisvalid = false ;
memoryusage usage = getoldgenused ( ) ;
validindex = false ;
sb . append ( string . format ( " % . 2f" , datasamples . get ( i ) [ j ] ) ) ;
private static class memoryestimator {
usedmemoryreal + " error = " +
} else if ( usageestimate < amlowthreshold ) {
sb . append (
value = lowerbound ;
if ( poolname . tolowercase ( ) . contains ( "old" ) ) {
. getgraphtaskmanager ( ) . createuncaughtexceptionhandler ( ) ) ;
"has not recognized it yet!" ) ;
private final atomiclong oocbytesinjected = new atomiclong ( 0 ) ;
oocengine . updaterequestscreditfraction ( 0 ) ;
isvalid = false ;
private final atomiclong oocbytesinjected ;
throw new illegalstateexception ( "missing memory usage before gc info" ) ;
public void run ( ) {
new floatconfoption ( "giraph . creditlowthreshold" , 0 . 90f ,
string . format ( " % . 2f" , time / 1000 . 0 ) +
setisvalid = true ;
public void clear ( ) {
do {
private volatile state state = state . stable ;
for ( memorypoolmxbean pool : memorypoollist ) {
( amhighthreshold - amlowthreshold ) ) ;
private memoryusage getoldgenused ( ) {
sb . append ( memorysamples . get ( i ) ) ;
private static boolean equal ( double val1 , double val2 ) {
list < integer > validcolumnindices , olsmultiplelinearregression mlr )
stringbuilder sb = new stringbuilder ( ) ;
if ( log . isinfoenabled ( ) ) {
"addrecord : avoiding to add the same entry as the last one!" ) ;
string . format ( " % . 2f" , used ) + " . " ) ;
memorysamples . todoublearray ( new double [ memorysamples . size ( ) ] ) ;
import org . apache . giraph . worker . workerprogress ;
boolean allequal = true ;
memoryusage after = null ;
import static com . google . common . base . preconditions . checkstate ;
long memused = after . getused ( ) ;
long edgesloaded = currentsuperstep > = 0 ? 0 :
import java . util . concurrent . atomic . atomiclong ;
public static final floatconfoption ooc threshold =
if ( validcolumnindices . get ( i ) = = coefindex ) {
thread . setuncaughtexceptionhandler ( oocengine . getserviceworker ( )
"monitors memory footprint ( in milliseconds ) " ) ;
boolean validindex = false ;
import java . lang . management . memorypoolmxbean ;
this . credithighthreshold = credit high threshold . get ( conf ) ;
logger . getlogger ( memoryestimatororacle . class ) ;
( credithighthreshold - creditlowthreshold ) ) ;
"happened in a while" ) ;
for ( int i = 0 ; i < memorysamples . size ( ) ; + + i ) {
if ( usageestimate > oocthreshold ) {
string cause = gcinfo . getgccause ( ) . tolowercase ( ) ;
changed = refinecoefficient ( 4 , 1 , 2 , xvalues , yvalues ) ;
public static final floatconfoption gc minimum reclaim fraction =
lastmajorgctime = system . currenttimemillis ( ) ;
( cause . contains ( "ergo" ) | | cause . contains ( "system" ) ) ) {
private static boolean islineardependence ( list < double [ ] > values ,
boolean istight = ( maxmem - memused ) < 2 * gcreclaimfraction * maxmem & &
"gc . if less than this amount is reclaimed , it is sage to say " +
boolean changed ;
long oldgenusageestimate = memoryestimator . getusageestimate ( ) ;
thread . start ( ) ;
long garbage = before . getused ( ) - after . getused ( ) ;
if ( usageestimate > 0 ) {
memorysamples . clear ( ) ;
"we are in a high memory situation and the estimation mechanism " +
( maxmem - memused ) ) ;
} else {
private final float gcreclaimfraction ;
garbagecollectionnotificationinfo gcinfo ) {
extreme [ 4 ] = - 1 ;
boolean predictionexist = memoryestimator . getusageestimate ( ) > 0 ;
boolean result = false ;
log . info ( "gccompleted : estimate = " + usedmemoryestimate + " real = " +
long receivedbytes = networkmetrics . getbytesreceivedpersuperstep ( ) ;
edgeinputsplitscallable . gettotaledgesloadedmeter ( ) . count ( ) ;
datasamples . add ( new double [ ] { edges , vertices , verticesprocessed ,
log . debug (
for ( double [ ] value : datasamples ) {
if ( ptr ! = - 1 ) {
this . memoryestimator = new memoryestimator ( this . oocbytesinjected ,
if ( action . contains ( "major" ) & &
for ( int j = 0 ; j < datasamples . get ( i ) . length ; + + j ) {
try {
double usageestimate = ( double ) usageestimatemem / maxmemory ;
oocengine . updaterequestscreditfraction ( 1 -
new floatconfoption ( "giraph . oocthreshold" , 0 . 90f ,
public class memoryestimatororacle implements outofcoreoracle {
this . creditlowthreshold = credit low threshold . get ( conf ) ;
long verticesloaded = currentsuperstep > = 0 ? 0 :
for ( map . entry < string , memoryusage > entry :
private static void fillxmatrix ( list < double [ ] > sourcevalues ,
} else if ( ! ( command instanceof waitiocommand ) ) {
updaterates ( 1 , 1 ) ;
usage = ( long ) ( edgesloaded * coefficient [ 0 ] +
datasamples . size ( ) > = validcolumnindices . size ( ) + 1 ) {
" ( compute / input ) are paused . " ) ;
list < integer > validcolumnindices ,
"if mem - usage is below this threshold , all active threads " +
private boolean refinecoefficient ( int coefindex , double lowerbound ,
if ( usageestimate > = amhighthreshold ) {
oocbytesinjected . set ( 0 ) ;
import org . apache . giraph . ooc . command . loadpartitioniocommand ;
verticesloaded , verticescomputed , receivedbytes , oocbytes ) ;
oocengine . updateactivethreadsfraction ( 1 -
import java . util . arraylist ;
while ( true ) {
new floatconfoption ( "giraph . garbageestimator . gcreclaimfraction" , 0 . 05f ,
private enum state {
thread thread = new thread ( new runnable ( ) {
public void commandcompleted ( iocommand command ) {
if ( extreme [ 3 ] ! = - 1 ) {
oocengine . getnetworkmetrics ( ) . getbytesreceivedpersuperstep ( ) ;
private list < double [ ] > datasamples = new arraylist < > ( ) ;
log . info ( "gccompleted : tight memory usage . starting to offload " +
throws exception {
"high memory pressure with no full gc from the jvm . " +
this . networkmetrics = networkmetrics ;
double [ ] last = datasamples . get ( datasamples . size ( ) - 1 ) ;
this . gcreclaimfraction = gc minimum reclaim fraction . get ( conf ) ;
" \ nedgesverticesv procreceivedoocmem used \ n" ) ;
verticesloaded * coefficient [ 1 ] +
long verticesloaded = oocengine . getsuperstep ( ) > = 0 ? 0 :
public memoryestimator ( atomiclong oocbytesinjected ,
string action = gcinfo . getgcaction ( ) . tolowercase ( ) ;
if ( firstvalue = = - 1 ) {
memoryestimator . setcurrentsuperstep ( oocengine . getsuperstep ( ) ) ;
memused + " maxmem = " + maxmem ) ;
for ( int i = 0 ; i < validcolumnindices . size ( ) ; + + i ) {
public static final floatconfoption manual gc memory pressure =
garbage < gcreclaimfraction * maxmem ;
changed | = refinecoefficient ( 3 , 0 , 2 , xvalues , yvalues ) ;
import org . apache . giraph . edge . abstractedgestore ;
import com . sun . management . garbagecollectionnotificationinfo ;
"setting it to the extreme of the bound" ) ;
if ( state = = state . offloading ) {
} while ( changed ) ;
@ override
package org . apache . giraph . ooc . policy ;
sb . append ( " \ n" ) ;
usedmemoryreal * 100 ) ) ;
private void printstats ( ) {
if ( math . abs ( ( value [ i ] - firstvalue ) / firstvalue ) > 0 . 01 ) {
new floatconfoption ( "giraph . credithighthreshold" , 0 . 95f ,
private volatile long lastmajorgctime = 0 ;
sb . append ( string . format ( " % . 2f" , coefficient [ i ] ) ) ;
continue ;
import org . apache . giraph . worker . vertexinputsplitscallable ;
private boolean isvalid = false ;
for ( int i = validcolumnindices . size ( ) - 1 ; i > = 0 ; - - i ) {
mlr . newsampledata ( yvalues , xvalues ) ;
new floatconfoption ( "giraph . amhighthreshold" , 0 . 95f ,
list < memorypoolmxbean > memorypoollist =
import java . util . concurrent . locks . reentrantlock ;
time = system . currenttimemillis ( ) - time ;
/ /
log . debug ( "addrecord : coefficient at index " + coefindex +
state = state . stable ;
double upperbound , double [ ] [ ] xvalues , double [ ] yvalues )
this . oocengine = oocengine ;
"minimum percentage of memory we expect to be reclaimed after a full " +
" valid columns in the regression" ) ;
coefficient [ coefindex ] > upperbound ) {
double [ ] yvalues =
thread . setdaemon ( true ) ;
oocengine . updateactivethreadsfraction ( 0 ) ;
if ( value [ i ] ! = 0 ) {
memorysamples . add ( ( double ) memused ) ;
for ( int i = 0 ; i < datasamples . size ( ) ; + + i ) {
sb . append ( "coefficient : \ n" ) ;
if ( after = = null ) {
import org . apache . log4j . logger ;
double [ ] [ ] xvalues = new double [ datasamples . size ( ) ] [ ] ;
if ( extreme [ 4 ] ! = - 1 ) {
}
updaterates ( - 1 , 1 ) ;
import org . apache . giraph . ooc . outofcoreengine ;
this . amhighthreshold = am high threshold . get ( conf ) ;
verticesprocessed = = last [ 2 ] & & bytesreceived = = last [ 3 ] & &
oocbytesinjected = = last [ 4 ] ) {
private double [ ] coefficient = new double [ 6 ] ;
long edgesloaded = oocengine . getsuperstep ( ) > = 0 ? 0 :
"the threshold above which gc is called manually if full gc has not " +
import java . lang . management . memoryusage ;
int ptr = - 1 ;
double used = ( double ) usage . getused ( ) / usage . getmax ( ) ;
private static final logger log =
( usageestimate - creditlowthreshold ) /
log . debug ( "printstats : isvalid = " + isvalid + sb . tostring ( ) ) ;
} finally {
oocengine . getnetworkmetrics ( ) ) ;
throw new exception ( "there are " + coefficient . length +
private static void calculateregression ( double [ ] coefficient ,
double firstvalue = - 1 ;
oocbytes * coefficient [ 4 ] +
private doublearraylist memorysamples = new doublearraylist ( ) ;
return math . abs ( val1 - val2 ) < 0 . 01 ;
after = gcinfo . getgcinfo ( ) . getmemoryusageaftergc ( ) . get ( poolname ) ;
private final float amhighthreshold ;
numbytestooffload . getandadd ( 0 - command . bytestransferred ( ) ) ;
public static final floatconfoption credit low threshold =
long verticescomputed = workerprogress . get ( ) . getverticescomputed ( ) +
log . info ( "gccompleted : garbage = " + garbage + " memused = " +
if ( istight & & !predictionexist ) {
workerprogress . get ( ) . getverticesstored ( ) +
private long currentsuperstep = - 1 ;
calculateregression ( coefficient , validcolumnindices , mlr ) ;
public boolean approve ( iocommand command ) {
long bytesreceived , long oocbytesinjected ) {
if ( islineardependence ( datasamples , i , col ) ) {
if ( oldgenusage > 0 ) {
private lock lock = new reentrantlock ( ) ;
ioaction . store partition } ;
gcinfo . getgcinfo ( ) . getmemoryusagebeforegc ( ) . entryset ( ) ) {
long maxmem = after . getmax ( ) ;
this . manualgcmemorypressure = manual gc memory pressure . get ( conf ) ;
validcolumnindices . clear ( ) ;
"if mem - usage is above this threshold , out of core threads starts " +
private final float oocthreshold ;
public void setcurrentsuperstep ( long superstep ) {
import org . apache . giraph . conf . longconfoption ;
before = entry . getvalue ( ) ;
break ;
return new ioaction [ ] { ioaction . load partition } ;
new floatconfoption ( "giraph . amlowthreshold" , 0 . 90f ,
if ( edges = = last [ 0 ] & & vertices = = last [ 1 ] & &
if ( validcolumnindices . size ( ) > = 1 & &
import org . apache . giraph . worker . edgeinputsplitscallable ;
validcolumnindices . add ( i ) ;
import java . util . list ;
numbytestooffload . set ( 0 ) ;
for ( double [ ] value : values ) {
final outofcoreengine oocengine ) {
"if mem - usage is above this threshold , credit is set to 0" ) ;
private final memoryestimator memoryestimator ;
double value ;
new longconfoption ( "giraph . garbageestimator . checkmemoryinterval" , 1000 ,
extreme [ 3 ] = - 1 ;
" ( compute / input ) are running . " ) ;
for ( int i = 0 ; i < coefficient . length ; + + i ) {
return new ioaction [ ] {
"if mem - usage is above this threshold , all active threads " +
oocbytesinjected . getandadd ( 0 - command . bytestransferred ( ) ) ;
} catch ( exception e ) {
numbytestooffload . getandadd ( command . bytestransferred ( ) ) ;
double firstval = 0 ;
long time = system . currenttimemillis ( ) ;
extreme [ coefindex ] = value ;
import org . apache . giraph . comm . networkmetrics ;
networkmetrics networkmetrics ) {
abstractedgestore . progress counter . reset ( ) ;
yvalues [ i ] - = value * datasamples . get ( i ) [ coefindex ] ;
"if mem - usage is below this threshold , credit is set to max" ) ;
coefficient [ coefindex ] = value ;
if ( command instanceof loadpartitioniocommand ) {
" is wrong in the regression , setting it to " + value ) ;
usage = getoldgenused ( ) ;
public void startiteration ( ) {
verticescomputed * coefficient [ 2 ] +
if ( time - lastmajorgctime > = 10000 ) {
"writing data to disk" ) ;
this . amlowthreshold = am low threshold . get ( conf ) ;
public static final longconfoption check memory interval =
if ( datasamples . size ( ) > 0 ) {
private list < integer > validcolumnindices = new arraylist < > ( ) ;
long oldgenusage = memoryestimator . getusageestimate ( ) ;
log . warn ( "run : exception occurred!" , e ) ;
( ( double ) math . abs ( usedmemoryestimate - usedmemoryreal ) /
lock . lock ( ) ;
coefficient [ validcolumnindices . get ( i ) ] = beta [ i ] ;
import java . lang . management . managementfactory ;
public synchronized void gccompleted (
xvalues [ i ] = new double [ validcolumnindices . size ( ) + 1 ] ;
oocengine . updateactivethreadsfraction ( 1 ) ;
lock . unlock ( ) ;
import org . apache . giraph . ooc . command . iocommand ;
long oocbytes = this . oocbytesinjected . get ( ) ;
new floatconfoption ( "giraph . garbageestimator . manualgcpressure" , 0 . 95f ,
if ( usedmemoryestimate > = 0 ) {
long verticesprocessed ,
private final float creditlowthreshold ;
memoryusage before = null ;
"until " + numbytestooffload . get ( ) + " bytes are offloaded" ) ;
log . warn ( "addrecord : exception occurred!" , e ) ;
allequal = false ;
firstval = val1 / val2 ;
if ( coefficient . length ! = validcolumnindices . size ( ) ) {
import org . apache . giraph . conf . floatconfoption ;
public long getusageestimate ( ) {
private final outofcoreengine oocengine ;
long usage = - 1 ;
for ( int j = 0 ; j < validcolumnindices . size ( ) ; + + j ) {
managementfactory . getmemorypoolmxbeans ( ) ;
"addrecord : coefficient was not in the regression , " +
" seconds . used fraction of old - gen is " +
ptr = i ;
} else if ( usageestimate < creditlowthreshold ) {
this . oocbytesinjected = oocbytesinjected ;
private void updaterates ( long usageestimatemem , long maxmemory ) {
datasamples . clear ( ) ;
return usage ;
firstvalseen = true ;
double val2 = value [ col2 ] ;
log . info (
return pool . getusage ( ) ;
private olsmultiplelinearregression mlr = new olsmultiplelinearregression ( ) ;
long usedmemoryreal = after . getused ( ) ;
coefficient [ 3 ] = extreme [ 3 ] ;
"calling gc manually . used fraction of old - gen is " +
public ioaction [ ] getnextioactions ( ) {
double [ ] beta = mlr . estimateregressionparameters ( ) ;
import org . apache . giraph . conf . immutableclassesgiraphconfiguration ;
long receivedbytes =
this . oocthreshold = ooc threshold . get ( conf ) ;
if ( coefficient [ coefindex ] < lowerbound ) {
"the interval where memory checker thread wakes up and " +
numbytestooffload . set ( ( long ) ( 2 * gcreclaimfraction * maxmem ) -
( usageestimate - amlowthreshold ) /
memoryestimator . clear ( ) ;
double [ ] [ ] xvalues ) {
for ( int col = i + 1 ; col < 5 ; + + col ) {
firstvalue = value [ i ] ;
if ( !equal ( ( val1 / val2 - firstval ) / firstval , 0 ) ) {
string normalname = pool . getname ( ) . tolowercase ( ) ;
oocbytesinjected . getandadd ( command . bytestransferred ( ) ) ;
if ( validindex ) {
result = false ;
" coefficients , but " + validcolumnindices . size ( ) +
double usageestimate = ( double ) oldgenusage / usage . getmax ( ) ;
import it . unimi . dsi . fastutil . doubles . doublearraylist ;
coefficient [ 4 ] = extreme [ 4 ] ;
ioaction . store messages and buffers , ioaction . store partition } ;
for ( int i = 0 ; i < sourcevalues . size ( ) ; + + i ) {
if ( equal ( val1 , 0 ) ) {
used = ( double ) usage . getused ( ) / usage . getmax ( ) ;
if ( usageestimate > = credithighthreshold ) {
private final networkmetrics networkmetrics ;
memoryestimator . addrecord ( getoldgenused ( ) . getused ( ) , edgesloaded ,
oocengine . updaterequestscreditfraction ( 1 ) ;
fillxmatrix ( datasamples , validcolumnindices , xvalues ) ;
public static final floatconfoption am high threshold =
return result ;
long oocbytes = oocbytesinjected . get ( ) ;
this . currentsuperstep = superstep ;
import org . apache . commons . math . stat . regression . olsmultiplelinearregression ;
vertexinputsplitscallable . gettotalverticesloadedmeter ( ) . count ( ) ;
long usedmemoryestimate = memoryestimator . getusageestimate ( ) ;
private final float amlowthreshold ;
if ( before = = null ) {
private final float credithighthreshold ;
if ( isvalid ) {
final long checkmemoryinterval = check memory interval . get ( conf ) ;
import java . util . arrays ;
double val1 = value [ col1 ] ;
coefficient [ 5 ] = beta [ validcolumnindices . size ( ) ] ;
if ( oldgenusageestimate > 0 ) {
updaterates ( oldgenusageestimate , usage . getmax ( ) ) ;
state = state . offloading ;
return ;
private final float manualgcmemorypressure ;
private static boolean calculateregression ( double [ ] coefficient ,
return false ;
result = calculateregression ( coefficient , validcolumnindices , mlr ) ;
}
result = refinecoefficient ( 3 , 0 , 2 , xvalues , yvalues ) ;
boolean result = null ;
double upperbound , double [ ] [ ] xvalues , double [ ] yvalues ) {
changed = result ;
result = refinecoefficient ( 4 , 1 , 2 , xvalues , yvalues ) ;
if ( !isregressionvalid ) {
return true ;
changed | = result ;
calculateregression ( coefficient , validcolumnindices , mlr ) ;
@ nullable
private boolean refinecoefficient ( int coefindex , double lowerbound ,
boolean isregressionvalid =
return ;
if ( !result ) {
return null ;
log . warn ( "there are " + coefficient . length +
import javax . annotation . nullable ;
list < integer > validcolumnindices , olsmultiplelinearregression mlr ) {
if ( result = = null ) {
return ( ( branchinstruction ) i1 ) . gettarget ( ) = = ( ( branchinstruction ) i2 ) . gettarget ( ) ;
if ( i1 . getopcode ( ) = = i2 . getopcode ( ) ) {
return "bootstrapmethod ( " + bootstrap method ref + " , " + bootstrap arguments . length + " , "
final int num bootstrap arguments = bootstrap arguments . length ;
this . bootstrap arguments = new int [ num bootstrap arguments ] ;
import java . util . arrays ;
+ arrays . tostring ( bootstrap arguments ) + " ) " ;
for ( int i = 0 ; i < bootstrap arguments . length ; i + + ) {
return bootstrap arguments . length ;
}
return false ;
if ( i1 instanceof branchinstruction ) {
if ( i1 = = i2 ) {
return true ;
}
public classpath ( ) {
this ( getclasspath ( ) ) ;
@ deprecated
try ( inputstream is = getinputstream ( name , suffix ) ) {
public byte [ ] getbytes ( final string name , final string suffix ) throws ioexception {
list . toarray ( paths ) ;
list . add ( new dir ( path ) ) ;
list < pathentry > list = new arraylist < > ( ) ;
paths = new pathentry [ list . size ( ) ] ;
list . add ( new zip ( new zipfile ( file ) ) ) ;
if ( !path . isempty ( ) ) {
long getsize ( ) ;
string getbase ( ) ;
inputstream getinputstream ( ) throws ioexception ;
string getpath ( ) ;
long gettime ( ) ;
private void updatelocalvariabletable ( localvariabletable a ) {
if ( lvg [ i ] . getname ( ) . equals ( l . getname ( ) ) & & lvg [ i ] . getindex ( ) = = l . getindex ( ) ) {
removelocalvariables ( ) ;
if ( null = = end ) {
this . local variable table = ( localvariabletable ) a ;
adjustlocalvariablelength ( lvt ) ;
final localvariable [ ] lv = a . getlocalvariabletable ( ) ;
}
for ( final localvariable l : lv ) {
instructionhandle end = il . findhandle ( l . getstartpc ( ) + l . getlength ( ) ) ;
start = il . getstart ( ) ;
localvariable [ ] lv = lvt . getlocalvariabletable ( ) ;
for ( localvariable l : lv ) {
if ( null = = start ) {
this . local variable type table = ( localvariabletypetable ) a ;
} else if ( a instanceof localvariabletypetable ) {
private localvariabletable local variable table = null ;
private void adjustlocalvariablelength ( localvariabletable lvt ) {
addcodeattribute ( local variable type table ) ;
updatelocalvariabletable ( local variable table ) ;
updatelocalvariabletable ( this . local variable table ) ;
for ( int i = 0 , length = lvg . length ; i < length ; i + + ) {
if ( local variable table ! = null ) {
instructionhandle start = il . findhandle ( l . getstartpc ( ) ) ;
lvg [ i ] . setlength ( l . getlength ( ) ) ;
import org . apache . bcel . classfile . localvariabletypetable ;
addlocalvariable ( l . getname ( ) , type . gettype ( l . getsignature ( ) ) , l
. getindex ( ) , start , end ) ;
private localvariabletypetable local variable type table = null ;
if ( local variable type table ! = null ) {
break ;
localvariable [ ] lvg = local variable type table . getlocalvariabletypetable ( ) ;
if ( lvt ! = null ) {
end = il . getend ( ) ;
if ( element . getname ( ) . equals ( l . getname ( ) ) & & element . getindex ( ) = = l . getindex ( ) ) {
for ( localvariable element : lvg ) {
element . setlength ( l . getlength ( ) ) ;
private void updatelocalvariabletable ( final localvariabletable a ) {
private void adjustlocalvariablelength ( final localvariabletable lvt ) {
classfile getclassfile ( final string name , final string suffix ) throws ioexception {
if ( cf ! = null ) {
public classfile getclassfile ( final string name ) throws ioexception {
public string getpath ( string name ) throws ioexception {
final classpath cp = ( classpath ) o ;
for ( final abstractpathentry path : paths ) {
return classpath . hashcode ( ) ;
return entry ! = null ? zip . getinputstream ( entry ) : null ;
private final zipfile zip ;
return "classes / " + packagetofolder ( name ) + suffix ;
return packagetofolder ( name ) + suffix ;
static string packagetofolder ( final string name ) {
private static final filenamefilter modules filter = new filenamefilter ( ) {
private static class dir extends abstractpathentry {
final file modules dir = new file ( modules path ) ;
string modules path = system . getproperty ( "java . modules . path" ) ;
final list < abstractpathentry > list = new arraylist < > ( ) ;
}
return file . exists ( ) ? new fileinputstream ( file ) : null ;
paths = new abstractpathentry [ list . size ( ) ] ;
public string tostring ( ) {
private final abstractpathentry [ ] paths ;
list . add ( new module ( new zipfile ( file ) ) ) ;
return entry ! = null ? new url ( "jar : file : " + zip . getname ( ) + "! / " + name ) : null ;
return classpath ;
return new file ( dir + file . separatorchar + name . replace ( ' / ' , file . separatorchar ) ) ;
public byte [ ] getbytes ( final string name ) throws ioexception {
final classfile cf = path . getclassfile ( name , suffix ) ;
if ( modules path = = null | | modules path . trim ( ) . isempty ( ) ) {
public classfile getclassfile ( final string name , final string suffix ) throws ioexception {
jar ( final zipfile zip ) {
modules path = system . getproperty ( "java . home" ) + file . separator + "jmods" ;
this . classpath = class path ;
name = name . tolowercase ( locale . english ) ;
final file file = new file ( dir + file . separatorchar + name . replace ( ' . ' , file . separatorchar ) + suffix ) ;
return classpath . hashcode ( ) + parent . hashcode ( ) ;
abstractzip ( final zipfile zip ) {
return zip . getname ( ) ;
return getinputstream ( packagetofolder ( name ) , " . class" ) ;
private static void getpathcomponents ( final string path , final list < string > list ) {
this . zip = zip ;
private abstract static class abstractpathentry {
public inputstream getinputstream ( final string name ) throws ioexception {
final string [ ] modules = modules dir . list ( modules filter ) ;
private final string classpath ;
private file tofile ( final string name ) {
protected string toentryname ( final string name , final string suffix ) {
if ( modules dir . exists ( ) ) {
private static abstract class abstractzip extends abstractpathentry {
final file file = tofile ( name ) ;
private static class jar extends abstractzip {
public boolean equals ( final object o ) {
return name . endswith ( " . jmod" ) ;
@ override
return classpath . equals ( cp . tostring ( ) ) ;
super ( zip ) ;
module ( final zipfile zip ) {
return cf ;
} ;
return null ;
final zipentry entry = zip . getentry ( toentryname ( name , suffix ) ) ;
public boolean accept ( final file dir , string name ) {
public inputstream getinputstream ( final string name , final string suffix ) throws ioexception {
} else if ( path . endswith ( " . jmod" ) ) {
list . add ( new jar ( new zipfile ( file ) ) ) ;
for ( final string module : modules ) {
protected abstract string toentryname ( final string name , final string suffix ) ;
return name . replace ( ' . ' , ' / ' ) ;
private static class module extends abstractzip {
abstract classfile getclassfile ( string name , string suffix ) throws ioexception ;
public string getpath ( final string name , final string suffix ) throws ioexception {
return parent + file . pathseparator + classpath ;
list . add ( modules dir . getpath ( ) + file . separatorchar + module ) ;
stmt . close ( ) ;
validatingset . add ( pconn ) ;
public boolean validateobject ( userpasskey key ,
public synchronized pooledconnectionandinfo makeobject ( userpasskey upkey )
valid = false ;
conn . rollback ( ) ;
throws exception {
validatingset . remove ( pconn ) ;
new hashset < pooledconnection > ( ) ;
if ( rset ! = null ) {
if ( stmt ! = null ) {
pc . removeconnectioneventlistener ( this ) ;
if ( rollbackaftervalidation ) {
public void setpool ( keyedobjectpool < userpasskey , pooledconnectionandinfo > pool ) {
pci = new pooledconnectionandinfo ( pc , username , password ) ;
pc . close ( ) ;
}
pooledconnectionandinfo pci = null ;
connectioneventlistener , pooledconnectionmanager {
import java . util . hashset ;
return pci ;
statement stmt = null ;
pcmap . put ( pc , pci ) ;
if ( null ! = query ) {
if ( conn ! = null ) {
public void passivateobject ( userpasskey key , pooledconnectionandinfo pci ) {
import java . util . set ;
conn . close ( ) ;
stmt = conn . createstatement ( ) ;
if ( !validatingset . contains ( pc ) ) {
} else {
public void activateobject ( userpasskey key , pooledconnectionandinfo pci ) {
pooledconnectionandinfo pci = pcmap . get ( pc ) ;
string query = validationquery ;
private final weakhashmap < pooledconnection , pooledconnectionandinfo > pcmap =
pci ) ;
conn = pconn . getconnection ( ) ;
valid = true ;
pooledconnection pconn = pci . getpooledconnection ( ) ;
if ( pci = = null ) {
pooledconnectionandinfo pci ) {
pool . invalidateobject ( pci . getuserpasskey ( ) , pci ) ;
} catch ( throwable t ) {
pooledconnection pc = pci . getpooledconnection ( ) ;
implements keyedpoolableobjectfactory < userpasskey , pooledconnectionandinfo > ,
resultset rset = null ;
connection conn = null ;
public void destroyobject ( userpasskey key , pooledconnectionandinfo pci )
public keyedobjectpool < userpasskey , pooledconnectionandinfo > getpool ( ) {
pool . returnobject ( pci . getuserpasskey ( ) , pci ) ;
new weakhashmap < pooledconnection , pooledconnectionandinfo > ( ) ;
} finally {
} catch ( exception e ) {
pcmap . remove ( pc ) ;
private keyedobjectpool < userpasskey , pooledconnectionandinfo > pool ;
rset = stmt . executequery ( query ) ;
rset . close ( ) ;
if ( rset . next ( ) ) {
private final set < pooledconnection > validatingset =
pooledconnectionandinfo info = pcmap . get ( pc ) ;
try {
new weakhashmap < > ( ) ;
private final set < pooledconnection > validatingset = new hashset < > ( ) ;
new weakhashmap < > ( ) ;
private final set < pooledconnection > validatingset = new hashset < > ( ) ;
new hashmap < > ( ) ;
} else if ( trace instanceof resultset ) {
list < abandonedtrace > traces = gettrace ( ) ;
}
cleartrace ( ) ;
setlastused ( 0 ) ;
while ( traceiter . hasnext ( ) ) {
conn . close ( ) ;
object trace = traceiter . next ( ) ;
if ( traces ! = null ) {
iterator < abandonedtrace > traceiter = traces . iterator ( ) ;
passivate ( ) ;
closed = true ;
( ( resultset ) trace ) . close ( ) ;
if ( trace instanceof statement ) {
( ( statement ) trace ) . close ( ) ;
super . close ( ) ;
protected final connection getdelegateinternal ( ) {
} finally {
try {
}
c = ( ( delegatingconnection < ? > ) c ) . getdelegateinternal ( ) ;
delegatingconnection < ? > c = ( delegatingconnection < ? > ) obj ;
( ( delegatingconnection < ? > ) conn ) . activate ( ) ;
public class delegatingconnection < c extends connection > extends abandonedtrace
( ( delegatingconnection < ? > ) conn ) . clearcachedstate ( ) ;
public void setdelegate ( c c ) {
private c conn = null ;
if ( obj instanceof delegatingconnection ) {
public delegatingconnection ( c c ) {
return pds ;
private long timebetweenevictionrunsmillis =
protected datasource createdatasourceinstance ( ) throws sqlexception {
private boolean testwhileidle = false ;
private classloader driverclassloader = null ;
private int initialsize = 0 ;
private boolean testonreturn = false ;
}
return connectionpool ;
private volatile datasource datasource = null ;
private volatile genericobjectpool < poolableconnection > connectionpool = null ;
private volatile string defaultcatalog = null ;
protected genericobjectpool < poolableconnection > getconnectionpool ( ) {
private long maxwaitmillis =
private volatile int validationquerytimeout = - 1 ;
protected boolean getdefaultreadonlyboolean ( ) {
private int numtestsperevictionrun =
return connectionproperties ;
properties getconnectionproperties ( ) {
private string url = null ;
private volatile string validationquery = null ;
private volatile list < string > connectioninitsqls ;
private int maxtotal = genericobjectpoolconfig . default max total ;
private properties connectionproperties = new properties ( ) ;
return defaultreadonly ;
private string username = null ;
private boolean testonborrow = true ;
public list < string > getconnectioninitsqls ( ) {
private long minevictableidletimemillis =
private volatile boolean defaultautocommit = true ;
private boolean closed ;
private string driverclassname = null ;
private volatile string password = null ;
private volatile int defaulttransactionisolation =
datasource = createdatasourceinstance ( ) ;
private int minidle = genericobjectpoolconfig . default min idle ;
private transient boolean defaultreadonly = null ;
private int maxidle = genericobjectpoolconfig . default max idle ;
list < string > result = connectioninitsqls ;
private printwriter logwriter = new printwriter ( new outputstreamwriter (
datasource . setlogwriter ( logwriter ) ;
import java . lang . ref . weakreference ;
import java . util . iterator ;
weakreference < abandonedtrace > ref = iter . next ( ) ;
}
} else if ( ref . get ( ) = = null ) {
this . tracelist . add ( new weakreference < > ( trace ) ) ;
while ( iter . hasnext ( ) ) {
return result ;
iter . remove ( ) ;
arraylist < abandonedtrace > result = new arraylist < > ( tracelist . size ( ) ) ;
result . add ( ref . get ( ) ) ;
private final list < weakreference < abandonedtrace > > tracelist = new arraylist < > ( ) ;
break ;
if ( ref . get ( ) = = null ) {
iterator < weakreference < abandonedtrace > > iter = tracelist . iterator ( ) ;
} else {
if ( trace . equals ( ref . get ( ) ) ) {
return fatalexception ;
if ( !fatalexception ) {
string sqlstate = e . getsqlstate ( ) ;
private final collection < string > disconnectionsqlcodes ;
disconnectionsqlcodes = disconnectsqlcodes ;
}
| | utils . disconnection sql codes . contains ( sqlstate ) : disconnectionsqlcodes . contains ( sqlstate ) ;
fatalsqlexceptionthrown | = isdisconnectionsqlexception ( e ) ;
if ( e . getnextexception ( ) ! = null ) {
fatalexception = isdisconnectionsqlexception ( e . getnextexception ( ) ) ;
if ( fastfailvalidation & & fatalsqlexceptionthrown ) {
throw new sqlexception ( utils . getmessage ( "poolableconnection . validate . fastfail" ) ) ;
fastfailvalidation = fastfailvalidation ;
super . handleexception ( e ) ;
private static mbeanserver mbean server = null ;
if ( sqlstate ! = null ) {
fatalexception = disconnectionsqlcodes = = null ? sqlstate . startswith ( utils . disconnection sql code prefix )
private boolean isdisconnectionsqlexception ( sqlexception e ) {
this ( conn , pool , jmxname , null , false ) ;
private boolean fatalsqlexceptionthrown = false ;
private final objectpool < poolableconnection > pool ;
private final boolean fastfailvalidation ;
objectpool < poolableconnection > pool , objectname jmxname , collection < string > disconnectsqlcodes ,
protected void handleexception ( sqlexception e ) throws sqlexception {
objectpool < poolableconnection > pool , objectname jmxname ) {
@ override
public poolableconnection ( connection conn ,
import java . util . collection ;
boolean fatalexception = false ;
boolean fastfailvalidation ) {
final connection key = getconnectionkey ( connection ) ;
final iterator < weakreference < abandonedtrace > > iter = tracelist . iterator ( ) ;
final xaresource xaresource = xaresources . get ( key ) ;
final arraylist < abandonedtrace > result = new arraylist < > ( size ) ;
final weakreference < abandonedtrace > ref = iter . next ( ) ;
final int size = tracelist . size ( ) ;
} catch ( final systemexception e ) {
final int status = transaction . getstatus ( ) ;
public string nativesql ( string sql ) throws sqlexception { checkopen ( ) ; return conn . nativesql ( sql ) ; }
) ;
public int gettransactionisolation ( ) throws sqlexception { checkopen ( ) ; return conn . gettransactionisolation ( ) ; }
pool . close ( ) ;
public connection getdelegate ( ) {
;
return false ;
public sqlwarning getwarnings ( ) throws sqlexception { checkopen ( ) ; return conn . getwarnings ( ) ; }
public databasemetadata getmetadata ( ) throws sqlexception { checkopen ( ) ; return conn . getmetadata ( ) ; }
return pool ;
if ( closed ) {
public void commit ( ) throws sqlexception { checkopen ( ) ; conn . commit ( ) ; }
( ( null = = resultsetconcurrency & & null = = key . resultsetconcurrency ) | | resultsetconcurrency . equals ( key . resultsetconcurrency ) )
public boolean equals ( object that ) {
}
protected string sql = null ;
stringbuffer buf = new stringbuffer ( ) ;
protected object createkey ( string sql ) {
protected void activate ( ) {
throw new runtimeexception ( e . tostring ( ) ) ;
buf . append ( " , resultsettype = " ) ;
} else {
stmtpoolfactory = stmtpoolfactory ;
string query = validationquery ;
public synchronized preparedstatement preparestatement ( string sql ) throws sqlexception {
if ( obj instanceof poolableconnection ) {
throw new illegalargumentexception ( ) ;
( ( poolableconnection ) obj ) . reallyclose ( ) ;
public void activateobject ( object obj ) {
synchronized public void setconnectionfactory ( connectionfactory connfactory ) {
synchronized public void setstatementpoolfactory ( keyedobjectpoolfactory stmtpoolfactory ) {
} catch ( throwable t ) {
protected integer resultsettype = null ;
validationquery = validationquery ;
} catch ( classcastexception e2 ) {
pstmtkey key = ( pstmtkey ) that ;
public class delegatingconnection implements connection {
throw e ;
public boolean isreadonly ( ) throws sqlexception { checkopen ( ) ; return conn . isreadonly ( ) ; }
public void setdelegate ( connection c ) {
return new poolablepreparedstatement ( getdelegate ( ) . preparestatement ( key . sql ) , key , pstmtpool , this ) ;
import java . util . map ;
( ( preparedstatement ) obj ) . close ( ) ;
return ( preparedstatement ) ( pstmtpool . borrowobject ( createkey ( sql , resultsettype , resultsetconcurrency ) ) ) ;
public objectpool getpool ( ) {
public synchronized preparedstatement preparestatement ( string sql , int resultsettype , int resultsetconcurrency ) throws sqlexception {
( ( delegatingpreparedstatement ) obj ) . passivate ( ) ;
pstmtpool . close ( ) ;
public synchronized void close ( ) throws sqlexception {
try {
keyedobjectpool stmtpool = stmtpoolfactory . createpool ( ) ;
synchronized public void setpool ( objectpool pool ) {
public void passivateobject ( object obj ) {
pstmtpool = null ;
conn . rollback ( ) ;
public void close ( ) throws sqlexception { passivate ( ) ; conn . close ( ) ; }
pstmtkey ( string sql , int resultsettype , int resultsetconcurrency ) {
} catch ( classcastexception e ) {
public void passivateobject ( object key , object obj ) {
} catch ( runtimeexception e ) {
( ( delegatingconnection ) conn ) . activate ( ) ;
public string getcatalog ( ) throws sqlexception { checkopen ( ) ; return conn . getcatalog ( ) ; }
if ( null ! = pool & & pool ! = pool ) {
public boolean isclosed ( ) throws sqlexception { return conn . isclosed ( ) ; }
statement stmt = null ;
if ( null ! = query ) {
if ( conn . isclosed ( ) ) {
public void setcatalog ( string catalog ) throws sqlexception { checkopen ( ) ; conn . setcatalog ( catalog ) ; }
stmt = conn . createstatement ( ) ;
closed = false ;
return true ;
if ( this = = c ) {
if ( conn instanceof delegatingconnection ) {
throw new sqlexception ( "connection is closed . " ) ;
if ( null = = obj | | ! ( obj instanceof pstmtkey ) ) {
conn . setautocommit ( defaultautocommit ) ;
return ( preparedstatement ) ( pstmtpool . borrowobject ( createkey ( sql ) ) ) ;
pstmtkey key = ( pstmtkey ) obj ;
return new pstmtkey ( normalizesql ( sql ) , resultsettype , resultsetconcurrency ) ;
public callablestatement preparecall ( string sql ) throws sqlexception { checkopen ( ) ; return conn . preparecall ( sql ) ; }
public poolableconnectionfactory ( connectionfactory connfactory , objectpool pool , keyedobjectpoolfactory stmtpoolfactory , string validationquery , boolean defaultreadonly , boolean defaultautocommit ) {
protected keyedobjectpoolfactory stmtpoolfactory = null ;
} catch ( sqlexception e ) {
while ( c ! = null & & c instanceof delegatingconnection ) {
public poolingconnection ( connection c , keyedobjectpool pool ) {
protected objectpool pool = null ;
resultset rset = null ;
public void setautocommit ( boolean autocommit ) throws sqlexception { checkopen ( ) ; conn . setautocommit ( autocommit ) ; }
public statement createstatement ( int resultsettype , int resultsetconcurrency ) throws sqlexception { checkopen ( ) ; return conn . createstatement ( resultsettype , resultsetconcurrency ) ; }
public void setdefaultreadonly ( boolean defaultreadonly ) {
connfactory = connfactory ;
( ( null = = resultsettype & & null = = key . resultsettype ) | | resultsettype . equals ( key . resultsettype ) ) & &
rset . close ( ) ;
if ( null ! = stmtpoolfactory ) {
defaultautocommit = defaultautocommit ;
( ( delegatingpreparedstatement ) obj ) . getinnermostdelegate ( ) . close ( ) ;
public void destroyobject ( object obj ) {
return ( ( ( null = = sql & & null = = key . sql ) | | sql . equals ( key . sql ) ) & &
public void setdefaultautocommit ( boolean defaultautocommit ) {
public void setreadonly ( boolean readonly ) throws sqlexception { checkopen ( ) ; conn . setreadonly ( readonly ) ; }
( ( delegatingconnection ) obj ) . passivate ( ) ;
public void settypemap ( map map ) throws sqlexception { checkopen ( ) ; conn . settypemap ( map ) ; }
if ( !conn . getautocommit ( ) ) {
buf . append ( resultsettype ) ;
stmtpool . setfactory ( ( poolingconnection ) conn ) ;
return buf . tostring ( ) ;
if ( obj instanceof connection ) {
resultsetconcurrency = new integer ( resultsetconcurrency ) ;
public void rollback ( ) throws sqlexception { checkopen ( ) ; conn . rollback ( ) ; }
super ( c ) ;
buf . append ( " , resultsetconcurrency = " ) ;
protected void checkopen ( ) throws sqlexception {
if ( obj instanceof delegatingconnection ) {
public void clearwarnings ( ) throws sqlexception { checkopen ( ) ; conn . clearwarnings ( ) ; }
return sql . trim ( ) ;
protected connection conn = null ;
public callablestatement preparecall ( string sql , int resultsettype , int resultsetconcurrency ) throws sqlexception { checkopen ( ) ; return conn . preparecall ( sql , resultsettype , resultsetconcurrency ) ; }
synchronized public object makeobject ( ) {
public class poolingconnection extends delegatingconnection implements connection , keyedpoolableobjectfactory {
public string tostring ( ) {
getinnermostdelegate ( ) . close ( ) ;
return conn ;
public class poolableconnectionfactory implements poolableobjectfactory {
protected integer resultsetconcurrency = null ;
package org . apache . commons . dbcp ;
pstmtkey ( string sql ) {
conn . setreadonly ( defaultreadonly ) ;
public delegatingconnection ( connection c ) {
import java . sql . * ;
protected string normalizesql ( string sql ) {
public statement createstatement ( ) throws sqlexception { checkopen ( ) ; return conn . createstatement ( ) ; }
public object makeobject ( object obj ) {
connection conn = connfactory . createconnection ( ) ;
buf . append ( "pstmtkey : sql = " ) ;
protected connectionfactory connfactory = null ;
sql = sql ;
protected void passivate ( ) {
return c ;
c = ( ( delegatingconnection ) c ) . getdelegate ( ) ;
public poolingconnection ( connection c ) {
return null ;
} catch ( exception e ) {
rset = stmt . executequery ( query ) ;
} catch ( sqlexception e2 ) {
if ( rset . next ( ) ) {
pool = pool ;
if ( null ! = pstmtpool ) {
} catch ( nullpointerexception e ) {
public preparedstatement preparestatement ( string sql , int resultsettype , int resultsetconcurrency ) throws sqlexception { checkopen ( ) ; return conn . preparestatement ( sql , resultsettype , resultsetconcurrency ) ; }
( ( delegatingpreparedstatement ) obj ) . activate ( ) ;
public void destroyobject ( object key , object obj ) {
synchronized public boolean validateobject ( object obj ) {
stmt . close ( ) ;
( ( delegatingconnection ) obj ) . activate ( ) ;
import org . apache . commons . pool . * ;
protected boolean defaultautocommit = true ;
return new pstmtkey ( normalizesql ( sql ) ) ;
connection conn = ( connection ) obj ;
if ( null = = key . resultsettype & & null = = key . resultsetconcurrency ) {
return "poolingconnection : " + pstmtpool . tostring ( ) ;
( ( preparedstatement ) obj ) . clearparameters ( ) ;
protected string validationquery = null ;
return new poolableconnection ( conn , pool ) ;
buf . append ( resultsetconcurrency ) ;
public boolean getautocommit ( ) throws sqlexception { checkopen ( ) ; return conn . getautocommit ( ) ; }
throw new sqlexception ( e . tostring ( ) ) ;
resultsettype = new integer ( resultsettype ) ;
public void settransactionisolation ( int level ) throws sqlexception { checkopen ( ) ; conn . settransactionisolation ( level ) ; }
public map gettypemap ( ) throws sqlexception { checkopen ( ) ; return conn . gettypemap ( ) ; }
protected boolean defaultreadonly = false ;
buf . append ( sql ) ;
conn = c ;
protected keyedobjectpool pstmtpool = null ;
return ( null = = sql ? 0 : sql . hashcode ( ) ) ;
public boolean validateobject ( object key , object obj ) {
closed = true ;
public connection getinnermostdelegate ( ) {
pool . setfactory ( this ) ;
public int hashcode ( ) {
public preparedstatement preparestatement ( string sql ) throws sqlexception { checkopen ( ) ; return conn . preparestatement ( sql ) ; }
connection c = conn ;
pstmtpool = pool ;
protected boolean closed = false ;
conn = new poolingconnection ( conn , stmtpool ) ;
defaultreadonly = defaultreadonly ;
public void activateobject ( object key , object obj ) {
( ( delegatingconnection ) conn ) . passivate ( ) ;
protected object createkey ( string sql , int resultsettype , int resultsetconcurrency ) {
synchronized public void setvalidationquery ( string validationquery ) {
return new poolablepreparedstatement ( getdelegate ( ) . preparestatement ( key . sql , key . resultsettype . intvalue ( ) , key . resultsetconcurrency . intvalue ( ) ) , key , pstmtpool , this ) ;
class pstmtkey {
string decodedmapkey = decodefieldkey ( mapkey ) ;
result = uniontomongo ( docf , fieldschema , storetype , value ) ;
for ( entry < charsequence , ? > e : value . entryset ( ) ) {
if ( list = = null ) {
final object value ) {
return new dirtymapwrapper ( rmap ) ;
rmap . put ( new utf8 ( decodedmapkey ) , o ) ;
todbobject ( docf , member . schema ( ) , innertype , innerstoretype ,
private object uniontomongo ( final string docf , final schema fieldschema ,
private object frommongolist ( final string docf , final schema fieldschema ,
list < object > list = easybson . getdblist ( docf ) ;
final schema fieldschema , final object value ) {
object result = todbobject ( docf , fieldschema , fieldtype , storetype ,
final schema fieldschema , final type fieldtype ) {
private basicdblist listtomongo ( final string docf , final collection < ? > array ,
}
return new dirtylistwrapper ( rlist ) ;
private basicdbobject maptomongo ( final string docf ,
result . put ( docf , o ) ;
string encodedmapkey = encodefieldkey ( mapkey ) ;
if ( map = = null ) {
object mapvalue = e . getvalue ( ) ;
result = todbobject ( docf , innerschema , type1 , storetype , value ) ;
rlist . add ( o ) ;
mapvalue ) ;
result = maptomongo ( docf , ( map < charsequence , ? > ) value , valueschema ,
private object frommongomap ( final string docf , final schema fieldschema ,
for ( object item : list ) {
final type fieldtype ) {
member . name ( ) ,
map . put ( encodedmapkey , result ) ;
object o = fromdbobject ( fieldschema . getelementtype ( ) , storetype , f ,
log . error ( "unknown field type : { } " , fieldschema . gettype ( ) ) ;
record . put (
object result = todbobject ( docf , fieldschema , fieldtype , storetype , item ) ;
elementschema . gettype ( ) ) ;
private basicdbobject recordtomongo ( final string docf ,
if ( value = = null )
object o = todbobject ( docf , f . schema ( ) , f . schema ( ) . gettype ( ) ,
result = frommongomap ( docf , fieldschema , easybson , field ) ;
final map < charsequence , ? > value , final schema fieldschema ,
string mapkey = e . getkey ( ) . tostring ( ) ;
result = frommongolist ( docf , fieldschema , easybson , field ) ;
basicdbobject map = easybson . getdbobject ( docf ) ;
final bsondecorator easybson , final field f ) {
list < object > rlist = new arraylist < object > ( ) ;
string mapkey = e . getkey ( ) ;
documentfieldtype storetype = mapping . getdocumentfieldtype ( docf ) ;
private object todbobject ( final string docf , final schema fieldschema ,
object o = fromdbobject ( fieldschema . getvaluetype ( ) , storetype , f , mapkey ,
result = recordtomongo ( docf , fieldschema , value ) ;
"item" , new bsondecorator ( new basicdbobject ( "item" , item ) ) ) ;
valueschema . gettype ( ) ) ;
return null ;
list . add ( result ) ;
innervalue ) ) ;
result = listtomongo ( docf , ( list < ? > ) value , elementschema ,
new bsondecorator ( map ) ) ;
storetype , value ) ;
final type fieldtype , final documentfieldtype storetype ,
map . put ( family + " : " + column + cassandrastore . union col sufix , field + cassandrastore . union col sufix ) ;
list . add ( column + cassandrastore . union col sufix ) ;
import org . apache . avro . schema ;
schema persistentschema = query . getdatastore ( ) . newpersistent ( ) . getschema ( ) ;
if ( persistentschema . getfield ( field ) . schema ( ) . gettype ( ) = = type . union )
import org . apache . gora . store . datastore ;
import org . apache . avro . schema . type ;
return this . cassandramapping . getkeyspacename ( ) ;
string colfamconsislvl = ( colfamconslvl ! = null & & !colfamconslvl . isempty ( ) ) ? colfamconslvl : default hector consis level ;
clmap . put ( colfamdef . getname ( ) , hconsistencylevel . valueof ( colfamconsislvl ) ) ;
ccl . setdefaultreadconsistencylevel ( hconsistencylevel . valueof ( opconsislvl ) ) ;
new cassandrahostconfigurator ( this . cassandramapping . gethostname ( ) ) ) ;
ccl . setwritecfconsistencylevels ( clmap ) ;
string opconsislvl = ( readopconslvl! = null | | !readopconslvl . isempty ( ) ) ? readopconslvl : default hector consis level ;
rangesuperslicesquery < k , string , bytebuffer , bytebuffer > rangesuperslicesquery = hfactory . createrangesuperslicesquery
( this . keyspace , this . keyserializer , bytebufferserializer . get ( ) , bytebufferserializer . get ( ) ) ;
import static org . apache . gora . cassandra . store . cassandrastore . colfamconslvl ;
rangeslicesquery < k , bytebuffer , bytebuffer > rangeslicesquery = hfactory . createrangeslicesquery
}
opconsislvl = ( writeopconslvl! = null | | !writeopconslvl . isempty ( ) ) ? writeopconslvl : default hector consis level ;
map < string , hconsistencylevel > clmap = new hashmap < string , hconsistencylevel > ( ) ;
this . cluster = hfactory . getorcreatecluster ( this . cassandramapping . getclustername ( ) ,
for ( columnfamilydefinition colfamdef : pcolfams )
import static org . apache . gora . cassandra . store . cassandrastore . writeopconslvl ;
private map < string , hconsistencylevel > getconsislevelforcolfams ( list < columnfamilydefinition > pcolfams ) {
map < string , hconsistencylevel > clmap = getconsislevelforcolfams ( columnfamilydefinitions ) ;
log . debug ( "hector write consistency configured to '" + opconsislvl + "' . " ) ;
import static org . apache . gora . cassandra . store . cassandrastore . readopconslvl ;
ccl . setreadcfconsistencylevels ( clmap ) ;
log . debug ( "columnfamily consistency level configured to '" + colfamconsislvl + "' . " ) ;
log . debug ( "hector read consistency configured to '" + opconsislvl + "' . " ) ;
configurableconsistencylevel ccl = new configurableconsistencylevel ( ) ;
hfactory . createkeyspace ( "keyspace" , this . cluster , ccl ) ;
import java . util . properties ;
return clmap ;
( this . keyspace , this . keyserializer , stringserializer . get ( ) , bytebufferserializer . get ( ) , bytebufferserializer . get ( ) ) ;
ccl . setdefaultwriteconsistencylevel ( hconsistencylevel . valueof ( opconsislvl ) ) ;
this . cassandramapping . getkeyspacename ( ) ,
public static final string default hector consis level = "quorum" ;
specificdatumreader < ? > reader = readermap . get ( schemaid ) ;
if ( solrjservertype . tolowercase ( locale . getdefault ( ) ) . equals ( "http" ) ) {
sb . append ( " \ \ " ) . append ( c ) ;
} else if ( solrjservertype . tolowercase ( locale . getdefault ( ) ) . equals ( "loadbalance" ) ) {
} else if ( solrjservertype . tolowercase ( locale . getdefault ( ) ) . equals ( "cloud" ) ) {
specificdatumwriter writer = writermap . get ( schemaid ) ;
} else if ( solrjservertype . tolowercase ( locale . getdefault ( ) ) . equals ( "concurrent" ) ) {
batch = new arraylist < > ( batchsize ) ;
for ( schema currentschema : punionschema . gettypes ( ) ) {
arraylist < partitionquery < k , t > > partitions = new arraylist < > ( ) ;
return new solrquery < > ( this ) ;
public static final concurrenthashmap < string , specificdatumreader < ? > > readermap = new concurrenthashmap < > ( ) ;
public static final concurrenthashmap < string , specificdatumwriter < ? > > writermap = new concurrenthashmap < > ( ) ;
for ( field otherfield : otherfields ) {
return new solrresult < > ( this , query , server , resultssize ) ;
list < partitionquery < k , t > > list = new arraylist < > ( ) ;
type schematype = currentschema . gettype ( ) ;
return new memresult < > ( this , query , submap ) ;
int index = otherfield . pos ( ) ;
return new memquery < > ( this ) ;
partitionqueryimpl < k , t > pqi = new partitionqueryimpl < > ( query ) ;
import org . apache . hadoop . conf . configurable ;
public static final string ispn connection string default = "127 . 0 . 0 . 1 : 11222" ;
public synchronized void initialize ( class < k > keyclass , class < t > persistentclass , properties properties ) throws exception {
public void putifabsent ( k key , t obj ) {
}
import org . infinispan . avro . client . support ;
public basiccache < k , t > getcache ( ) {
cache . remove ( key ) ;
marshaller < t > marshaller = new marshaller < t > ( persistentclass ) ;
log . debug ( "close ( ) " ) ;
private configuration conf ;
return cache . containskey ( key ) ;
import org . apache . hadoop . conf . configuration ;
public synchronized void put ( k key , t val ) {
return ( querybuilder ) qf . from ( persistentclass ) ;
cachemanager = new remotecachemanager ( builder . build ( ) ) ;
cache . clear ( ) ;
configurationbuilder builder = new configurationbuilder ( ) ;
public class infinispanclient < k , t extends persistentbase > implements configurable {
toput . clear ( ) ;
import org . infinispan . client . hotrod . configuration . configurationbuilder ;
public infinispanclient ( ) {
public synchronized void close ( ) {
cacheexists = false ;
import java . util . properties ;
properties . setproperty ( ispn connection string key , host ) ;
import java . util . map ;
if ( cache! = null )
public static final logger log = loggerfactory . getlogger ( infinispanclient . class ) ;
try {
toput . put ( key , val ) ;
private remotecache < k , t > cache ;
} catch ( instantiationexception | illegalaccessexception e ) {
import java . util . hashmap ;
support . registerschema ( cachemanager , persistentclass . newinstance ( ) . getschema ( ) ) ;
getcache ( ) . stop ( ) ;
this . persistentclass = persistentclass ;
if ( !toput . isempty ( ) ) cache . putall ( toput ) ;
private class < t > persistentclass ;
import org . apache . gora . persistency . impl . persistentbase ;
throw new runtimeexception ( e ) ;
private map < k , t > toput ;
import org . infinispan . avro . client . marshaller ;
public querybuilder getquerybuilder ( ) {
log . debug ( "flush ( ) " ) ;
public void flush ( ) {
private boolean cacheexists ;
import org . infinispan . avro . hotrod . queryfactory ;
package org . apache . gora . infinispan . store ;
flush ( ) ;
import org . slf4j . loggerfactory ;
private class < k > keyclass ;
toput = new hashmap < > ( ) ;
qf = org . infinispan . avro . hotrod . search . getqueryfactory ( cache ) ;
public boolean containskey ( k key ) {
return cache . get ( key ) ;
public void createcache ( ) {
return this . persistentclass . getsimplename ( ) ;
public static final string ispn connection string key = "infinispan . connectionstring" ;
cacheexists = true ;
public void dropcache ( ) {
conf = new configuration ( ) ;
import org . infinispan . commons . api . basiccache ;
import org . infinispan . client . hotrod . remotecachemanager ;
createschema ( ) ;
return conf ;
import org . slf4j . logger ;
cachemanager . stop ( ) ;
getconf ( ) . get ( ispn connection string key , ispn connection string default ) ) ;
@ override
this . cache . putifabsent ( key , obj ) ;
private remotecachemanager cachemanager ;
this . conf = conf ;
public boolean cacheexists ( ) {
builder . marshaller ( marshaller ) ;
private queryfactory qf ;
conf . set ( ispn connection string key , host ) ;
public void deletebykey ( k key ) {
this . keyclass = keyclass ;
public t get ( k key ) {
string host = properties . getproperty ( ispn connection string key ,
cache = cachemanager . getcache ( persistentclass . getsimplename ( ) ) ;
import org . infinispan . avro . hotrod . querybuilder ;
builder . addservers ( host ) ;
import org . infinispan . client . hotrod . remotecache ;
public void setconf ( configuration conf ) {
public void createschema ( ) {
return ;
cachemanager . start ( ) ;
public configuration getconf ( ) {
return cacheexists ;
log . info ( "connecting client to " + host ) ;
public string getcachename ( ) {
return this . cache ;
void clearfield ( string field ) ;
result < k , t > result = query . execute ( ) ;
field specificfield = getschema ( ) . getfield ( field ) ;
continue ;
deletedrows + + ;
string [ ] fields = getfieldstoquery ( query . getfields ( ) ) ;
line ( piden , " @ override" ) ;
if ( unmanagedfields . contains ( specificfield ) ) {
put ( specificfield . pos ( ) , persistentdata . get ( ) . deepcopy ( specificfield . schema ( ) ,
persistentdata . get ( ) . getdefaultvalue ( specificfield ) ) ) ;
}
cleardirynessiffieldisdirtyable ( specificfield . pos ( ) ) ;
line ( piden , "public void clearfield ( string field ) { } " ) ;
collection < field > unmanagedfields = getunmanagedfields ( ) ;
if ( isallfields ) {
for ( string field : fields ) {
result . get ( ) . clearfield ( field ) ;
@ override
while ( result . next ( ) ) {
boolean isallfields = arrays . equals ( fields , getfields ( ) ) ;
public void clearfield ( string field ) {
if ( delete ( result . getkey ( ) ) ) {
public void clearfield ( string field ) { }
try {
import java . util . locale ;
if ( !schemaexists ( ) )
solr server user auth , "false" ) ;
return false ;
fieldvalue = fieldvalue . tostring ( ) ;
if ( e . getmessage ( ) . contains ( "no such core" ) ) {
params . set ( commonparams . qt , " / get" ) ;
fieldvalue = solrvalue ;
updateresponse rsp = server . deletebyquery ( q ) ;
else
coreadminresponse rsp = coreadminrequest . getstatus ( mapping . getcorename ( ) ,
object sv = doc . get ( sf ) ;
solrjserverimpl = datastorefactory . findproperty ( properties , this ,
protected static final string solr server user auth = "solr . solrjserver . user auth" ;
if ( sv = = null ) {
for ( element field : fields ) {
if ( sf = = null ) {
protected static final string solr url property = "solr . url" ;
for ( int i = 0 ; i < arr . length ; i + + ) {
import java . util . map ;
protected static final string solr solrjserver impl = "solr . solrjserver" ;
element primarykeyel = classelement . getchild ( "primarykey" ) ;
import java . util . concurrent . concurrenthashmap ;
protected static final string solr server username = "solr . solrjserver . username" ;
return persistent ;
specificdatumreader localreader = null ;
if ( writer = = null ) {
import org . apache . solr . client . solrj . impl . lbhttpsolrclient ;
return reader ;
if ( pk . equals ( f ) ) {
fieldschema = fieldschema . gettypes ( ) . get ( 0 ) ;
log . info ( rsp . tostring ( ) ) ;
case fixed :
if ( serveruserauth ) {
return true ;
type type1 = fieldschema . gettypes ( ) . get ( 1 ) . gettype ( ) ;
public void flush ( ) {
public long deletebyquery ( query < k , t > query ) {
import org . apache . avro . schema . field ;
object v = deserializefieldvalue ( field , fieldschema , sv , persistent ) ;
protected static final string solr commit within property = "solr . commit within" ;
fieldvalue = serilazedata ;
import org . slf4j . loggerfactory ;
} else if ( solrjservertype . tolowercase ( locale . getdefault ( ) ) . equals ( "concurrent" ) ) {
arraylist < partitionquery < k , t > > partitions = new arraylist < > ( ) ;
private int commitwithin = default commit within ;
return mapping . getcorename ( ) ;
list < element > classes = doc . getrootelement ( ) . getchildren ( "class" ) ;
this . adminserver = new concurrentupdatesolrclient ( solrclienturl , 1000 , 10 ) ;
import org . apache . solr . common . solrinputdocument ;
fieldvalue = ioutils . deserialize ( ( byte [ ] ) solrvalue , unionreader ,
} catch ( exception ex ) {
return default union schema ;
} else if ( solrjservertype . tolowercase ( locale . getdefault ( ) ) . equals ( "loadbalance" ) ) {
private string serverpassword ;
log . warn ( "invalid batch size ' { } ' , using default { } " , batchsizestring , default batch size ) ;
server . add ( batch , commitwithin ) ;
public static final concurrenthashmap < schema , specificdatumwriter < ? > > writermap = new concurrenthashmap < > ( ) ;
else if ( pvalue instanceof bytebuffer & & schematype . equals ( type . bytes ) )
commitwithin = integer . parseint ( commitwithinstring ) ;
boolean exists = false ;
if ( fields = = null ) {
protected static final int default batch size = 100 ;
object fieldvalue = null ;
if ( type0 . equals ( schema . type . null ) )
} catch ( ioexception e ) {
import org . apache . hadoop . util . stringutils ;
return null ;
import org . apache . solr . client . solrj . request . coreadminrequest ;
stringbuilder sb = new stringbuilder ( ) ;
else if ( pvalue instanceof integer & & schematype . equals ( type . int ) )
if ( reader = = null ) {
fieldvalue = avroutils . getenumvalue ( fieldschema , ( string ) solrvalue ) ;
default :
( defaulthttpclient ) ( ( cloudsolrclient ) server ) . getlbclient ( ) . gethttpclient ( ) ,
import org . apache . avro . util . utf8 ;
fields = fieldmap . keyset ( ) . toarray ( new string [ fieldmap . size ( ) ] ) ;
log . info ( "using httpsolrclient solrj implementation . " ) ;
specificdatumwriter writer = getdatumwriter ( fieldschema ) ;
log . info ( "using concurrentupdatesolrclient solrj implementation . " ) ;
if ( resultssizestring ! = null ) {
schema schema = persistent . getschema ( ) ;
return map ;
import org . apache . avro . schema ;
string mappingfile = datastorefactory . getmappingfile ( properties , this ,
protected static final string solr config property = "solr . config" ;
this . server = new lbhttpsolrclient ( solrurlelements + " / " + mapping . getcorename ( ) ) ;
import org . apache . avro . specific . specificdatumreader ;
solr solrjserver impl , "http" ) ;
private boolean serveruserauth ;
import org . apache . solr . common . params . modifiablesolrparams ;
queryresponse rsp = server . query ( params ) ;
writermap . put ( fieldschema , writer ) ;
specificdatumreader reader = getdatumreader ( fieldschema ) ;
} else {
batchsize = integer . parseint ( batchsizestring ) ;
server . deletebyquery ( " * : * " ) ;
protected static final string default mapping file = "gora - solr - mapping . xml" ;
} catch ( numberformatexception nfe ) {
import org . apache . gora . query . partitionquery ;
exists = rsp . getuptime ( mapping . getcorename ( ) ) ! = null ;
import org . jdom . input . saxbuilder ;
saxbuilder builder = new saxbuilder ( ) ;
if ( autocreateschema ) {
reader = localreader ;
specificdatumreader < ? > reader = readermap . get ( fieldschema ) ;
import java . nio . bytebuffer ;
return exists ;
if ( commitwithinstring ! = null ) {
( defaulthttpclient ) ( ( httpsolrclient ) server ) . gethttpclient ( ) ,
sf = mapping . getsolrfield ( f ) ;
super . initialize ( keyclass , persistentclass , properties ) ;
try {
this . adminserver = new cloudsolrclient ( solrclienturl ) ;
switch ( c ) {
log . info ( "using cloudsolrclient solrj implementation . " ) ;
+ "no mapping has been initialized for class mapping at position "
@ suppresswarnings ( "unchecked" )
map . addfield ( fieldname , columnname ) ;
import org . apache . avro . schema . type ;
if ( i > 0 )
mapping = readmapping ( mappingfile ) ;
specificdatumreader unionreader = getdatumreader ( fieldschema ) ;
this . adminserver = new lbhttpsolrclient ( solrurlelements ) ;
resultssize = integer . parseint ( resultssizestring ) ;
solrclienturl = datastorefactory . findproperty ( properties , this ,
public string getschemaname ( ) {
string fieldname = field . getattributevalue ( "name" ) ;
throw new runtimeexception ( e ) ;
import org . apache . solr . common . params . commonparams ;
char c = key . charat ( i ) ;
import org . apache . solr . client . solrj . impl . cloudsolrclient ;
params . set ( "id" , key . tostring ( ) ) ;
type type0 = fieldschema . gettypes ( ) . get ( 0 ) . gettype ( ) ;
protected static final int default commit within = 1000 ;
private arraylist < solrinputdocument > batch ;
+ escapequerykey ( key . tostring ( ) ) ) ;
byte [ ] serilazedata = null ;
persistent ) ;
string keyfield = mapping . getprimarykey ( ) ;
import java . util . arraylist ;
persistent . cleardirty ( ) ;
sb . append ( c ) ;
string q = ( ( solrquery < k , t > ) query ) . tosolrquery ( ) ;
map . setcorename ( tablename ) ;
protected static final int default results size = 100 ;
string tablename = getschemaname (
return new solrresult < > ( this , query , server , resultssize ) ;
if ( key = = null ) {
throws ioexception {
import org . apache . solr . client . solrj . impl . httpclientutil ;
public static final concurrenthashmap < schema , specificdatumreader < ? > > readermap = new concurrenthashmap < > ( ) ;
public query < k , t > newquery ( ) {
case map :
return mapping ;
import org . apache . gora . query . impl . partitionqueryimpl ;
public list < partitionquery < k , t > > getpartitions ( query < k , t > query )
string sf = mapping . getsolrfield ( field . name ( ) ) ;
for ( field field : fields ) {
solr results size property , null ) ;
server . commit ( false , true , true ) ;
for ( schema currentschema : punionschema . gettypes ( ) ) {
@ override
private specificdatumreader getdatumreader ( schema fieldschema ) {
serveruserauth = datastorefactory . findbooleanproperty ( properties , this ,
log . error ( e . getmessage ( ) ) ;
. getresourceasstream ( filename ) ) ;
fieldvalue = new utf8 ( solrvalue . tostring ( ) ) ;
mapping . getcorename ( ) , adminserver , solrconfig , solrschema ) ;
else if ( pvalue instanceof long & & schematype . equals ( type . long ) )
private solrclient server , adminserver ;
continue ;
if ( fieldschema . gettypes ( ) . size ( ) = = 2 & & isnullable ( fieldschema ) ) {
t persistent = newpersistent ( ) ;
import org . apache . gora . query . result ;
serverusername = datastorefactory . findproperty ( properties , this ,
adminserver ) ;
for ( element classelement : classes ) {
this . server = new httpsolrclient ( solrclienturl + " / " + mapping . getcorename ( ) ) ;
v = serializefieldvalue ( fieldschema , v ) ;
partitions . add ( pqi ) ;
list < element > fields = classelement . getchildren ( "field" ) ;
object o = rsp . getresponse ( ) . get ( "doc" ) ;
object v = persistent . get ( field . pos ( ) ) ;
return writer ;
byte [ ] data = null ;
this . adminserver = new httpsolrclient ( solrclienturl ) ;
modifiablesolrparams params = new modifiablesolrparams ( ) ;
else if ( pvalue instanceof list & & schematype . equals ( type . array ) )
persistent . setdirty ( field . pos ( ) ) ;
if ( solrjservertype . tolowercase ( locale . getdefault ( ) ) . equals ( "http" ) ) {
string commitwithinstring = datastorefactory . findproperty ( properties , this ,
import org . apache . gora . persistency . persistent ;
string resultssizestring = datastorefactory . findproperty ( properties , this ,
solrmapping map = new solrmapping ( ) ;
private string serverusername ;
}
case array :
throw new ioexception ( " ? ? ? " ) ;
import org . apache . gora . solr . query . solrresult ;
import org . apache . solr . client . solrj . response . queryresponse ;
if ( !persistent . isdirty ( ) ) {
} else if ( solrjservertype . tolowercase ( locale . getdefault ( ) ) . equals ( "cloud" ) ) {
this . server = new cloudsolrclient ( solrclienturl + " / " + mapping . getcorename ( ) ) ;
pqi . setconf ( getconf ( ) ) ;
import org . apache . solr . client . solrj . response . updateresponse ;
import java . io . ioexception ;
import org . apache . solr . client . solrj . solrserverexception ;
( defaulthttpclient ) ( ( lbhttpsolrclient ) server ) . gethttpclient ( ) ,
persistent . put ( field . pos ( ) , v ) ;
persistent . get ( field . pos ( ) ) ) ;
import java . util . properties ;
public static final int default union schema = 0 ;
sb . append ( sep ) ;
string [ ] solrurlelements = stringutils . split ( solrclienturl ) ;
private static final string todelimitedstring ( string [ ] arr , string sep ) {
return 0 ;
solrinputdocument doc = new solrinputdocument ( ) ;
import org . jdom . document ;
serilazedata = ioutils . serialize ( writer , fieldvalue ) ;
import org . apache . solr . client . solrj . impl . httpsolrclient ;
fieldvalue = deserializefieldvalue ( field , fieldschema , solrvalue ,
if ( v = = null ) {
data = ioutils . serialize ( writer , fieldvalue ) ;
+ "match with intended values . a mapping mismatch has been found therefore "
httpclientutil . setbasicauth (
solr server password , null ) ;
serverusername , serverpassword ) ;
batch = new arraylist < > ( batchsize ) ;
import org . apache . gora . solr . query . solrquery ;
break ;
private void add ( arraylist < solrinputdocument > batch , int commitwithin )
case record :
string columnname = field . getattributevalue ( "column" ) ;
persistentclass . getcanonicalname ( ) ) ) {
reader = new specificdatumreader ( fieldschema ) ;
import java . util . list ;
batch . add ( doc ) ;
import org . apache . gora . util . ioutils ;
params . set ( commonparams . fl , todelimitedstring ( fields , " , " ) ) ;
serverpassword = datastorefactory . findproperty ( properties , this ,
createschema ( ) ;
else if ( pvalue instanceof boolean & & schematype . equals ( type . boolean ) )
import org . apache . gora . store . impl . datastorebase ;
import org . apache . solr . client . solrj . impl . concurrentupdatesolrclient ;
import org . apache . solr . client . solrj . solrclient ;
public result < k , t > execute ( query < k , t > query ) {
if ( o = = null ) {
fieldvalue = ioutils . deserialize ( ( byte [ ] ) solrvalue , reader ,
for ( string f : fields ) {
private string solrclienturl , solrconfig , solrschema , solrjserverimpl ;
else if ( pvalue instanceof persistent & & schematype . equals ( type . record ) )
} catch ( exception e ) {
fieldvalue = serializefieldvalue ( unionschema , fieldvalue ) ;
public void put ( k key , t persistent ) {
map . setprimarykey ( primarykeyel . getattributevalue ( "column" ) ) ;
fieldvalue = ( ( bytebuffer ) fieldvalue ) . array ( ) ;
else if ( pvalue instanceof map & & schematype . equals ( type . map ) )
case string :
sb . append ( " \ \ " ) . append ( c ) ;
solr commit within property , null ) ;
private int resultssize = default results size ;
log . info ( "putting document : " + doc ) ;
field field = fieldmap . get ( f ) ;
specificdatumwriter writer = writermap . get ( fieldschema ) ;
log . error ( e . getmessage ( ) , e ) ;
case ' : ' :
+ " { } in mapping file . " , classes . indexof ( classelement ) ) ;
private solrmapping readmapping ( string filename ) throws ioexception {
return partitions ;
coreadminrequest . unloadcore ( mapping . getcorename ( ) , adminserver ) ;
properties properties ) {
solrconfig = datastorefactory . findproperty ( properties , this ,
return newinstance ( ( solrdocument ) o , fields ) ;
public void createschema ( ) {
this . server = new concurrentupdatesolrclient ( solrclienturl + " / " + mapping . getcorename ( ) , 1000 , 10 ) ;
throws solrserverexception , ioexception {
@ suppresswarnings ( "rawtypes" )
log . warn ( "invalid commit within ' { } ' , using default { } " , commitwithinstring , default commit within ) ;
string pk = mapping . getprimarykey ( ) ;
doc . addfield ( mapping . getprimarykey ( ) , key ) ;
if ( innerschema . gettype ( ) . equals ( schema . type . null ) ) {
import org . apache . solr . client . solrj . response . coreadminresponse ;
private static final logger log = loggerfactory . getlogger ( solrstore . class ) ;
import org . apache . gora . util . avroutils ;
default mapping file ) ;
if ( ( localreader = readermap . putifabsent ( fieldschema , reader ) ) ! = null ) {
keyclass . getcanonicalname ( ) )
if ( commitwithin = = 0 ) {
switch ( fieldschema . gettype ( ) ) {
else if ( pvalue instanceof double & & schematype . equals ( type . double ) )
private object deserializefieldvalue ( field field , schema fieldschema ,
writer = new specificdatumwriter ( fieldschema ) ;
private object serializefieldvalue ( schema fieldschema , object fieldvalue ) {
import org . jdom . element ;
import org . apache . http . impl . client . defaulthttpclient ;
public boolean schemaexists ( ) {
object solrvalue , t persistent ) throws ioexception {
list < field > fields = schema . getfields ( ) ;
int schemapos = getunionschema ( fieldvalue , fieldschema ) ;
server . commit ( ) ;
case ' * ' :
log . info ( "using solr server at " + solrclienturl ) ;
case enum :
import org . apache . avro . specific . specificdatumwriter ;
coreadminrequest . createcore ( mapping . getcorename ( ) ,
server . add ( batch ) ;
throw new ioexception ( ex ) ;
case bytes :
updateresponse rsp = server . deletebyquery ( keyfield + " : "
public void initialize ( class < k > keyclass , class < t > persistentclass ,
private boolean isnullable ( schema unionschema ) {
if ( pvalue instanceof utf8 & & schematype . equals ( type . string ) )
( defaulthttpclient ) ( ( lbhttpsolrclient ) adminserver ) . gethttpclient ( ) ,
fieldvalue = data ;
partitionqueryimpl < k , t > pqi = new partitionqueryimpl < > ( query ) ;
public class solrstore < k , t extends persistentbase > extends datastorebase < k , t > {
for ( int i = 0 ; i < key . length ( ) ; i + + ) {
( defaulthttpclient ) ( ( httpsolrclient ) adminserver ) . gethttpclient ( ) ,
private int getunionschema ( object pvalue , schema punionschema ) {
if ( classelement . getattributevalue ( "keyclass" ) . equals (
& & classelement . getattributevalue ( "name" ) . equals (
import org . apache . gora . persistency . impl . persistentbase ;
string sf = null ;
public t newinstance ( solrdocument doc , string [ ] fields ) throws ioexception {
fieldvalue = bytebuffer . wrap ( ( byte [ ] ) solrvalue ) ;
schema fieldschema = field . schema ( ) ;
public void deleteschema ( ) {
public void close ( ) {
private solrmapping mapping ;
sb . append ( arr [ i ] ) ;
flush ( ) ;
add ( batch , commitwithin ) ;
if ( batch . size ( ) > 0 ) {
solrschema = datastorefactory . findproperty ( properties , this ,
return new solrquery < > ( this ) ;
case union :
} catch ( malformedurlexception e ) {
return sb . tostring ( ) ;
public solrmapping getmapping ( ) {
if ( !type0 . equals ( type1 ) ) {
for ( schema innerschema : unionschema . gettypes ( ) ) {
fieldschema = fieldschema . gettypes ( ) . get ( 1 ) ;
return "" ;
log . warn ( "check that 'keyclass' and 'name' parameters in gora - solr - mapping . xml "
log . warn ( "invalid results size ' { } ' , using default { } " , resultssizestring , default results size ) ;
solr batch size property , null ) ;
if ( arr = = null | | arr . length = = 0 ) {
int unionschemapos = 0 ;
import org . slf4j . logger ;
return unionschemapos ;
doc . addfield ( sf , v ) ;
if ( batchsizestring ! = null ) {
string batchsizestring = datastorefactory . findproperty ( properties , this ,
solr server username , null ) ;
else if ( pvalue instanceof float & & schematype . equals ( type . float ) )
protected static final string solr schema property = "solr . schema" ;
solr url property , null ) ;
return fieldvalue ;
document doc = builder . build ( getclass ( ) . getclassloader ( )
type schematype = currentschema . gettype ( ) ;
classelement . getattributevalue ( "table" ) , persistentclass ) ;
private specificdatumwriter getdatumwriter ( schema fieldschema ) {
import org . apache . gora . store . datastorefactory ;
if ( batch . size ( ) > = batchsize ) {
import org . apache . gora . query . query ;
unionschemapos + + ;
public t get ( k key , string [ ] fields ) {
solr schema property , null ) ;
public static string escapequerykey ( string key ) {
protected static final string solr server password = "solr . solrjserver . password" ;
protected static final string solr results size property = "solr . results size" ;
string solrjservertype = ( ( solrjserverimpl = = null | | solrjserverimpl . equals ( "" ) ) ? "http" : solrjserverimpl ) ;
sf = f ;
public boolean delete ( k key ) {
package org . apache . gora . solr . store ;
import org . apache . solr . common . solrdocument ;
protected static final string solr batch size property = "solr . batch size" ;
log . info ( "using lbhttpsolrclient solrj implementation . " ) ;
batch . clear ( ) ;
import java . net . malformedurlexception ;
schema unionschema = fieldschema . gettypes ( ) . get ( schemapos ) ;
public void truncateschema ( ) {
return ;
( defaulthttpclient ) ( ( cloudsolrclient ) adminserver ) . getlbclient ( ) . gethttpclient ( ) ,
solr config property , null ) ;
private int batchsize = default batch size ;